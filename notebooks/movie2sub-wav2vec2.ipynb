{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11772275,"sourceType":"datasetVersion","datasetId":7343266}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataloading & Preprocessing","metadata":{}},{"cell_type":"code","source":"!pip install jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:25:51.127399Z","iopub.execute_input":"2025-05-11T18:25:51.127665Z","iopub.status.idle":"2025-05-11T18:25:56.739377Z","shell.execute_reply.started":"2025-05-11T18:25:51.127643Z","shell.execute_reply":"2025-05-11T18:25:56.738338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torchaudio\n\nfrom datasets import Dataset, DatasetDict\nfrom functools import partial\nfrom transformers import Wav2Vec2Processor\nfrom typing import Dict\nimport numpy as np\n\n\ndef load_movie2sub_data(root_dir: str):\n    audio_paths = []\n    texts = []\n\n    for movie_name in os.listdir(root_dir):\n        movie_path = os.path.join(root_dir, movie_name)\n        if not os.path.isdir(movie_path):\n            continue\n\n        for fname in os.listdir(movie_path):\n            if fname.endswith(\".wav\"):\n                base = os.path.splitext(fname)[0]\n                wav_path = os.path.join(movie_path, f\"{base}.wav\")\n                txt_path = os.path.join(movie_path, f\"{base}.txt\")\n\n                if os.path.exists(wav_path) and os.path.exists(txt_path):\n                    audio_paths.append(wav_path)\n                    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n                        texts.append(f.read().strip())\n\n    return Dataset.from_dict({\"audio\": audio_paths, \"text\": texts})\n\ndef preprocess(batch: Dict[str, any], processor: Wav2Vec2Processor, resample_rate=16_000) -> Dict[str, any]:\n    waveform, sample_rate = torchaudio.load(batch[\"audio\"])\n\n    # resample\n    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=resample_rate)\n    waveform = resampler(waveform)\n\n    # convert to mono\n    if waveform.size(0) > 1:\n            waveform = waveform.mean(dim=0, keepdim=True)\n\n    # normalize\n    waveform = waveform / waveform.abs().max()\n    \n    input = processor(waveform.squeeze().numpy(), sampling_rate=resample_rate, return_attention_mask=True)\n    input_values = input.input_values[0]\n\n    input_values_min, input_values_max = min(input_values), max(input_values)\n    input_values = 2 * (input_values - input_values_min) / (input_values_max - input_values_min) - 1\n    \n    batch[\"input_values\"] = input_values\n    batch[\"attention_mask\"] = input.attention_mask[0]\n\n    # process text before tokenization\n    processed_text = batch[\"text\"].upper().replace(\" \", \"|\").replace(\"\\n\", \"|\")\n    batch[\"labels\"] = processor.tokenizer(processed_text).input_ids\n    \n    return batch\n\ndef prepare_dataset(raw_dataset: Dataset, processor: Wav2Vec2Processor, test_size: float | None = None) -> DatasetDict:\n    dataset = raw_dataset.train_test_split(test_size=test_size) if test_size is not None else raw_dataset\n    preprocess_fn = partial(preprocess, processor=processor)\n    return dataset.map(preprocess_fn, remove_columns=[\"audio\", \"text\"])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:28:23.546067Z","iopub.execute_input":"2025-05-11T18:28:23.546950Z","iopub.status.idle":"2025-05-11T18:28:23.562376Z","shell.execute_reply.started":"2025-05-11T18:28:23.546914Z","shell.execute_reply":"2025-05-11T18:28:23.561523Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Trainer","metadata":{}},{"cell_type":"code","source":"from jiwer import wer\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback, EvalPrediction\n\ntraining_args = TrainingArguments(\n    output_dir=\"./wav2vec2-finetuned-moviesubs\",\n    group_by_length=True,\n    dataloader_num_workers=4,\n    per_device_train_batch_size=2,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    num_train_epochs=80,\n    fp16=False,\n    learning_rate=1e-7,\n    logging_strategy=\"steps\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    report_to=\"none\",\n    save_total_limit=1,\n    remove_unused_columns=False,\n    max_grad_norm=0.05,\n)\n\ndef wer_metric(pred: EvalPrediction, processor: Wav2Vec2Processor):\n    pred_ids = pred.predictions.argmax(-1)\n    \n    pred_str = processor.batch_decode(pred_ids)\n    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n\n    wer_loss = wer(label_str, pred_str)\n    return {\"wer\": wer_loss}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:26:07.132494Z","iopub.execute_input":"2025-05-11T18:26:07.132834Z","iopub.status.idle":"2025-05-11T18:26:23.076741Z","shell.execute_reply.started":"2025-05-11T18:26:07.132815Z","shell.execute_reply":"2025-05-11T18:26:23.075961Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DataCollator","metadata":{}},{"cell_type":"code","source":"class DataCollatorCTC:\n    def __init__(self, processor: Wav2Vec2Processor):\n        self.processor = processor\n\n    def __call__(self, features):\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        # pad inputs\n        batch = self.processor.pad(input_features, padding=True, return_tensors=\"pt\")\n        \n        # pad labels\n        with self.processor.as_target_processor():\n            labels_batch = self.processor.pad(\n                label_features,\n                padding=True,\n                return_tensors=\"pt\"\n            )\n        \n        # replace padding with -100 to ignore loss\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n        \n        batch[\"labels\"] = labels\n        return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:26:23.077573Z","iopub.execute_input":"2025-05-11T18:26:23.078138Z","iopub.status.idle":"2025-05-11T18:26:23.083369Z","shell.execute_reply.started":"2025-05-11T18:26:23.078106Z","shell.execute_reply":"2025-05-11T18:26:23.082661Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport warnings\n\nfrom transformers import Wav2Vec2ForCTC\n\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"WANDB_DISABLED\"] = \"true\"\ntorch.autograd.set_detect_anomaly(True) # to crash in case of anomaly\n\nmodel_str = \"facebook/wav2vec2-base-960h\"\n\nprocessor = Wav2Vec2Processor.from_pretrained(model_str, do_normalize=True, feature_size=1, padding_value=0.0, return_attention_mask=True)\ndata_collator = DataCollatorCTC(processor=processor)\n\nraw_dataset = load_movie2sub_data(\"/kaggle/input/movie2sub-dataset/dataset/train\")\ntest_dataset = load_movie2sub_data(\"/kaggle/input/movie2sub-dataset/dataset/test\")\n\ndataset = prepare_dataset(raw_dataset, processor, test_size=0.2)\ntest_dataset = prepare_dataset(test_dataset, processor)\n\nmodel = Wav2Vec2ForCTC.from_pretrained(\n    model_str,\n    vocab_size=len(processor.tokenizer),\n    ctc_loss_reduction=\"mean\",\n    pad_token_id=processor.tokenizer.pad_token_id,\n    ctc_zero_infinity=True,\n)\n\nwrapped_compute_metrics = partial(wer_metric, processor=processor)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    tokenizer=processor,\n    data_collator=data_collator,\n    compute_metrics=wrapped_compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:28:33.805922Z","iopub.execute_input":"2025-05-11T18:28:33.806193Z","iopub.status.idle":"2025-05-11T18:30:31.911032Z","shell.execute_reply.started":"2025-05-11T18:28:33.806171Z","shell.execute_reply":"2025-05-11T18:30:31.908438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\nimport matplotlib.pyplot as plt\n\nlog_history = trainer.state.log_history\n\ntrain_loss = []\ntrain_epochs = []\neval_loss = []\neval_epochs = []\n\nfor log in log_history:\n    if \"loss\" in log and \"epoch\" in log:\n        train_loss.append(log[\"loss\"])\n        train_epochs.append(log[\"epoch\"])\n    if \"eval_loss\" in log and \"epoch\" in log:\n        eval_loss.append(log[\"eval_loss\"])\n        eval_epochs.append(log[\"epoch\"])\n\nplt.figure(figsize=(10, 5))\nplt.plot(train_epochs, train_loss, label=\"Training Loss\")\nplt.plot(eval_epochs, eval_loss, label=\"Validation Loss\", marker='o')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\n\nplt.savefig(\"./train_vs_val_loss.png\", transparent=True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:26:32.910090Z","iopub.status.idle":"2025-05-11T18:26:32.910510Z","shell.execute_reply.started":"2025-05-11T18:26:32.910322Z","shell.execute_reply":"2025-05-11T18:26:32.910336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom torch.utils.data import DataLoader\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=8,\n    shuffle=False,\n    collate_fn=data_collator,\n)\n\noutput_dir = \"./predictions\"\nos.makedirs(output_dir, exist_ok=True)\n\nfor _, batch in enumerate(test_loader):\n    with torch.no_grad():\n        output = model(input_values=batch[\"input_values\"].to(device))\n    \n        with processor.as_target_processor():\n            ground_truths = processor.batch_decode(batch[\"labels\"], group_tokens=False, skip_special_tokens=True)\n            \n        predicted_ids = torch.argmax(output.logits, dim=-1)\n        decoded_texts = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n        \n        for j, (gt, pred) in enumerate(zip(ground_truths, decoded_texts)):\n            print(f\"\\tInput {j:02}\")\n            print(f\"Ground truth: {gt}\\n\")\n            print(f\"Decoded text: {pred}\\n\")\n    \n            with open(os.path.join(output_dir, f\"sample_{j}_gt.txt\"), \"w\", encoding=\"utf-8\") as f:\n                f.write(gt)\n    \n            with open(os.path.join(output_dir, f\"sample_{j}_pred.txt\"), \"w\", encoding=\"utf-8\") as f:\n                f.write(pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:26:32.911324Z","iopub.status.idle":"2025-05-11T18:26:32.911626Z","shell.execute_reply.started":"2025-05-11T18:26:32.911480Z","shell.execute_reply":"2025-05-11T18:26:32.911494Z"}},"outputs":[],"execution_count":null}]}