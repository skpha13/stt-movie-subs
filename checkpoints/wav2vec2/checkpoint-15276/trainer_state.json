{
  "best_global_step": 15276,
  "best_metric": 1.7371054887771606,
  "best_model_checkpoint": "./wav2vec2-finetuned-moviesubs/checkpoint-15276",
  "epoch": 76.0,
  "eval_steps": 500,
  "global_step": 15276,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04975124378109453,
      "grad_norm": 52.81590270996094,
      "learning_rate": 9.994402985074626e-08,
      "loss": 4.0916,
      "step": 10
    },
    {
      "epoch": 0.09950248756218906,
      "grad_norm": 41.61384582519531,
      "learning_rate": 9.98818407960199e-08,
      "loss": 4.2592,
      "step": 20
    },
    {
      "epoch": 0.14925373134328357,
      "grad_norm": 65.37448120117188,
      "learning_rate": 9.981965174129353e-08,
      "loss": 4.0337,
      "step": 30
    },
    {
      "epoch": 0.19900497512437812,
      "grad_norm": 55.43102264404297,
      "learning_rate": 9.975746268656717e-08,
      "loss": 3.9585,
      "step": 40
    },
    {
      "epoch": 0.24875621890547264,
      "grad_norm": 47.168113708496094,
      "learning_rate": 9.969527363184079e-08,
      "loss": 3.7674,
      "step": 50
    },
    {
      "epoch": 0.29850746268656714,
      "grad_norm": 57.355098724365234,
      "learning_rate": 9.963308457711443e-08,
      "loss": 4.1081,
      "step": 60
    },
    {
      "epoch": 0.3482587064676617,
      "grad_norm": 24.76104164123535,
      "learning_rate": 9.957089552238805e-08,
      "loss": 3.8411,
      "step": 70
    },
    {
      "epoch": 0.39800995024875624,
      "grad_norm": 150.49609375,
      "learning_rate": 9.950870646766169e-08,
      "loss": 4.0791,
      "step": 80
    },
    {
      "epoch": 0.44776119402985076,
      "grad_norm": 41.12049865722656,
      "learning_rate": 9.944651741293531e-08,
      "loss": 4.0405,
      "step": 90
    },
    {
      "epoch": 0.4975124378109453,
      "grad_norm": 80.15460205078125,
      "learning_rate": 9.938432835820895e-08,
      "loss": 3.6742,
      "step": 100
    },
    {
      "epoch": 0.5472636815920398,
      "grad_norm": 35.714500427246094,
      "learning_rate": 9.932213930348258e-08,
      "loss": 3.8503,
      "step": 110
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 38.28269577026367,
      "learning_rate": 9.925995024875622e-08,
      "loss": 3.7867,
      "step": 120
    },
    {
      "epoch": 0.6467661691542289,
      "grad_norm": 39.388641357421875,
      "learning_rate": 9.919776119402986e-08,
      "loss": 3.8077,
      "step": 130
    },
    {
      "epoch": 0.6965174129353234,
      "grad_norm": 56.71574020385742,
      "learning_rate": 9.913557213930348e-08,
      "loss": 3.6798,
      "step": 140
    },
    {
      "epoch": 0.746268656716418,
      "grad_norm": 39.88447189331055,
      "learning_rate": 9.907338308457712e-08,
      "loss": 3.5837,
      "step": 150
    },
    {
      "epoch": 0.7960199004975125,
      "grad_norm": 65.20458984375,
      "learning_rate": 9.901119402985074e-08,
      "loss": 3.5843,
      "step": 160
    },
    {
      "epoch": 0.845771144278607,
      "grad_norm": 20.960344314575195,
      "learning_rate": 9.894900497512438e-08,
      "loss": 3.6777,
      "step": 170
    },
    {
      "epoch": 0.8955223880597015,
      "grad_norm": 38.21106719970703,
      "learning_rate": 9.8886815920398e-08,
      "loss": 3.4896,
      "step": 180
    },
    {
      "epoch": 0.945273631840796,
      "grad_norm": 42.69917678833008,
      "learning_rate": 9.882462686567164e-08,
      "loss": 3.632,
      "step": 190
    },
    {
      "epoch": 0.9950248756218906,
      "grad_norm": 19.981046676635742,
      "learning_rate": 9.876243781094526e-08,
      "loss": 3.5532,
      "step": 200
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.9520517587661743,
      "eval_runtime": 54.7294,
      "eval_samples_per_second": 3.673,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6464492287917738,
      "step": 201
    },
    {
      "epoch": 1.044776119402985,
      "grad_norm": 99.06240844726562,
      "learning_rate": 9.87002487562189e-08,
      "loss": 3.6313,
      "step": 210
    },
    {
      "epoch": 1.0945273631840795,
      "grad_norm": 61.56353759765625,
      "learning_rate": 9.863805970149253e-08,
      "loss": 3.483,
      "step": 220
    },
    {
      "epoch": 1.144278606965174,
      "grad_norm": 31.73395347595215,
      "learning_rate": 9.857587064676617e-08,
      "loss": 3.4638,
      "step": 230
    },
    {
      "epoch": 1.1940298507462686,
      "grad_norm": 84.20970916748047,
      "learning_rate": 9.85136815920398e-08,
      "loss": 3.6926,
      "step": 240
    },
    {
      "epoch": 1.243781094527363,
      "grad_norm": 14.044694900512695,
      "learning_rate": 9.845149253731343e-08,
      "loss": 3.4217,
      "step": 250
    },
    {
      "epoch": 1.2935323383084576,
      "grad_norm": 40.62930679321289,
      "learning_rate": 9.838930348258706e-08,
      "loss": 3.3923,
      "step": 260
    },
    {
      "epoch": 1.3432835820895521,
      "grad_norm": 29.6202335357666,
      "learning_rate": 9.832711442786069e-08,
      "loss": 3.4963,
      "step": 270
    },
    {
      "epoch": 1.3930348258706466,
      "grad_norm": 28.667993545532227,
      "learning_rate": 9.826492537313432e-08,
      "loss": 3.3908,
      "step": 280
    },
    {
      "epoch": 1.4427860696517412,
      "grad_norm": 74.09520721435547,
      "learning_rate": 9.820273631840795e-08,
      "loss": 3.3412,
      "step": 290
    },
    {
      "epoch": 1.4925373134328357,
      "grad_norm": 57.29233169555664,
      "learning_rate": 9.814054726368158e-08,
      "loss": 3.2948,
      "step": 300
    },
    {
      "epoch": 1.5422885572139302,
      "grad_norm": 34.75337219238281,
      "learning_rate": 9.807835820895521e-08,
      "loss": 3.2163,
      "step": 310
    },
    {
      "epoch": 1.5920398009950247,
      "grad_norm": 37.35295486450195,
      "learning_rate": 9.801616915422886e-08,
      "loss": 3.3741,
      "step": 320
    },
    {
      "epoch": 1.6417910447761193,
      "grad_norm": 39.98225021362305,
      "learning_rate": 9.795398009950248e-08,
      "loss": 3.3288,
      "step": 330
    },
    {
      "epoch": 1.6915422885572138,
      "grad_norm": 36.651702880859375,
      "learning_rate": 9.789179104477612e-08,
      "loss": 3.3715,
      "step": 340
    },
    {
      "epoch": 1.7412935323383083,
      "grad_norm": 43.58995819091797,
      "learning_rate": 9.782960199004975e-08,
      "loss": 3.4423,
      "step": 350
    },
    {
      "epoch": 1.7910447761194028,
      "grad_norm": 47.04574966430664,
      "learning_rate": 9.776741293532338e-08,
      "loss": 3.5318,
      "step": 360
    },
    {
      "epoch": 1.8407960199004973,
      "grad_norm": 34.42634963989258,
      "learning_rate": 9.770522388059701e-08,
      "loss": 3.3304,
      "step": 370
    },
    {
      "epoch": 1.890547263681592,
      "grad_norm": 24.903247833251953,
      "learning_rate": 9.764303482587064e-08,
      "loss": 3.2696,
      "step": 380
    },
    {
      "epoch": 1.9402985074626866,
      "grad_norm": 73.34808349609375,
      "learning_rate": 9.758084577114427e-08,
      "loss": 3.2483,
      "step": 390
    },
    {
      "epoch": 1.9900497512437811,
      "grad_norm": 30.106985092163086,
      "learning_rate": 9.75186567164179e-08,
      "loss": 3.2331,
      "step": 400
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.93924880027771,
      "eval_runtime": 55.6553,
      "eval_samples_per_second": 3.612,
      "eval_steps_per_second": 0.234,
      "eval_wer": 0.6481362467866324,
      "step": 402
    },
    {
      "epoch": 2.0398009950248754,
      "grad_norm": 20.186975479125977,
      "learning_rate": 9.745646766169153e-08,
      "loss": 3.1852,
      "step": 410
    },
    {
      "epoch": 2.08955223880597,
      "grad_norm": 35.518489837646484,
      "learning_rate": 9.739427860696517e-08,
      "loss": 3.215,
      "step": 420
    },
    {
      "epoch": 2.1393034825870645,
      "grad_norm": 118.48173522949219,
      "learning_rate": 9.73320895522388e-08,
      "loss": 3.2934,
      "step": 430
    },
    {
      "epoch": 2.189054726368159,
      "grad_norm": 31.505048751831055,
      "learning_rate": 9.726990049751243e-08,
      "loss": 3.213,
      "step": 440
    },
    {
      "epoch": 2.2388059701492535,
      "grad_norm": 31.2657470703125,
      "learning_rate": 9.720771144278607e-08,
      "loss": 3.2552,
      "step": 450
    },
    {
      "epoch": 2.288557213930348,
      "grad_norm": 18.79732894897461,
      "learning_rate": 9.71455223880597e-08,
      "loss": 3.2508,
      "step": 460
    },
    {
      "epoch": 2.3383084577114426,
      "grad_norm": 34.001495361328125,
      "learning_rate": 9.708333333333333e-08,
      "loss": 3.269,
      "step": 470
    },
    {
      "epoch": 2.388059701492537,
      "grad_norm": 140.06863403320312,
      "learning_rate": 9.702114427860696e-08,
      "loss": 3.2703,
      "step": 480
    },
    {
      "epoch": 2.4378109452736316,
      "grad_norm": 19.752235412597656,
      "learning_rate": 9.695895522388059e-08,
      "loss": 3.2438,
      "step": 490
    },
    {
      "epoch": 2.487562189054726,
      "grad_norm": 58.452327728271484,
      "learning_rate": 9.689676616915422e-08,
      "loss": 3.2648,
      "step": 500
    },
    {
      "epoch": 2.5373134328358207,
      "grad_norm": 20.223711013793945,
      "learning_rate": 9.683457711442785e-08,
      "loss": 3.1915,
      "step": 510
    },
    {
      "epoch": 2.587064676616915,
      "grad_norm": 19.159420013427734,
      "learning_rate": 9.67723880597015e-08,
      "loss": 3.1402,
      "step": 520
    },
    {
      "epoch": 2.6368159203980097,
      "grad_norm": 44.22649002075195,
      "learning_rate": 9.671019900497512e-08,
      "loss": 3.1722,
      "step": 530
    },
    {
      "epoch": 2.6865671641791042,
      "grad_norm": 33.30509948730469,
      "learning_rate": 9.664800995024876e-08,
      "loss": 3.1693,
      "step": 540
    },
    {
      "epoch": 2.7363184079601988,
      "grad_norm": 52.03491973876953,
      "learning_rate": 9.658582089552239e-08,
      "loss": 3.0653,
      "step": 550
    },
    {
      "epoch": 2.7860696517412933,
      "grad_norm": 67.61563110351562,
      "learning_rate": 9.652363184079601e-08,
      "loss": 3.139,
      "step": 560
    },
    {
      "epoch": 2.835820895522388,
      "grad_norm": 75.11573028564453,
      "learning_rate": 9.646144278606965e-08,
      "loss": 3.2402,
      "step": 570
    },
    {
      "epoch": 2.8855721393034823,
      "grad_norm": 30.606857299804688,
      "learning_rate": 9.639925373134327e-08,
      "loss": 3.0997,
      "step": 580
    },
    {
      "epoch": 2.935323383084577,
      "grad_norm": 28.164274215698242,
      "learning_rate": 9.633706467661691e-08,
      "loss": 3.2824,
      "step": 590
    },
    {
      "epoch": 2.9850746268656714,
      "grad_norm": 19.927082061767578,
      "learning_rate": 9.627487562189053e-08,
      "loss": 3.1793,
      "step": 600
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.9289344549179077,
      "eval_runtime": 54.8813,
      "eval_samples_per_second": 3.662,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6488592544987146,
      "step": 603
    },
    {
      "epoch": 3.0348258706467663,
      "grad_norm": 49.54451370239258,
      "learning_rate": 9.621268656716417e-08,
      "loss": 3.2267,
      "step": 610
    },
    {
      "epoch": 3.084577114427861,
      "grad_norm": 28.22410011291504,
      "learning_rate": 9.615049751243781e-08,
      "loss": 3.1436,
      "step": 620
    },
    {
      "epoch": 3.1343283582089554,
      "grad_norm": 23.215248107910156,
      "learning_rate": 9.608830845771144e-08,
      "loss": 3.1277,
      "step": 630
    },
    {
      "epoch": 3.18407960199005,
      "grad_norm": 57.440399169921875,
      "learning_rate": 9.602611940298507e-08,
      "loss": 3.1927,
      "step": 640
    },
    {
      "epoch": 3.2338308457711444,
      "grad_norm": 11.843758583068848,
      "learning_rate": 9.59639303482587e-08,
      "loss": 2.9891,
      "step": 650
    },
    {
      "epoch": 3.283582089552239,
      "grad_norm": 38.784339904785156,
      "learning_rate": 9.590174129353234e-08,
      "loss": 3.1499,
      "step": 660
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 18.914674758911133,
      "learning_rate": 9.583955223880596e-08,
      "loss": 3.1869,
      "step": 670
    },
    {
      "epoch": 3.383084577114428,
      "grad_norm": 68.69857788085938,
      "learning_rate": 9.57773631840796e-08,
      "loss": 3.0624,
      "step": 680
    },
    {
      "epoch": 3.4328358208955225,
      "grad_norm": 20.689233779907227,
      "learning_rate": 9.571517412935322e-08,
      "loss": 3.1682,
      "step": 690
    },
    {
      "epoch": 3.482587064676617,
      "grad_norm": 44.778621673583984,
      "learning_rate": 9.565298507462686e-08,
      "loss": 3.126,
      "step": 700
    },
    {
      "epoch": 3.5323383084577116,
      "grad_norm": 25.949298858642578,
      "learning_rate": 9.559079601990048e-08,
      "loss": 3.0904,
      "step": 710
    },
    {
      "epoch": 3.582089552238806,
      "grad_norm": 20.120065689086914,
      "learning_rate": 9.552860696517413e-08,
      "loss": 3.1141,
      "step": 720
    },
    {
      "epoch": 3.6318407960199006,
      "grad_norm": 23.51906967163086,
      "learning_rate": 9.546641791044776e-08,
      "loss": 3.0032,
      "step": 730
    },
    {
      "epoch": 3.681592039800995,
      "grad_norm": 14.044002532958984,
      "learning_rate": 9.540422885572139e-08,
      "loss": 3.081,
      "step": 740
    },
    {
      "epoch": 3.7313432835820897,
      "grad_norm": 17.48064422607422,
      "learning_rate": 9.534203980099502e-08,
      "loss": 3.0302,
      "step": 750
    },
    {
      "epoch": 3.781094527363184,
      "grad_norm": 22.05286979675293,
      "learning_rate": 9.527985074626865e-08,
      "loss": 3.1471,
      "step": 760
    },
    {
      "epoch": 3.8308457711442787,
      "grad_norm": 29.49858856201172,
      "learning_rate": 9.521766169154229e-08,
      "loss": 2.9661,
      "step": 770
    },
    {
      "epoch": 3.8805970149253732,
      "grad_norm": 36.43048095703125,
      "learning_rate": 9.515547263681591e-08,
      "loss": 3.08,
      "step": 780
    },
    {
      "epoch": 3.9303482587064678,
      "grad_norm": 26.66364097595215,
      "learning_rate": 9.509328358208955e-08,
      "loss": 3.0626,
      "step": 790
    },
    {
      "epoch": 3.9800995024875623,
      "grad_norm": 11.592057228088379,
      "learning_rate": 9.503109452736317e-08,
      "loss": 3.2582,
      "step": 800
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.9172852039337158,
      "eval_runtime": 54.6596,
      "eval_samples_per_second": 3.677,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.648939588688946,
      "step": 804
    },
    {
      "epoch": 4.029850746268656,
      "grad_norm": 34.679325103759766,
      "learning_rate": 9.496890547263681e-08,
      "loss": 3.0648,
      "step": 810
    },
    {
      "epoch": 4.079601990049751,
      "grad_norm": 10.261878967285156,
      "learning_rate": 9.490671641791045e-08,
      "loss": 2.9893,
      "step": 820
    },
    {
      "epoch": 4.129353233830845,
      "grad_norm": 13.54428768157959,
      "learning_rate": 9.484452736318408e-08,
      "loss": 3.1328,
      "step": 830
    },
    {
      "epoch": 4.17910447761194,
      "grad_norm": 27.156063079833984,
      "learning_rate": 9.47823383084577e-08,
      "loss": 3.0971,
      "step": 840
    },
    {
      "epoch": 4.2288557213930345,
      "grad_norm": 9.543243408203125,
      "learning_rate": 9.472014925373134e-08,
      "loss": 3.0237,
      "step": 850
    },
    {
      "epoch": 4.278606965174129,
      "grad_norm": 14.0496244430542,
      "learning_rate": 9.465796019900498e-08,
      "loss": 2.9671,
      "step": 860
    },
    {
      "epoch": 4.3283582089552235,
      "grad_norm": 11.95229434967041,
      "learning_rate": 9.45957711442786e-08,
      "loss": 2.9945,
      "step": 870
    },
    {
      "epoch": 4.378109452736318,
      "grad_norm": 47.22742462158203,
      "learning_rate": 9.453358208955224e-08,
      "loss": 3.0611,
      "step": 880
    },
    {
      "epoch": 4.4278606965174125,
      "grad_norm": 25.670747756958008,
      "learning_rate": 9.447139303482586e-08,
      "loss": 2.9687,
      "step": 890
    },
    {
      "epoch": 4.477611940298507,
      "grad_norm": 14.060393333435059,
      "learning_rate": 9.44092039800995e-08,
      "loss": 2.9921,
      "step": 900
    },
    {
      "epoch": 4.5273631840796025,
      "grad_norm": 44.872344970703125,
      "learning_rate": 9.434701492537312e-08,
      "loss": 3.0508,
      "step": 910
    },
    {
      "epoch": 4.577114427860696,
      "grad_norm": 15.964935302734375,
      "learning_rate": 9.428482587064677e-08,
      "loss": 3.0439,
      "step": 920
    },
    {
      "epoch": 4.6268656716417915,
      "grad_norm": 19.177242279052734,
      "learning_rate": 9.42226368159204e-08,
      "loss": 3.0507,
      "step": 930
    },
    {
      "epoch": 4.676616915422885,
      "grad_norm": 23.849905014038086,
      "learning_rate": 9.416044776119403e-08,
      "loss": 2.9649,
      "step": 940
    },
    {
      "epoch": 4.726368159203981,
      "grad_norm": 12.14055061340332,
      "learning_rate": 9.409825870646765e-08,
      "loss": 3.211,
      "step": 950
    },
    {
      "epoch": 4.776119402985074,
      "grad_norm": 30.271059036254883,
      "learning_rate": 9.403606965174129e-08,
      "loss": 3.0586,
      "step": 960
    },
    {
      "epoch": 4.82587064676617,
      "grad_norm": 25.349117279052734,
      "learning_rate": 9.397388059701493e-08,
      "loss": 3.1126,
      "step": 970
    },
    {
      "epoch": 4.875621890547263,
      "grad_norm": 38.92596435546875,
      "learning_rate": 9.391169154228855e-08,
      "loss": 3.0635,
      "step": 980
    },
    {
      "epoch": 4.925373134328359,
      "grad_norm": 50.357421875,
      "learning_rate": 9.384950248756219e-08,
      "loss": 3.0209,
      "step": 990
    },
    {
      "epoch": 4.975124378109452,
      "grad_norm": 10.041421890258789,
      "learning_rate": 9.378731343283581e-08,
      "loss": 3.0556,
      "step": 1000
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.9042221307754517,
      "eval_runtime": 55.3178,
      "eval_samples_per_second": 3.634,
      "eval_steps_per_second": 0.235,
      "eval_wer": 0.6476542416452442,
      "step": 1005
    },
    {
      "epoch": 5.024875621890548,
      "grad_norm": 29.511476516723633,
      "learning_rate": 9.372512437810945e-08,
      "loss": 2.9276,
      "step": 1010
    },
    {
      "epoch": 5.074626865671641,
      "grad_norm": 21.41852569580078,
      "learning_rate": 9.366293532338308e-08,
      "loss": 2.9591,
      "step": 1020
    },
    {
      "epoch": 5.124378109452737,
      "grad_norm": 35.19390106201172,
      "learning_rate": 9.360074626865672e-08,
      "loss": 2.9494,
      "step": 1030
    },
    {
      "epoch": 5.174129353233831,
      "grad_norm": 67.26006317138672,
      "learning_rate": 9.353855721393034e-08,
      "loss": 3.0255,
      "step": 1040
    },
    {
      "epoch": 5.223880597014926,
      "grad_norm": 35.43225860595703,
      "learning_rate": 9.347636815920398e-08,
      "loss": 3.0539,
      "step": 1050
    },
    {
      "epoch": 5.273631840796019,
      "grad_norm": 12.110766410827637,
      "learning_rate": 9.34141791044776e-08,
      "loss": 3.0271,
      "step": 1060
    },
    {
      "epoch": 5.323383084577115,
      "grad_norm": 19.332408905029297,
      "learning_rate": 9.335199004975124e-08,
      "loss": 2.9983,
      "step": 1070
    },
    {
      "epoch": 5.373134328358209,
      "grad_norm": 32.275177001953125,
      "learning_rate": 9.328980099502488e-08,
      "loss": 2.9543,
      "step": 1080
    },
    {
      "epoch": 5.422885572139304,
      "grad_norm": 12.641383171081543,
      "learning_rate": 9.32276119402985e-08,
      "loss": 2.9761,
      "step": 1090
    },
    {
      "epoch": 5.472636815920398,
      "grad_norm": 25.462278366088867,
      "learning_rate": 9.316542288557214e-08,
      "loss": 3.0125,
      "step": 1100
    },
    {
      "epoch": 5.522388059701493,
      "grad_norm": 27.42340087890625,
      "learning_rate": 9.310323383084576e-08,
      "loss": 2.9577,
      "step": 1110
    },
    {
      "epoch": 5.572139303482587,
      "grad_norm": 26.82093620300293,
      "learning_rate": 9.304104477611941e-08,
      "loss": 2.8612,
      "step": 1120
    },
    {
      "epoch": 5.621890547263682,
      "grad_norm": 8.497031211853027,
      "learning_rate": 9.297885572139303e-08,
      "loss": 3.0982,
      "step": 1130
    },
    {
      "epoch": 5.6716417910447765,
      "grad_norm": 11.691890716552734,
      "learning_rate": 9.291666666666667e-08,
      "loss": 2.8644,
      "step": 1140
    },
    {
      "epoch": 5.721393034825871,
      "grad_norm": 19.985288619995117,
      "learning_rate": 9.285447761194029e-08,
      "loss": 2.9815,
      "step": 1150
    },
    {
      "epoch": 5.7711442786069655,
      "grad_norm": 14.271354675292969,
      "learning_rate": 9.279228855721393e-08,
      "loss": 2.929,
      "step": 1160
    },
    {
      "epoch": 5.82089552238806,
      "grad_norm": 56.41835021972656,
      "learning_rate": 9.273009950248755e-08,
      "loss": 2.9936,
      "step": 1170
    },
    {
      "epoch": 5.870646766169155,
      "grad_norm": 33.35626983642578,
      "learning_rate": 9.266791044776119e-08,
      "loss": 2.9992,
      "step": 1180
    },
    {
      "epoch": 5.920398009950249,
      "grad_norm": 58.28657913208008,
      "learning_rate": 9.260572139303482e-08,
      "loss": 3.0629,
      "step": 1190
    },
    {
      "epoch": 5.970149253731344,
      "grad_norm": 25.092761993408203,
      "learning_rate": 9.254353233830845e-08,
      "loss": 3.0159,
      "step": 1200
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.8916476964950562,
      "eval_runtime": 54.7544,
      "eval_samples_per_second": 3.671,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6490199228791774,
      "step": 1206
    },
    {
      "epoch": 6.019900497512438,
      "grad_norm": 36.248207092285156,
      "learning_rate": 9.248134328358208e-08,
      "loss": 3.0341,
      "step": 1210
    },
    {
      "epoch": 6.069651741293533,
      "grad_norm": 46.627742767333984,
      "learning_rate": 9.241915422885572e-08,
      "loss": 2.9991,
      "step": 1220
    },
    {
      "epoch": 6.119402985074627,
      "grad_norm": 29.162826538085938,
      "learning_rate": 9.235696517412936e-08,
      "loss": 2.902,
      "step": 1230
    },
    {
      "epoch": 6.169154228855722,
      "grad_norm": 8.449566841125488,
      "learning_rate": 9.229477611940298e-08,
      "loss": 2.9925,
      "step": 1240
    },
    {
      "epoch": 6.218905472636816,
      "grad_norm": 12.014678001403809,
      "learning_rate": 9.223258706467662e-08,
      "loss": 2.8733,
      "step": 1250
    },
    {
      "epoch": 6.268656716417911,
      "grad_norm": 21.945690155029297,
      "learning_rate": 9.217039800995024e-08,
      "loss": 2.968,
      "step": 1260
    },
    {
      "epoch": 6.318407960199005,
      "grad_norm": 9.693615913391113,
      "learning_rate": 9.210820895522388e-08,
      "loss": 2.9834,
      "step": 1270
    },
    {
      "epoch": 6.3681592039801,
      "grad_norm": 31.624420166015625,
      "learning_rate": 9.204601990049751e-08,
      "loss": 2.97,
      "step": 1280
    },
    {
      "epoch": 6.417910447761194,
      "grad_norm": 12.592489242553711,
      "learning_rate": 9.198383084577114e-08,
      "loss": 2.9767,
      "step": 1290
    },
    {
      "epoch": 6.467661691542289,
      "grad_norm": 16.8156681060791,
      "learning_rate": 9.192164179104477e-08,
      "loss": 2.9252,
      "step": 1300
    },
    {
      "epoch": 6.517412935323383,
      "grad_norm": 27.86859703063965,
      "learning_rate": 9.18594527363184e-08,
      "loss": 3.0633,
      "step": 1310
    },
    {
      "epoch": 6.567164179104478,
      "grad_norm": 39.3943977355957,
      "learning_rate": 9.179726368159205e-08,
      "loss": 2.9204,
      "step": 1320
    },
    {
      "epoch": 6.616915422885572,
      "grad_norm": 15.897247314453125,
      "learning_rate": 9.173507462686567e-08,
      "loss": 3.079,
      "step": 1330
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 19.42824935913086,
      "learning_rate": 9.16728855721393e-08,
      "loss": 2.9344,
      "step": 1340
    },
    {
      "epoch": 6.7164179104477615,
      "grad_norm": 22.017024993896484,
      "learning_rate": 9.161069651741293e-08,
      "loss": 2.923,
      "step": 1350
    },
    {
      "epoch": 6.766169154228856,
      "grad_norm": 35.0894660949707,
      "learning_rate": 9.154850746268657e-08,
      "loss": 3.0123,
      "step": 1360
    },
    {
      "epoch": 6.8159203980099505,
      "grad_norm": 9.844803810119629,
      "learning_rate": 9.148631840796019e-08,
      "loss": 2.8572,
      "step": 1370
    },
    {
      "epoch": 6.865671641791045,
      "grad_norm": 14.600975036621094,
      "learning_rate": 9.142412935323383e-08,
      "loss": 2.826,
      "step": 1380
    },
    {
      "epoch": 6.91542288557214,
      "grad_norm": 16.12067985534668,
      "learning_rate": 9.136194029850746e-08,
      "loss": 2.9042,
      "step": 1390
    },
    {
      "epoch": 6.965174129353234,
      "grad_norm": 17.590526580810547,
      "learning_rate": 9.129975124378109e-08,
      "loss": 2.8192,
      "step": 1400
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.8785794973373413,
      "eval_runtime": 55.117,
      "eval_samples_per_second": 3.647,
      "eval_steps_per_second": 0.236,
      "eval_wer": 0.6487789203084833,
      "step": 1407
    },
    {
      "epoch": 7.014925373134329,
      "grad_norm": 11.743035316467285,
      "learning_rate": 9.123756218905472e-08,
      "loss": 2.929,
      "step": 1410
    },
    {
      "epoch": 7.064676616915423,
      "grad_norm": 23.59311866760254,
      "learning_rate": 9.117537313432836e-08,
      "loss": 3.0115,
      "step": 1420
    },
    {
      "epoch": 7.114427860696518,
      "grad_norm": 36.39549255371094,
      "learning_rate": 9.1113184079602e-08,
      "loss": 2.8187,
      "step": 1430
    },
    {
      "epoch": 7.164179104477612,
      "grad_norm": 40.04682540893555,
      "learning_rate": 9.105099502487562e-08,
      "loss": 2.9002,
      "step": 1440
    },
    {
      "epoch": 7.213930348258707,
      "grad_norm": 29.616771697998047,
      "learning_rate": 9.098880597014925e-08,
      "loss": 2.9377,
      "step": 1450
    },
    {
      "epoch": 7.263681592039801,
      "grad_norm": 25.342334747314453,
      "learning_rate": 9.092661691542288e-08,
      "loss": 3.0183,
      "step": 1460
    },
    {
      "epoch": 7.313432835820896,
      "grad_norm": 9.908034324645996,
      "learning_rate": 9.086442786069651e-08,
      "loss": 2.8377,
      "step": 1470
    },
    {
      "epoch": 7.36318407960199,
      "grad_norm": 16.774641036987305,
      "learning_rate": 9.080223880597014e-08,
      "loss": 2.8489,
      "step": 1480
    },
    {
      "epoch": 7.412935323383085,
      "grad_norm": 28.89239501953125,
      "learning_rate": 9.074004975124377e-08,
      "loss": 2.8891,
      "step": 1490
    },
    {
      "epoch": 7.462686567164179,
      "grad_norm": 58.11760330200195,
      "learning_rate": 9.067786069651741e-08,
      "loss": 2.8882,
      "step": 1500
    },
    {
      "epoch": 7.512437810945274,
      "grad_norm": 22.358165740966797,
      "learning_rate": 9.061567164179103e-08,
      "loss": 2.8497,
      "step": 1510
    },
    {
      "epoch": 7.562189054726368,
      "grad_norm": 41.861873626708984,
      "learning_rate": 9.055348258706468e-08,
      "loss": 2.922,
      "step": 1520
    },
    {
      "epoch": 7.611940298507463,
      "grad_norm": 11.754576683044434,
      "learning_rate": 9.049129353233831e-08,
      "loss": 3.1104,
      "step": 1530
    },
    {
      "epoch": 7.661691542288557,
      "grad_norm": 30.035001754760742,
      "learning_rate": 9.042910447761194e-08,
      "loss": 2.9284,
      "step": 1540
    },
    {
      "epoch": 7.711442786069652,
      "grad_norm": 46.51190948486328,
      "learning_rate": 9.036691542288557e-08,
      "loss": 2.8866,
      "step": 1550
    },
    {
      "epoch": 7.7611940298507465,
      "grad_norm": 14.139452934265137,
      "learning_rate": 9.03047263681592e-08,
      "loss": 2.8868,
      "step": 1560
    },
    {
      "epoch": 7.810945273631841,
      "grad_norm": 14.141725540161133,
      "learning_rate": 9.024253731343283e-08,
      "loss": 2.8853,
      "step": 1570
    },
    {
      "epoch": 7.8606965174129355,
      "grad_norm": 19.336048126220703,
      "learning_rate": 9.018034825870646e-08,
      "loss": 2.8843,
      "step": 1580
    },
    {
      "epoch": 7.91044776119403,
      "grad_norm": 55.934288024902344,
      "learning_rate": 9.011815920398009e-08,
      "loss": 2.8614,
      "step": 1590
    },
    {
      "epoch": 7.960199004975125,
      "grad_norm": 14.65905475616455,
      "learning_rate": 9.005597014925372e-08,
      "loss": 2.9559,
      "step": 1600
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.8698697090148926,
      "eval_runtime": 54.1642,
      "eval_samples_per_second": 3.711,
      "eval_steps_per_second": 0.24,
      "eval_wer": 0.6486182519280206,
      "step": 1608
    },
    {
      "epoch": 8.009950248756219,
      "grad_norm": 20.289518356323242,
      "learning_rate": 8.999378109452736e-08,
      "loss": 2.904,
      "step": 1610
    },
    {
      "epoch": 8.059701492537313,
      "grad_norm": 18.91230583190918,
      "learning_rate": 8.9931592039801e-08,
      "loss": 2.8274,
      "step": 1620
    },
    {
      "epoch": 8.109452736318408,
      "grad_norm": 14.062386512756348,
      "learning_rate": 8.986940298507463e-08,
      "loss": 2.914,
      "step": 1630
    },
    {
      "epoch": 8.159203980099502,
      "grad_norm": 21.285778045654297,
      "learning_rate": 8.980721393034826e-08,
      "loss": 2.8164,
      "step": 1640
    },
    {
      "epoch": 8.208955223880597,
      "grad_norm": 9.038252830505371,
      "learning_rate": 8.974502487562189e-08,
      "loss": 2.8402,
      "step": 1650
    },
    {
      "epoch": 8.25870646766169,
      "grad_norm": 23.477052688598633,
      "learning_rate": 8.968283582089552e-08,
      "loss": 2.8534,
      "step": 1660
    },
    {
      "epoch": 8.308457711442786,
      "grad_norm": 19.973278045654297,
      "learning_rate": 8.962064676616915e-08,
      "loss": 2.7952,
      "step": 1670
    },
    {
      "epoch": 8.35820895522388,
      "grad_norm": 30.830739974975586,
      "learning_rate": 8.955845771144278e-08,
      "loss": 2.799,
      "step": 1680
    },
    {
      "epoch": 8.407960199004975,
      "grad_norm": 9.952239990234375,
      "learning_rate": 8.949626865671641e-08,
      "loss": 2.951,
      "step": 1690
    },
    {
      "epoch": 8.457711442786069,
      "grad_norm": 16.07948112487793,
      "learning_rate": 8.943407960199005e-08,
      "loss": 2.8575,
      "step": 1700
    },
    {
      "epoch": 8.507462686567164,
      "grad_norm": 110.9641342163086,
      "learning_rate": 8.937189054726367e-08,
      "loss": 2.9282,
      "step": 1710
    },
    {
      "epoch": 8.557213930348258,
      "grad_norm": 15.530111312866211,
      "learning_rate": 8.930970149253731e-08,
      "loss": 2.9492,
      "step": 1720
    },
    {
      "epoch": 8.606965174129353,
      "grad_norm": 23.03366470336914,
      "learning_rate": 8.924751243781094e-08,
      "loss": 2.7975,
      "step": 1730
    },
    {
      "epoch": 8.656716417910447,
      "grad_norm": 36.17377853393555,
      "learning_rate": 8.918532338308458e-08,
      "loss": 2.8486,
      "step": 1740
    },
    {
      "epoch": 8.706467661691542,
      "grad_norm": 15.830889701843262,
      "learning_rate": 8.91231343283582e-08,
      "loss": 2.8999,
      "step": 1750
    },
    {
      "epoch": 8.756218905472636,
      "grad_norm": 11.994380950927734,
      "learning_rate": 8.906094527363184e-08,
      "loss": 2.85,
      "step": 1760
    },
    {
      "epoch": 8.805970149253731,
      "grad_norm": 11.581160545349121,
      "learning_rate": 8.899875621890546e-08,
      "loss": 2.9053,
      "step": 1770
    },
    {
      "epoch": 8.855721393034825,
      "grad_norm": 15.451041221618652,
      "learning_rate": 8.89365671641791e-08,
      "loss": 2.718,
      "step": 1780
    },
    {
      "epoch": 8.90547263681592,
      "grad_norm": 10.136153221130371,
      "learning_rate": 8.887437810945272e-08,
      "loss": 2.8366,
      "step": 1790
    },
    {
      "epoch": 8.955223880597014,
      "grad_norm": 16.584659576416016,
      "learning_rate": 8.881218905472636e-08,
      "loss": 2.7781,
      "step": 1800
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.8637783527374268,
      "eval_runtime": 54.478,
      "eval_samples_per_second": 3.69,
      "eval_steps_per_second": 0.239,
      "eval_wer": 0.6481362467866324,
      "step": 1809
    },
    {
      "epoch": 9.00497512437811,
      "grad_norm": 18.36239242553711,
      "learning_rate": 8.875e-08,
      "loss": 2.7975,
      "step": 1810
    },
    {
      "epoch": 9.054726368159203,
      "grad_norm": 13.045573234558105,
      "learning_rate": 8.868781094527362e-08,
      "loss": 2.8554,
      "step": 1820
    },
    {
      "epoch": 9.104477611940299,
      "grad_norm": 20.98431968688965,
      "learning_rate": 8.862562189054727e-08,
      "loss": 2.8223,
      "step": 1830
    },
    {
      "epoch": 9.154228855721392,
      "grad_norm": 8.474494934082031,
      "learning_rate": 8.85634328358209e-08,
      "loss": 2.8373,
      "step": 1840
    },
    {
      "epoch": 9.203980099502488,
      "grad_norm": 32.05399703979492,
      "learning_rate": 8.850124378109453e-08,
      "loss": 2.9647,
      "step": 1850
    },
    {
      "epoch": 9.253731343283581,
      "grad_norm": 20.61164665222168,
      "learning_rate": 8.843905472636815e-08,
      "loss": 2.7951,
      "step": 1860
    },
    {
      "epoch": 9.303482587064677,
      "grad_norm": 19.98482322692871,
      "learning_rate": 8.837686567164179e-08,
      "loss": 2.8453,
      "step": 1870
    },
    {
      "epoch": 9.35323383084577,
      "grad_norm": 75.70906829833984,
      "learning_rate": 8.831467661691541e-08,
      "loss": 2.9614,
      "step": 1880
    },
    {
      "epoch": 9.402985074626866,
      "grad_norm": 14.056557655334473,
      "learning_rate": 8.825248756218905e-08,
      "loss": 2.7348,
      "step": 1890
    },
    {
      "epoch": 9.45273631840796,
      "grad_norm": 16.19362449645996,
      "learning_rate": 8.819029850746267e-08,
      "loss": 2.8518,
      "step": 1900
    },
    {
      "epoch": 9.502487562189055,
      "grad_norm": 9.416305541992188,
      "learning_rate": 8.812810945273631e-08,
      "loss": 2.8281,
      "step": 1910
    },
    {
      "epoch": 9.552238805970148,
      "grad_norm": 15.272223472595215,
      "learning_rate": 8.806592039800995e-08,
      "loss": 2.8421,
      "step": 1920
    },
    {
      "epoch": 9.601990049751244,
      "grad_norm": 27.36576271057129,
      "learning_rate": 8.800373134328358e-08,
      "loss": 2.848,
      "step": 1930
    },
    {
      "epoch": 9.65174129353234,
      "grad_norm": 11.80379867553711,
      "learning_rate": 8.794154228855722e-08,
      "loss": 2.8467,
      "step": 1940
    },
    {
      "epoch": 9.701492537313433,
      "grad_norm": 18.217376708984375,
      "learning_rate": 8.787935323383084e-08,
      "loss": 2.8769,
      "step": 1950
    },
    {
      "epoch": 9.751243781094526,
      "grad_norm": 9.364083290100098,
      "learning_rate": 8.781716417910448e-08,
      "loss": 2.8599,
      "step": 1960
    },
    {
      "epoch": 9.800995024875622,
      "grad_norm": 8.150951385498047,
      "learning_rate": 8.77549751243781e-08,
      "loss": 2.7882,
      "step": 1970
    },
    {
      "epoch": 9.850746268656717,
      "grad_norm": 24.42496681213379,
      "learning_rate": 8.769278606965174e-08,
      "loss": 2.8273,
      "step": 1980
    },
    {
      "epoch": 9.900497512437811,
      "grad_norm": 13.766109466552734,
      "learning_rate": 8.763059701492536e-08,
      "loss": 2.8639,
      "step": 1990
    },
    {
      "epoch": 9.950248756218905,
      "grad_norm": 13.325282096862793,
      "learning_rate": 8.7568407960199e-08,
      "loss": 2.8222,
      "step": 2000
    },
    {
      "epoch": 10.0,
      "grad_norm": 65.98360443115234,
      "learning_rate": 8.750621890547264e-08,
      "loss": 2.9125,
      "step": 2010
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.8542158603668213,
      "eval_runtime": 56.06,
      "eval_samples_per_second": 3.585,
      "eval_steps_per_second": 0.232,
      "eval_wer": 0.6473329048843187,
      "step": 2010
    },
    {
      "epoch": 10.049751243781095,
      "grad_norm": 10.841636657714844,
      "learning_rate": 8.744402985074626e-08,
      "loss": 2.7487,
      "step": 2020
    },
    {
      "epoch": 10.099502487562189,
      "grad_norm": 14.26783275604248,
      "learning_rate": 8.73818407960199e-08,
      "loss": 2.8688,
      "step": 2030
    },
    {
      "epoch": 10.149253731343283,
      "grad_norm": 9.670403480529785,
      "learning_rate": 8.731965174129353e-08,
      "loss": 2.7876,
      "step": 2040
    },
    {
      "epoch": 10.199004975124378,
      "grad_norm": 19.09012222290039,
      "learning_rate": 8.725746268656717e-08,
      "loss": 2.7675,
      "step": 2050
    },
    {
      "epoch": 10.248756218905474,
      "grad_norm": 23.491981506347656,
      "learning_rate": 8.719527363184079e-08,
      "loss": 2.8689,
      "step": 2060
    },
    {
      "epoch": 10.298507462686567,
      "grad_norm": 16.866426467895508,
      "learning_rate": 8.713308457711443e-08,
      "loss": 2.8073,
      "step": 2070
    },
    {
      "epoch": 10.348258706467663,
      "grad_norm": 11.320743560791016,
      "learning_rate": 8.707089552238805e-08,
      "loss": 2.813,
      "step": 2080
    },
    {
      "epoch": 10.398009950248756,
      "grad_norm": 21.0335636138916,
      "learning_rate": 8.700870646766169e-08,
      "loss": 2.8165,
      "step": 2090
    },
    {
      "epoch": 10.447761194029852,
      "grad_norm": 18.237953186035156,
      "learning_rate": 8.694651741293531e-08,
      "loss": 2.8092,
      "step": 2100
    },
    {
      "epoch": 10.497512437810945,
      "grad_norm": 10.954788208007812,
      "learning_rate": 8.688432835820895e-08,
      "loss": 2.8324,
      "step": 2110
    },
    {
      "epoch": 10.547263681592039,
      "grad_norm": 15.507596015930176,
      "learning_rate": 8.682213930348258e-08,
      "loss": 2.6067,
      "step": 2120
    },
    {
      "epoch": 10.597014925373134,
      "grad_norm": 15.47692584991455,
      "learning_rate": 8.675995024875622e-08,
      "loss": 2.8664,
      "step": 2130
    },
    {
      "epoch": 10.64676616915423,
      "grad_norm": 16.980791091918945,
      "learning_rate": 8.669776119402986e-08,
      "loss": 2.9026,
      "step": 2140
    },
    {
      "epoch": 10.696517412935323,
      "grad_norm": 10.095736503601074,
      "learning_rate": 8.663557213930348e-08,
      "loss": 2.8158,
      "step": 2150
    },
    {
      "epoch": 10.746268656716419,
      "grad_norm": 23.174922943115234,
      "learning_rate": 8.657338308457712e-08,
      "loss": 2.8715,
      "step": 2160
    },
    {
      "epoch": 10.796019900497512,
      "grad_norm": 23.191822052001953,
      "learning_rate": 8.651119402985074e-08,
      "loss": 2.7927,
      "step": 2170
    },
    {
      "epoch": 10.845771144278608,
      "grad_norm": 40.5777702331543,
      "learning_rate": 8.644900497512438e-08,
      "loss": 2.765,
      "step": 2180
    },
    {
      "epoch": 10.895522388059701,
      "grad_norm": 14.8229341506958,
      "learning_rate": 8.6386815920398e-08,
      "loss": 2.89,
      "step": 2190
    },
    {
      "epoch": 10.945273631840797,
      "grad_norm": 10.143345832824707,
      "learning_rate": 8.632462686567164e-08,
      "loss": 2.765,
      "step": 2200
    },
    {
      "epoch": 10.99502487562189,
      "grad_norm": 11.138708114624023,
      "learning_rate": 8.626243781094526e-08,
      "loss": 2.7109,
      "step": 2210
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.8495728969573975,
      "eval_runtime": 55.0265,
      "eval_samples_per_second": 3.653,
      "eval_steps_per_second": 0.236,
      "eval_wer": 0.6462885604113111,
      "step": 2211
    },
    {
      "epoch": 11.044776119402986,
      "grad_norm": 14.056020736694336,
      "learning_rate": 8.62002487562189e-08,
      "loss": 2.7588,
      "step": 2220
    },
    {
      "epoch": 11.09452736318408,
      "grad_norm": 10.362235069274902,
      "learning_rate": 8.613805970149253e-08,
      "loss": 2.8134,
      "step": 2230
    },
    {
      "epoch": 11.144278606965175,
      "grad_norm": 36.2872314453125,
      "learning_rate": 8.607587064676617e-08,
      "loss": 2.8948,
      "step": 2240
    },
    {
      "epoch": 11.194029850746269,
      "grad_norm": 8.405220031738281,
      "learning_rate": 8.60136815920398e-08,
      "loss": 2.9326,
      "step": 2250
    },
    {
      "epoch": 11.243781094527364,
      "grad_norm": 29.920900344848633,
      "learning_rate": 8.595149253731343e-08,
      "loss": 2.8508,
      "step": 2260
    },
    {
      "epoch": 11.293532338308458,
      "grad_norm": 11.703344345092773,
      "learning_rate": 8.588930348258707e-08,
      "loss": 2.7373,
      "step": 2270
    },
    {
      "epoch": 11.343283582089553,
      "grad_norm": 21.277074813842773,
      "learning_rate": 8.582711442786069e-08,
      "loss": 2.7939,
      "step": 2280
    },
    {
      "epoch": 11.393034825870647,
      "grad_norm": 13.463208198547363,
      "learning_rate": 8.576492537313433e-08,
      "loss": 2.83,
      "step": 2290
    },
    {
      "epoch": 11.442786069651742,
      "grad_norm": 17.690372467041016,
      "learning_rate": 8.570273631840795e-08,
      "loss": 2.817,
      "step": 2300
    },
    {
      "epoch": 11.492537313432836,
      "grad_norm": 17.860116958618164,
      "learning_rate": 8.564054726368159e-08,
      "loss": 2.7291,
      "step": 2310
    },
    {
      "epoch": 11.542288557213931,
      "grad_norm": 20.364076614379883,
      "learning_rate": 8.557835820895521e-08,
      "loss": 2.7947,
      "step": 2320
    },
    {
      "epoch": 11.592039800995025,
      "grad_norm": 10.832269668579102,
      "learning_rate": 8.551616915422886e-08,
      "loss": 2.8388,
      "step": 2330
    },
    {
      "epoch": 11.64179104477612,
      "grad_norm": 25.716693878173828,
      "learning_rate": 8.545398009950248e-08,
      "loss": 2.7404,
      "step": 2340
    },
    {
      "epoch": 11.691542288557214,
      "grad_norm": 9.687882423400879,
      "learning_rate": 8.539179104477612e-08,
      "loss": 2.7286,
      "step": 2350
    },
    {
      "epoch": 11.74129353233831,
      "grad_norm": 10.403105735778809,
      "learning_rate": 8.532960199004975e-08,
      "loss": 2.7117,
      "step": 2360
    },
    {
      "epoch": 11.791044776119403,
      "grad_norm": 102.30384063720703,
      "learning_rate": 8.526741293532338e-08,
      "loss": 2.8077,
      "step": 2370
    },
    {
      "epoch": 11.840796019900498,
      "grad_norm": 25.61990737915039,
      "learning_rate": 8.520522388059701e-08,
      "loss": 2.7139,
      "step": 2380
    },
    {
      "epoch": 11.890547263681592,
      "grad_norm": 15.271239280700684,
      "learning_rate": 8.514303482587064e-08,
      "loss": 2.7482,
      "step": 2390
    },
    {
      "epoch": 11.940298507462687,
      "grad_norm": 16.923686981201172,
      "learning_rate": 8.508084577114427e-08,
      "loss": 2.7935,
      "step": 2400
    },
    {
      "epoch": 11.990049751243781,
      "grad_norm": 30.989599227905273,
      "learning_rate": 8.50186567164179e-08,
      "loss": 2.8395,
      "step": 2410
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.8423733711242676,
      "eval_runtime": 54.5598,
      "eval_samples_per_second": 3.684,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.644280205655527,
      "step": 2412
    },
    {
      "epoch": 12.039800995024876,
      "grad_norm": 16.595073699951172,
      "learning_rate": 8.495646766169153e-08,
      "loss": 2.7631,
      "step": 2420
    },
    {
      "epoch": 12.08955223880597,
      "grad_norm": 12.15278148651123,
      "learning_rate": 8.489427860696517e-08,
      "loss": 2.6939,
      "step": 2430
    },
    {
      "epoch": 12.139303482587065,
      "grad_norm": 25.80365753173828,
      "learning_rate": 8.483208955223881e-08,
      "loss": 2.8985,
      "step": 2440
    },
    {
      "epoch": 12.189054726368159,
      "grad_norm": 11.227471351623535,
      "learning_rate": 8.476990049751243e-08,
      "loss": 2.735,
      "step": 2450
    },
    {
      "epoch": 12.238805970149254,
      "grad_norm": 10.587419509887695,
      "learning_rate": 8.470771144278607e-08,
      "loss": 2.6953,
      "step": 2460
    },
    {
      "epoch": 12.288557213930348,
      "grad_norm": 18.992671966552734,
      "learning_rate": 8.46455223880597e-08,
      "loss": 2.7962,
      "step": 2470
    },
    {
      "epoch": 12.338308457711443,
      "grad_norm": 25.72074317932129,
      "learning_rate": 8.458333333333333e-08,
      "loss": 2.7936,
      "step": 2480
    },
    {
      "epoch": 12.388059701492537,
      "grad_norm": 17.57948112487793,
      "learning_rate": 8.452114427860696e-08,
      "loss": 2.8276,
      "step": 2490
    },
    {
      "epoch": 12.437810945273633,
      "grad_norm": 14.893308639526367,
      "learning_rate": 8.445895522388059e-08,
      "loss": 2.7851,
      "step": 2500
    },
    {
      "epoch": 12.487562189054726,
      "grad_norm": 19.531227111816406,
      "learning_rate": 8.439676616915422e-08,
      "loss": 2.7454,
      "step": 2510
    },
    {
      "epoch": 12.537313432835822,
      "grad_norm": 9.653219223022461,
      "learning_rate": 8.433457711442785e-08,
      "loss": 2.7477,
      "step": 2520
    },
    {
      "epoch": 12.587064676616915,
      "grad_norm": 11.20486068725586,
      "learning_rate": 8.42723880597015e-08,
      "loss": 2.7763,
      "step": 2530
    },
    {
      "epoch": 12.63681592039801,
      "grad_norm": 20.651714324951172,
      "learning_rate": 8.421019900497512e-08,
      "loss": 2.7563,
      "step": 2540
    },
    {
      "epoch": 12.686567164179104,
      "grad_norm": 16.771394729614258,
      "learning_rate": 8.414800995024876e-08,
      "loss": 2.7529,
      "step": 2550
    },
    {
      "epoch": 12.7363184079602,
      "grad_norm": 13.57751178741455,
      "learning_rate": 8.408582089552239e-08,
      "loss": 2.7223,
      "step": 2560
    },
    {
      "epoch": 12.786069651741293,
      "grad_norm": 12.066554069519043,
      "learning_rate": 8.402363184079602e-08,
      "loss": 2.8011,
      "step": 2570
    },
    {
      "epoch": 12.835820895522389,
      "grad_norm": 16.356164932250977,
      "learning_rate": 8.396144278606965e-08,
      "loss": 2.749,
      "step": 2580
    },
    {
      "epoch": 12.885572139303482,
      "grad_norm": 12.453943252563477,
      "learning_rate": 8.389925373134328e-08,
      "loss": 2.7376,
      "step": 2590
    },
    {
      "epoch": 12.935323383084578,
      "grad_norm": 16.511383056640625,
      "learning_rate": 8.383706467661691e-08,
      "loss": 2.7485,
      "step": 2600
    },
    {
      "epoch": 12.985074626865671,
      "grad_norm": 14.05982780456543,
      "learning_rate": 8.377487562189053e-08,
      "loss": 2.7503,
      "step": 2610
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.8405011892318726,
      "eval_runtime": 54.7886,
      "eval_samples_per_second": 3.669,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.643155526992288,
      "step": 2613
    },
    {
      "epoch": 13.034825870646767,
      "grad_norm": 16.277889251708984,
      "learning_rate": 8.371268656716417e-08,
      "loss": 2.8003,
      "step": 2620
    },
    {
      "epoch": 13.08457711442786,
      "grad_norm": 17.767255783081055,
      "learning_rate": 8.365049751243781e-08,
      "loss": 2.8049,
      "step": 2630
    },
    {
      "epoch": 13.134328358208956,
      "grad_norm": 18.755382537841797,
      "learning_rate": 8.358830845771144e-08,
      "loss": 2.7358,
      "step": 2640
    },
    {
      "epoch": 13.18407960199005,
      "grad_norm": 18.234514236450195,
      "learning_rate": 8.352611940298507e-08,
      "loss": 2.7614,
      "step": 2650
    },
    {
      "epoch": 13.233830845771145,
      "grad_norm": 24.657100677490234,
      "learning_rate": 8.34639303482587e-08,
      "loss": 2.7123,
      "step": 2660
    },
    {
      "epoch": 13.283582089552239,
      "grad_norm": 29.938636779785156,
      "learning_rate": 8.340174129353234e-08,
      "loss": 2.7359,
      "step": 2670
    },
    {
      "epoch": 13.333333333333334,
      "grad_norm": 22.625469207763672,
      "learning_rate": 8.333955223880596e-08,
      "loss": 2.6456,
      "step": 2680
    },
    {
      "epoch": 13.383084577114428,
      "grad_norm": 13.262571334838867,
      "learning_rate": 8.32773631840796e-08,
      "loss": 2.7889,
      "step": 2690
    },
    {
      "epoch": 13.432835820895523,
      "grad_norm": 10.53028678894043,
      "learning_rate": 8.321517412935322e-08,
      "loss": 2.8339,
      "step": 2700
    },
    {
      "epoch": 13.482587064676617,
      "grad_norm": 7.8212890625,
      "learning_rate": 8.315298507462686e-08,
      "loss": 2.6943,
      "step": 2710
    },
    {
      "epoch": 13.532338308457712,
      "grad_norm": 13.198220252990723,
      "learning_rate": 8.309079601990048e-08,
      "loss": 2.7809,
      "step": 2720
    },
    {
      "epoch": 13.582089552238806,
      "grad_norm": 25.368030548095703,
      "learning_rate": 8.302860696517413e-08,
      "loss": 2.7051,
      "step": 2730
    },
    {
      "epoch": 13.631840796019901,
      "grad_norm": 11.268147468566895,
      "learning_rate": 8.296641791044776e-08,
      "loss": 2.6705,
      "step": 2740
    },
    {
      "epoch": 13.681592039800995,
      "grad_norm": 16.701082229614258,
      "learning_rate": 8.290422885572139e-08,
      "loss": 2.693,
      "step": 2750
    },
    {
      "epoch": 13.73134328358209,
      "grad_norm": 15.099906921386719,
      "learning_rate": 8.284203980099502e-08,
      "loss": 2.7549,
      "step": 2760
    },
    {
      "epoch": 13.781094527363184,
      "grad_norm": 17.919626235961914,
      "learning_rate": 8.277985074626865e-08,
      "loss": 2.7753,
      "step": 2770
    },
    {
      "epoch": 13.83084577114428,
      "grad_norm": 13.317949295043945,
      "learning_rate": 8.271766169154229e-08,
      "loss": 2.8817,
      "step": 2780
    },
    {
      "epoch": 13.880597014925373,
      "grad_norm": 23.777944564819336,
      "learning_rate": 8.265547263681591e-08,
      "loss": 2.7443,
      "step": 2790
    },
    {
      "epoch": 13.930348258706468,
      "grad_norm": 20.376741409301758,
      "learning_rate": 8.259328358208955e-08,
      "loss": 2.7795,
      "step": 2800
    },
    {
      "epoch": 13.980099502487562,
      "grad_norm": 12.85524845123291,
      "learning_rate": 8.253109452736317e-08,
      "loss": 2.7634,
      "step": 2810
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.8321056365966797,
      "eval_runtime": 55.6513,
      "eval_samples_per_second": 3.612,
      "eval_steps_per_second": 0.234,
      "eval_wer": 0.6417095115681234,
      "step": 2814
    },
    {
      "epoch": 14.029850746268657,
      "grad_norm": 17.461442947387695,
      "learning_rate": 8.246890547263681e-08,
      "loss": 2.8351,
      "step": 2820
    },
    {
      "epoch": 14.07960199004975,
      "grad_norm": 10.564457893371582,
      "learning_rate": 8.240671641791045e-08,
      "loss": 2.6348,
      "step": 2830
    },
    {
      "epoch": 14.129353233830846,
      "grad_norm": 45.25846862792969,
      "learning_rate": 8.234452736318408e-08,
      "loss": 2.7787,
      "step": 2840
    },
    {
      "epoch": 14.17910447761194,
      "grad_norm": 18.53340721130371,
      "learning_rate": 8.22823383084577e-08,
      "loss": 2.6832,
      "step": 2850
    },
    {
      "epoch": 14.228855721393035,
      "grad_norm": 11.085210800170898,
      "learning_rate": 8.222014925373134e-08,
      "loss": 2.7768,
      "step": 2860
    },
    {
      "epoch": 14.278606965174129,
      "grad_norm": 12.76481819152832,
      "learning_rate": 8.215796019900498e-08,
      "loss": 2.7491,
      "step": 2870
    },
    {
      "epoch": 14.328358208955224,
      "grad_norm": 17.42875862121582,
      "learning_rate": 8.20957711442786e-08,
      "loss": 2.8556,
      "step": 2880
    },
    {
      "epoch": 14.378109452736318,
      "grad_norm": 16.794933319091797,
      "learning_rate": 8.203358208955224e-08,
      "loss": 2.646,
      "step": 2890
    },
    {
      "epoch": 14.427860696517413,
      "grad_norm": 9.386385917663574,
      "learning_rate": 8.197139303482586e-08,
      "loss": 2.7553,
      "step": 2900
    },
    {
      "epoch": 14.477611940298507,
      "grad_norm": 24.372648239135742,
      "learning_rate": 8.19092039800995e-08,
      "loss": 2.7985,
      "step": 2910
    },
    {
      "epoch": 14.527363184079602,
      "grad_norm": 12.002032279968262,
      "learning_rate": 8.184701492537312e-08,
      "loss": 2.7618,
      "step": 2920
    },
    {
      "epoch": 14.577114427860696,
      "grad_norm": 34.29991149902344,
      "learning_rate": 8.178482587064677e-08,
      "loss": 2.7261,
      "step": 2930
    },
    {
      "epoch": 14.626865671641792,
      "grad_norm": 18.56515884399414,
      "learning_rate": 8.17226368159204e-08,
      "loss": 2.658,
      "step": 2940
    },
    {
      "epoch": 14.676616915422885,
      "grad_norm": 15.931058883666992,
      "learning_rate": 8.166044776119403e-08,
      "loss": 2.7294,
      "step": 2950
    },
    {
      "epoch": 14.72636815920398,
      "grad_norm": 14.454503059387207,
      "learning_rate": 8.159825870646765e-08,
      "loss": 2.8275,
      "step": 2960
    },
    {
      "epoch": 14.776119402985074,
      "grad_norm": 26.89356803894043,
      "learning_rate": 8.153606965174129e-08,
      "loss": 2.8193,
      "step": 2970
    },
    {
      "epoch": 14.82587064676617,
      "grad_norm": 10.626486778259277,
      "learning_rate": 8.147388059701493e-08,
      "loss": 2.7072,
      "step": 2980
    },
    {
      "epoch": 14.875621890547263,
      "grad_norm": 17.253116607666016,
      "learning_rate": 8.141169154228855e-08,
      "loss": 2.7019,
      "step": 2990
    },
    {
      "epoch": 14.925373134328359,
      "grad_norm": 22.61011505126953,
      "learning_rate": 8.134950248756219e-08,
      "loss": 2.6288,
      "step": 3000
    },
    {
      "epoch": 14.975124378109452,
      "grad_norm": 15.425240516662598,
      "learning_rate": 8.128731343283581e-08,
      "loss": 2.6135,
      "step": 3010
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.829451322555542,
      "eval_runtime": 55.4599,
      "eval_samples_per_second": 3.624,
      "eval_steps_per_second": 0.234,
      "eval_wer": 0.6405848329048843,
      "step": 3015
    },
    {
      "epoch": 15.024875621890548,
      "grad_norm": 9.538034439086914,
      "learning_rate": 8.122512437810945e-08,
      "loss": 2.6714,
      "step": 3020
    },
    {
      "epoch": 15.074626865671641,
      "grad_norm": 23.636117935180664,
      "learning_rate": 8.116293532338308e-08,
      "loss": 2.8493,
      "step": 3030
    },
    {
      "epoch": 15.124378109452737,
      "grad_norm": 28.60354232788086,
      "learning_rate": 8.110074626865672e-08,
      "loss": 2.7058,
      "step": 3040
    },
    {
      "epoch": 15.17412935323383,
      "grad_norm": 13.28507137298584,
      "learning_rate": 8.103855721393034e-08,
      "loss": 2.6207,
      "step": 3050
    },
    {
      "epoch": 15.223880597014926,
      "grad_norm": 11.067270278930664,
      "learning_rate": 8.097636815920398e-08,
      "loss": 2.6552,
      "step": 3060
    },
    {
      "epoch": 15.27363184079602,
      "grad_norm": 20.80853271484375,
      "learning_rate": 8.09141791044776e-08,
      "loss": 2.7264,
      "step": 3070
    },
    {
      "epoch": 15.323383084577115,
      "grad_norm": 28.49663734436035,
      "learning_rate": 8.085199004975124e-08,
      "loss": 2.8262,
      "step": 3080
    },
    {
      "epoch": 15.373134328358208,
      "grad_norm": 9.803523063659668,
      "learning_rate": 8.078980099502488e-08,
      "loss": 2.7123,
      "step": 3090
    },
    {
      "epoch": 15.422885572139304,
      "grad_norm": 8.823195457458496,
      "learning_rate": 8.07276119402985e-08,
      "loss": 2.6958,
      "step": 3100
    },
    {
      "epoch": 15.472636815920398,
      "grad_norm": 17.853727340698242,
      "learning_rate": 8.066542288557214e-08,
      "loss": 2.7172,
      "step": 3110
    },
    {
      "epoch": 15.522388059701493,
      "grad_norm": 17.662208557128906,
      "learning_rate": 8.060323383084576e-08,
      "loss": 2.7728,
      "step": 3120
    },
    {
      "epoch": 15.572139303482587,
      "grad_norm": 35.81214904785156,
      "learning_rate": 8.054104477611941e-08,
      "loss": 2.8001,
      "step": 3130
    },
    {
      "epoch": 15.621890547263682,
      "grad_norm": 10.171026229858398,
      "learning_rate": 8.047885572139303e-08,
      "loss": 2.828,
      "step": 3140
    },
    {
      "epoch": 15.671641791044776,
      "grad_norm": 13.630511283874512,
      "learning_rate": 8.041666666666667e-08,
      "loss": 2.879,
      "step": 3150
    },
    {
      "epoch": 15.721393034825871,
      "grad_norm": 19.05656623840332,
      "learning_rate": 8.035447761194029e-08,
      "loss": 2.6959,
      "step": 3160
    },
    {
      "epoch": 15.771144278606965,
      "grad_norm": 11.255326271057129,
      "learning_rate": 8.029228855721393e-08,
      "loss": 2.5486,
      "step": 3170
    },
    {
      "epoch": 15.82089552238806,
      "grad_norm": 22.636281967163086,
      "learning_rate": 8.023009950248755e-08,
      "loss": 2.7598,
      "step": 3180
    },
    {
      "epoch": 15.870646766169154,
      "grad_norm": 13.88830852508545,
      "learning_rate": 8.016791044776119e-08,
      "loss": 2.7559,
      "step": 3190
    },
    {
      "epoch": 15.92039800995025,
      "grad_norm": 12.993346214294434,
      "learning_rate": 8.010572139303482e-08,
      "loss": 2.7128,
      "step": 3200
    },
    {
      "epoch": 15.970149253731343,
      "grad_norm": 10.537925720214844,
      "learning_rate": 8.004353233830845e-08,
      "loss": 2.7412,
      "step": 3210
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.827473759651184,
      "eval_runtime": 56.2876,
      "eval_samples_per_second": 3.571,
      "eval_steps_per_second": 0.231,
      "eval_wer": 0.6395404884318766,
      "step": 3216
    },
    {
      "epoch": 16.019900497512438,
      "grad_norm": 6.670629501342773,
      "learning_rate": 7.998134328358208e-08,
      "loss": 2.5861,
      "step": 3220
    },
    {
      "epoch": 16.069651741293534,
      "grad_norm": 10.572278022766113,
      "learning_rate": 7.991915422885572e-08,
      "loss": 2.6621,
      "step": 3230
    },
    {
      "epoch": 16.119402985074625,
      "grad_norm": 16.394071578979492,
      "learning_rate": 7.985696517412936e-08,
      "loss": 2.7482,
      "step": 3240
    },
    {
      "epoch": 16.16915422885572,
      "grad_norm": 12.65933609008789,
      "learning_rate": 7.979477611940298e-08,
      "loss": 2.7241,
      "step": 3250
    },
    {
      "epoch": 16.218905472636816,
      "grad_norm": 22.966014862060547,
      "learning_rate": 7.973258706467662e-08,
      "loss": 2.7528,
      "step": 3260
    },
    {
      "epoch": 16.26865671641791,
      "grad_norm": 8.388383865356445,
      "learning_rate": 7.967039800995024e-08,
      "loss": 2.7109,
      "step": 3270
    },
    {
      "epoch": 16.318407960199004,
      "grad_norm": 11.883491516113281,
      "learning_rate": 7.960820895522388e-08,
      "loss": 2.7126,
      "step": 3280
    },
    {
      "epoch": 16.3681592039801,
      "grad_norm": 8.505130767822266,
      "learning_rate": 7.954601990049751e-08,
      "loss": 2.7468,
      "step": 3290
    },
    {
      "epoch": 16.417910447761194,
      "grad_norm": 8.721114158630371,
      "learning_rate": 7.948383084577114e-08,
      "loss": 2.6349,
      "step": 3300
    },
    {
      "epoch": 16.46766169154229,
      "grad_norm": 9.483895301818848,
      "learning_rate": 7.942164179104477e-08,
      "loss": 2.6671,
      "step": 3310
    },
    {
      "epoch": 16.51741293532338,
      "grad_norm": 11.738534927368164,
      "learning_rate": 7.93594527363184e-08,
      "loss": 2.6841,
      "step": 3320
    },
    {
      "epoch": 16.567164179104477,
      "grad_norm": 10.321829795837402,
      "learning_rate": 7.929726368159205e-08,
      "loss": 2.7227,
      "step": 3330
    },
    {
      "epoch": 16.616915422885572,
      "grad_norm": 13.008976936340332,
      "learning_rate": 7.923507462686567e-08,
      "loss": 2.7685,
      "step": 3340
    },
    {
      "epoch": 16.666666666666668,
      "grad_norm": 18.715858459472656,
      "learning_rate": 7.91728855721393e-08,
      "loss": 2.8681,
      "step": 3350
    },
    {
      "epoch": 16.71641791044776,
      "grad_norm": 11.3253755569458,
      "learning_rate": 7.911069651741293e-08,
      "loss": 2.7729,
      "step": 3360
    },
    {
      "epoch": 16.766169154228855,
      "grad_norm": 12.6301851272583,
      "learning_rate": 7.904850746268657e-08,
      "loss": 2.6903,
      "step": 3370
    },
    {
      "epoch": 16.81592039800995,
      "grad_norm": 20.07337188720703,
      "learning_rate": 7.898631840796019e-08,
      "loss": 2.8144,
      "step": 3380
    },
    {
      "epoch": 16.865671641791046,
      "grad_norm": 13.391197204589844,
      "learning_rate": 7.892412935323383e-08,
      "loss": 2.6257,
      "step": 3390
    },
    {
      "epoch": 16.915422885572138,
      "grad_norm": 18.82671356201172,
      "learning_rate": 7.886194029850746e-08,
      "loss": 2.6548,
      "step": 3400
    },
    {
      "epoch": 16.965174129353233,
      "grad_norm": 12.231903076171875,
      "learning_rate": 7.879975124378109e-08,
      "loss": 2.6741,
      "step": 3410
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.8240097761154175,
      "eval_runtime": 54.5557,
      "eval_samples_per_second": 3.684,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6383354755784062,
      "step": 3417
    },
    {
      "epoch": 17.01492537313433,
      "grad_norm": 11.282583236694336,
      "learning_rate": 7.873756218905472e-08,
      "loss": 2.6552,
      "step": 3420
    },
    {
      "epoch": 17.064676616915424,
      "grad_norm": 8.543099403381348,
      "learning_rate": 7.867537313432836e-08,
      "loss": 2.7289,
      "step": 3430
    },
    {
      "epoch": 17.114427860696516,
      "grad_norm": 11.248555183410645,
      "learning_rate": 7.8613184079602e-08,
      "loss": 2.7071,
      "step": 3440
    },
    {
      "epoch": 17.16417910447761,
      "grad_norm": 22.90814208984375,
      "learning_rate": 7.855099502487562e-08,
      "loss": 2.7166,
      "step": 3450
    },
    {
      "epoch": 17.213930348258707,
      "grad_norm": 13.499836921691895,
      "learning_rate": 7.848880597014926e-08,
      "loss": 2.6631,
      "step": 3460
    },
    {
      "epoch": 17.263681592039802,
      "grad_norm": 8.929615020751953,
      "learning_rate": 7.842661691542288e-08,
      "loss": 2.6863,
      "step": 3470
    },
    {
      "epoch": 17.313432835820894,
      "grad_norm": 11.842375755310059,
      "learning_rate": 7.836442786069652e-08,
      "loss": 2.7535,
      "step": 3480
    },
    {
      "epoch": 17.36318407960199,
      "grad_norm": 9.183212280273438,
      "learning_rate": 7.830223880597014e-08,
      "loss": 2.6878,
      "step": 3490
    },
    {
      "epoch": 17.412935323383085,
      "grad_norm": 11.240518569946289,
      "learning_rate": 7.824004975124377e-08,
      "loss": 2.762,
      "step": 3500
    },
    {
      "epoch": 17.46268656716418,
      "grad_norm": 16.72664451599121,
      "learning_rate": 7.817786069651741e-08,
      "loss": 2.7018,
      "step": 3510
    },
    {
      "epoch": 17.512437810945272,
      "grad_norm": 11.163666725158691,
      "learning_rate": 7.811567164179103e-08,
      "loss": 2.7645,
      "step": 3520
    },
    {
      "epoch": 17.562189054726367,
      "grad_norm": 11.0314302444458,
      "learning_rate": 7.805348258706468e-08,
      "loss": 2.6692,
      "step": 3530
    },
    {
      "epoch": 17.611940298507463,
      "grad_norm": 25.08024024963379,
      "learning_rate": 7.799129353233831e-08,
      "loss": 2.7104,
      "step": 3540
    },
    {
      "epoch": 17.66169154228856,
      "grad_norm": 21.341114044189453,
      "learning_rate": 7.792910447761194e-08,
      "loss": 2.6152,
      "step": 3550
    },
    {
      "epoch": 17.71144278606965,
      "grad_norm": 9.161097526550293,
      "learning_rate": 7.786691542288557e-08,
      "loss": 2.7724,
      "step": 3560
    },
    {
      "epoch": 17.761194029850746,
      "grad_norm": 10.31374740600586,
      "learning_rate": 7.78047263681592e-08,
      "loss": 2.7875,
      "step": 3570
    },
    {
      "epoch": 17.81094527363184,
      "grad_norm": 16.98878288269043,
      "learning_rate": 7.774253731343283e-08,
      "loss": 2.6975,
      "step": 3580
    },
    {
      "epoch": 17.860696517412936,
      "grad_norm": 10.657710075378418,
      "learning_rate": 7.768034825870646e-08,
      "loss": 2.7189,
      "step": 3590
    },
    {
      "epoch": 17.91044776119403,
      "grad_norm": 383.8722839355469,
      "learning_rate": 7.761815920398009e-08,
      "loss": 2.8247,
      "step": 3600
    },
    {
      "epoch": 17.960199004975124,
      "grad_norm": 12.865944862365723,
      "learning_rate": 7.755597014925372e-08,
      "loss": 2.7647,
      "step": 3610
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.8235832452774048,
      "eval_runtime": 55.744,
      "eval_samples_per_second": 3.606,
      "eval_steps_per_second": 0.233,
      "eval_wer": 0.6375321336760925,
      "step": 3618
    },
    {
      "epoch": 18.00995024875622,
      "grad_norm": 21.101224899291992,
      "learning_rate": 7.749378109452736e-08,
      "loss": 2.7719,
      "step": 3620
    },
    {
      "epoch": 18.059701492537314,
      "grad_norm": 11.254598617553711,
      "learning_rate": 7.7431592039801e-08,
      "loss": 2.6626,
      "step": 3630
    },
    {
      "epoch": 18.109452736318406,
      "grad_norm": 9.341910362243652,
      "learning_rate": 7.736940298507463e-08,
      "loss": 2.7861,
      "step": 3640
    },
    {
      "epoch": 18.1592039800995,
      "grad_norm": 12.621024131774902,
      "learning_rate": 7.730721393034826e-08,
      "loss": 2.7108,
      "step": 3650
    },
    {
      "epoch": 18.208955223880597,
      "grad_norm": 9.108014106750488,
      "learning_rate": 7.724502487562189e-08,
      "loss": 2.6652,
      "step": 3660
    },
    {
      "epoch": 18.258706467661693,
      "grad_norm": 18.388813018798828,
      "learning_rate": 7.718283582089552e-08,
      "loss": 2.7855,
      "step": 3670
    },
    {
      "epoch": 18.308457711442784,
      "grad_norm": 14.45107364654541,
      "learning_rate": 7.712064676616915e-08,
      "loss": 2.5979,
      "step": 3680
    },
    {
      "epoch": 18.35820895522388,
      "grad_norm": 13.313628196716309,
      "learning_rate": 7.705845771144278e-08,
      "loss": 2.507,
      "step": 3690
    },
    {
      "epoch": 18.407960199004975,
      "grad_norm": 7.352657318115234,
      "learning_rate": 7.699626865671641e-08,
      "loss": 2.7389,
      "step": 3700
    },
    {
      "epoch": 18.45771144278607,
      "grad_norm": 27.20574951171875,
      "learning_rate": 7.693407960199005e-08,
      "loss": 2.7185,
      "step": 3710
    },
    {
      "epoch": 18.507462686567163,
      "grad_norm": 22.067760467529297,
      "learning_rate": 7.687189054726367e-08,
      "loss": 2.7257,
      "step": 3720
    },
    {
      "epoch": 18.557213930348258,
      "grad_norm": 9.608321189880371,
      "learning_rate": 7.680970149253731e-08,
      "loss": 2.7412,
      "step": 3730
    },
    {
      "epoch": 18.606965174129353,
      "grad_norm": 18.844270706176758,
      "learning_rate": 7.674751243781095e-08,
      "loss": 2.6565,
      "step": 3740
    },
    {
      "epoch": 18.65671641791045,
      "grad_norm": 20.191890716552734,
      "learning_rate": 7.668532338308458e-08,
      "loss": 2.6829,
      "step": 3750
    },
    {
      "epoch": 18.70646766169154,
      "grad_norm": 31.303558349609375,
      "learning_rate": 7.66231343283582e-08,
      "loss": 2.7326,
      "step": 3760
    },
    {
      "epoch": 18.756218905472636,
      "grad_norm": 12.487874031066895,
      "learning_rate": 7.656094527363184e-08,
      "loss": 2.6986,
      "step": 3770
    },
    {
      "epoch": 18.80597014925373,
      "grad_norm": 6.956881523132324,
      "learning_rate": 7.649875621890547e-08,
      "loss": 2.6566,
      "step": 3780
    },
    {
      "epoch": 18.855721393034827,
      "grad_norm": 28.780864715576172,
      "learning_rate": 7.64365671641791e-08,
      "loss": 2.796,
      "step": 3790
    },
    {
      "epoch": 18.90547263681592,
      "grad_norm": 18.573652267456055,
      "learning_rate": 7.637437810945272e-08,
      "loss": 2.6309,
      "step": 3800
    },
    {
      "epoch": 18.955223880597014,
      "grad_norm": 25.475536346435547,
      "learning_rate": 7.631218905472636e-08,
      "loss": 2.6227,
      "step": 3810
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.8183845281600952,
      "eval_runtime": 54.9135,
      "eval_samples_per_second": 3.66,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6364877892030848,
      "step": 3819
    },
    {
      "epoch": 19.00497512437811,
      "grad_norm": 11.953489303588867,
      "learning_rate": 7.625e-08,
      "loss": 2.8211,
      "step": 3820
    },
    {
      "epoch": 19.054726368159205,
      "grad_norm": 7.680521488189697,
      "learning_rate": 7.618781094527362e-08,
      "loss": 2.603,
      "step": 3830
    },
    {
      "epoch": 19.104477611940297,
      "grad_norm": 9.331663131713867,
      "learning_rate": 7.612562189054727e-08,
      "loss": 2.5487,
      "step": 3840
    },
    {
      "epoch": 19.154228855721392,
      "grad_norm": 22.96463394165039,
      "learning_rate": 7.60634328358209e-08,
      "loss": 2.7384,
      "step": 3850
    },
    {
      "epoch": 19.203980099502488,
      "grad_norm": 8.68323040008545,
      "learning_rate": 7.600124378109453e-08,
      "loss": 2.6564,
      "step": 3860
    },
    {
      "epoch": 19.253731343283583,
      "grad_norm": 23.72835350036621,
      "learning_rate": 7.593905472636815e-08,
      "loss": 2.7338,
      "step": 3870
    },
    {
      "epoch": 19.303482587064675,
      "grad_norm": 11.818121910095215,
      "learning_rate": 7.587686567164179e-08,
      "loss": 2.6679,
      "step": 3880
    },
    {
      "epoch": 19.35323383084577,
      "grad_norm": 17.817873001098633,
      "learning_rate": 7.581467661691541e-08,
      "loss": 2.6647,
      "step": 3890
    },
    {
      "epoch": 19.402985074626866,
      "grad_norm": 12.85451889038086,
      "learning_rate": 7.575248756218905e-08,
      "loss": 2.6999,
      "step": 3900
    },
    {
      "epoch": 19.45273631840796,
      "grad_norm": 6.91808557510376,
      "learning_rate": 7.569029850746267e-08,
      "loss": 2.6152,
      "step": 3910
    },
    {
      "epoch": 19.502487562189053,
      "grad_norm": 11.61572551727295,
      "learning_rate": 7.562810945273631e-08,
      "loss": 2.7084,
      "step": 3920
    },
    {
      "epoch": 19.55223880597015,
      "grad_norm": 27.4916934967041,
      "learning_rate": 7.556592039800995e-08,
      "loss": 2.7692,
      "step": 3930
    },
    {
      "epoch": 19.601990049751244,
      "grad_norm": 12.958518028259277,
      "learning_rate": 7.550373134328358e-08,
      "loss": 2.6912,
      "step": 3940
    },
    {
      "epoch": 19.65174129353234,
      "grad_norm": 9.466227531433105,
      "learning_rate": 7.544154228855722e-08,
      "loss": 2.7242,
      "step": 3950
    },
    {
      "epoch": 19.701492537313435,
      "grad_norm": 12.134029388427734,
      "learning_rate": 7.537935323383084e-08,
      "loss": 2.7538,
      "step": 3960
    },
    {
      "epoch": 19.751243781094526,
      "grad_norm": 9.257498741149902,
      "learning_rate": 7.531716417910448e-08,
      "loss": 2.6915,
      "step": 3970
    },
    {
      "epoch": 19.800995024875622,
      "grad_norm": 19.702234268188477,
      "learning_rate": 7.52549751243781e-08,
      "loss": 2.632,
      "step": 3980
    },
    {
      "epoch": 19.850746268656717,
      "grad_norm": 22.23696517944336,
      "learning_rate": 7.519278606965174e-08,
      "loss": 2.6842,
      "step": 3990
    },
    {
      "epoch": 19.90049751243781,
      "grad_norm": 7.985266208648682,
      "learning_rate": 7.513059701492536e-08,
      "loss": 2.7636,
      "step": 4000
    },
    {
      "epoch": 19.950248756218905,
      "grad_norm": 16.802181243896484,
      "learning_rate": 7.5068407960199e-08,
      "loss": 2.7509,
      "step": 4010
    },
    {
      "epoch": 20.0,
      "grad_norm": 10.247150421142578,
      "learning_rate": 7.500621890547264e-08,
      "loss": 2.6283,
      "step": 4020
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.8165626525878906,
      "eval_runtime": 54.8147,
      "eval_samples_per_second": 3.667,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6355237789203085,
      "step": 4020
    },
    {
      "epoch": 20.049751243781095,
      "grad_norm": 7.163029193878174,
      "learning_rate": 7.494402985074626e-08,
      "loss": 2.5953,
      "step": 4030
    },
    {
      "epoch": 20.09950248756219,
      "grad_norm": 9.459470748901367,
      "learning_rate": 7.48818407960199e-08,
      "loss": 2.6085,
      "step": 4040
    },
    {
      "epoch": 20.149253731343283,
      "grad_norm": 18.156658172607422,
      "learning_rate": 7.481965174129353e-08,
      "loss": 2.6231,
      "step": 4050
    },
    {
      "epoch": 20.199004975124378,
      "grad_norm": 27.953472137451172,
      "learning_rate": 7.475746268656717e-08,
      "loss": 2.61,
      "step": 4060
    },
    {
      "epoch": 20.248756218905474,
      "grad_norm": 7.369011402130127,
      "learning_rate": 7.469527363184079e-08,
      "loss": 2.6961,
      "step": 4070
    },
    {
      "epoch": 20.298507462686565,
      "grad_norm": 9.755149841308594,
      "learning_rate": 7.463308457711443e-08,
      "loss": 2.702,
      "step": 4080
    },
    {
      "epoch": 20.34825870646766,
      "grad_norm": 12.758254051208496,
      "learning_rate": 7.457089552238805e-08,
      "loss": 2.6816,
      "step": 4090
    },
    {
      "epoch": 20.398009950248756,
      "grad_norm": 4.977162837982178,
      "learning_rate": 7.450870646766169e-08,
      "loss": 2.6511,
      "step": 4100
    },
    {
      "epoch": 20.44776119402985,
      "grad_norm": 20.092811584472656,
      "learning_rate": 7.444651741293531e-08,
      "loss": 2.7894,
      "step": 4110
    },
    {
      "epoch": 20.497512437810947,
      "grad_norm": 9.44950008392334,
      "learning_rate": 7.438432835820895e-08,
      "loss": 2.7195,
      "step": 4120
    },
    {
      "epoch": 20.54726368159204,
      "grad_norm": 33.4273567199707,
      "learning_rate": 7.432213930348258e-08,
      "loss": 2.6302,
      "step": 4130
    },
    {
      "epoch": 20.597014925373134,
      "grad_norm": 8.689030647277832,
      "learning_rate": 7.425995024875622e-08,
      "loss": 2.5824,
      "step": 4140
    },
    {
      "epoch": 20.64676616915423,
      "grad_norm": 19.37832260131836,
      "learning_rate": 7.419776119402986e-08,
      "loss": 2.5665,
      "step": 4150
    },
    {
      "epoch": 20.696517412935325,
      "grad_norm": 11.948487281799316,
      "learning_rate": 7.413557213930348e-08,
      "loss": 2.6348,
      "step": 4160
    },
    {
      "epoch": 20.746268656716417,
      "grad_norm": 32.66345977783203,
      "learning_rate": 7.407338308457712e-08,
      "loss": 2.8252,
      "step": 4170
    },
    {
      "epoch": 20.796019900497512,
      "grad_norm": 32.81978225708008,
      "learning_rate": 7.401119402985074e-08,
      "loss": 2.7295,
      "step": 4180
    },
    {
      "epoch": 20.845771144278608,
      "grad_norm": 22.341094970703125,
      "learning_rate": 7.394900497512438e-08,
      "loss": 2.7454,
      "step": 4190
    },
    {
      "epoch": 20.895522388059703,
      "grad_norm": 16.170106887817383,
      "learning_rate": 7.3886815920398e-08,
      "loss": 2.731,
      "step": 4200
    },
    {
      "epoch": 20.945273631840795,
      "grad_norm": 19.01198959350586,
      "learning_rate": 7.382462686567164e-08,
      "loss": 2.7727,
      "step": 4210
    },
    {
      "epoch": 20.99502487562189,
      "grad_norm": 10.42905330657959,
      "learning_rate": 7.376243781094526e-08,
      "loss": 2.6645,
      "step": 4220
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.810294270515442,
      "eval_runtime": 54.909,
      "eval_samples_per_second": 3.661,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6341580976863753,
      "step": 4221
    },
    {
      "epoch": 21.044776119402986,
      "grad_norm": 15.751816749572754,
      "learning_rate": 7.37002487562189e-08,
      "loss": 2.6378,
      "step": 4230
    },
    {
      "epoch": 21.09452736318408,
      "grad_norm": 17.25919532775879,
      "learning_rate": 7.363805970149253e-08,
      "loss": 2.6181,
      "step": 4240
    },
    {
      "epoch": 21.144278606965173,
      "grad_norm": 13.23126220703125,
      "learning_rate": 7.357587064676617e-08,
      "loss": 2.7501,
      "step": 4250
    },
    {
      "epoch": 21.19402985074627,
      "grad_norm": 7.983088493347168,
      "learning_rate": 7.35136815920398e-08,
      "loss": 2.7029,
      "step": 4260
    },
    {
      "epoch": 21.243781094527364,
      "grad_norm": 12.163625717163086,
      "learning_rate": 7.345149253731343e-08,
      "loss": 2.6044,
      "step": 4270
    },
    {
      "epoch": 21.29353233830846,
      "grad_norm": 26.465110778808594,
      "learning_rate": 7.338930348258707e-08,
      "loss": 2.6834,
      "step": 4280
    },
    {
      "epoch": 21.34328358208955,
      "grad_norm": 8.740859031677246,
      "learning_rate": 7.332711442786069e-08,
      "loss": 2.614,
      "step": 4290
    },
    {
      "epoch": 21.393034825870647,
      "grad_norm": 10.852879524230957,
      "learning_rate": 7.326492537313433e-08,
      "loss": 2.6847,
      "step": 4300
    },
    {
      "epoch": 21.442786069651742,
      "grad_norm": 8.569189071655273,
      "learning_rate": 7.320273631840795e-08,
      "loss": 2.6817,
      "step": 4310
    },
    {
      "epoch": 21.492537313432837,
      "grad_norm": 13.937314987182617,
      "learning_rate": 7.314054726368159e-08,
      "loss": 2.7245,
      "step": 4320
    },
    {
      "epoch": 21.54228855721393,
      "grad_norm": 10.690689086914062,
      "learning_rate": 7.307835820895521e-08,
      "loss": 2.7012,
      "step": 4330
    },
    {
      "epoch": 21.592039800995025,
      "grad_norm": 12.18982219696045,
      "learning_rate": 7.301616915422886e-08,
      "loss": 2.6826,
      "step": 4340
    },
    {
      "epoch": 21.64179104477612,
      "grad_norm": 24.41484832763672,
      "learning_rate": 7.295398009950248e-08,
      "loss": 2.6424,
      "step": 4350
    },
    {
      "epoch": 21.691542288557216,
      "grad_norm": 20.23264503479004,
      "learning_rate": 7.289179104477612e-08,
      "loss": 2.7074,
      "step": 4360
    },
    {
      "epoch": 21.741293532338307,
      "grad_norm": 11.027195930480957,
      "learning_rate": 7.282960199004975e-08,
      "loss": 2.6165,
      "step": 4370
    },
    {
      "epoch": 21.791044776119403,
      "grad_norm": 33.532493591308594,
      "learning_rate": 7.276741293532338e-08,
      "loss": 2.7862,
      "step": 4380
    },
    {
      "epoch": 21.8407960199005,
      "grad_norm": 12.75308609008789,
      "learning_rate": 7.270522388059701e-08,
      "loss": 2.6442,
      "step": 4390
    },
    {
      "epoch": 21.890547263681594,
      "grad_norm": 10.185760498046875,
      "learning_rate": 7.264303482587064e-08,
      "loss": 2.5041,
      "step": 4400
    },
    {
      "epoch": 21.940298507462686,
      "grad_norm": 16.918930053710938,
      "learning_rate": 7.258084577114427e-08,
      "loss": 2.7204,
      "step": 4410
    },
    {
      "epoch": 21.99004975124378,
      "grad_norm": 22.879114151000977,
      "learning_rate": 7.25186567164179e-08,
      "loss": 2.7472,
      "step": 4420
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.807182788848877,
      "eval_runtime": 54.8945,
      "eval_samples_per_second": 3.662,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6335957583547558,
      "step": 4422
    },
    {
      "epoch": 22.039800995024876,
      "grad_norm": 8.819696426391602,
      "learning_rate": 7.245646766169153e-08,
      "loss": 2.649,
      "step": 4430
    },
    {
      "epoch": 22.08955223880597,
      "grad_norm": 10.360767364501953,
      "learning_rate": 7.239427860696517e-08,
      "loss": 2.6457,
      "step": 4440
    },
    {
      "epoch": 22.139303482587064,
      "grad_norm": 17.25057601928711,
      "learning_rate": 7.233208955223881e-08,
      "loss": 2.6731,
      "step": 4450
    },
    {
      "epoch": 22.18905472636816,
      "grad_norm": 30.972156524658203,
      "learning_rate": 7.226990049751243e-08,
      "loss": 2.6329,
      "step": 4460
    },
    {
      "epoch": 22.238805970149254,
      "grad_norm": 6.157905578613281,
      "learning_rate": 7.220771144278607e-08,
      "loss": 2.7565,
      "step": 4470
    },
    {
      "epoch": 22.28855721393035,
      "grad_norm": 13.438813209533691,
      "learning_rate": 7.21455223880597e-08,
      "loss": 2.6099,
      "step": 4480
    },
    {
      "epoch": 22.33830845771144,
      "grad_norm": 11.847139358520508,
      "learning_rate": 7.208333333333333e-08,
      "loss": 2.5949,
      "step": 4490
    },
    {
      "epoch": 22.388059701492537,
      "grad_norm": 10.665842056274414,
      "learning_rate": 7.202114427860696e-08,
      "loss": 2.6127,
      "step": 4500
    },
    {
      "epoch": 22.437810945273633,
      "grad_norm": 14.563610076904297,
      "learning_rate": 7.195895522388059e-08,
      "loss": 2.5528,
      "step": 4510
    },
    {
      "epoch": 22.487562189054728,
      "grad_norm": 12.786352157592773,
      "learning_rate": 7.189676616915422e-08,
      "loss": 2.7659,
      "step": 4520
    },
    {
      "epoch": 22.53731343283582,
      "grad_norm": 8.973712921142578,
      "learning_rate": 7.183457711442785e-08,
      "loss": 2.7145,
      "step": 4530
    },
    {
      "epoch": 22.587064676616915,
      "grad_norm": 13.921359062194824,
      "learning_rate": 7.17723880597015e-08,
      "loss": 2.6855,
      "step": 4540
    },
    {
      "epoch": 22.63681592039801,
      "grad_norm": 11.181320190429688,
      "learning_rate": 7.171019900497512e-08,
      "loss": 2.6837,
      "step": 4550
    },
    {
      "epoch": 22.686567164179106,
      "grad_norm": 9.049168586730957,
      "learning_rate": 7.164800995024876e-08,
      "loss": 2.602,
      "step": 4560
    },
    {
      "epoch": 22.736318407960198,
      "grad_norm": 29.459789276123047,
      "learning_rate": 7.158582089552239e-08,
      "loss": 2.8534,
      "step": 4570
    },
    {
      "epoch": 22.786069651741293,
      "grad_norm": 13.27587890625,
      "learning_rate": 7.152363184079602e-08,
      "loss": 2.6143,
      "step": 4580
    },
    {
      "epoch": 22.83582089552239,
      "grad_norm": 12.674696922302246,
      "learning_rate": 7.146144278606965e-08,
      "loss": 2.635,
      "step": 4590
    },
    {
      "epoch": 22.885572139303484,
      "grad_norm": 12.352592468261719,
      "learning_rate": 7.139925373134328e-08,
      "loss": 2.7233,
      "step": 4600
    },
    {
      "epoch": 22.935323383084576,
      "grad_norm": 9.997132301330566,
      "learning_rate": 7.133706467661691e-08,
      "loss": 2.6708,
      "step": 4610
    },
    {
      "epoch": 22.98507462686567,
      "grad_norm": 11.787625312805176,
      "learning_rate": 7.127487562189054e-08,
      "loss": 2.6462,
      "step": 4620
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.8047884702682495,
      "eval_runtime": 54.9333,
      "eval_samples_per_second": 3.659,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6324710796915167,
      "step": 4623
    },
    {
      "epoch": 23.034825870646767,
      "grad_norm": 17.687253952026367,
      "learning_rate": 7.121268656716417e-08,
      "loss": 2.6567,
      "step": 4630
    },
    {
      "epoch": 23.084577114427862,
      "grad_norm": 17.785890579223633,
      "learning_rate": 7.115049751243781e-08,
      "loss": 2.6421,
      "step": 4640
    },
    {
      "epoch": 23.134328358208954,
      "grad_norm": 6.870771884918213,
      "learning_rate": 7.108830845771145e-08,
      "loss": 2.6521,
      "step": 4650
    },
    {
      "epoch": 23.18407960199005,
      "grad_norm": 6.272050380706787,
      "learning_rate": 7.102611940298507e-08,
      "loss": 2.7353,
      "step": 4660
    },
    {
      "epoch": 23.233830845771145,
      "grad_norm": 26.316814422607422,
      "learning_rate": 7.09639303482587e-08,
      "loss": 2.7662,
      "step": 4670
    },
    {
      "epoch": 23.28358208955224,
      "grad_norm": 8.528743743896484,
      "learning_rate": 7.090174129353234e-08,
      "loss": 2.5967,
      "step": 4680
    },
    {
      "epoch": 23.333333333333332,
      "grad_norm": 23.913970947265625,
      "learning_rate": 7.083955223880596e-08,
      "loss": 2.7513,
      "step": 4690
    },
    {
      "epoch": 23.383084577114428,
      "grad_norm": 33.10710906982422,
      "learning_rate": 7.07773631840796e-08,
      "loss": 2.6955,
      "step": 4700
    },
    {
      "epoch": 23.432835820895523,
      "grad_norm": 12.168835639953613,
      "learning_rate": 7.071517412935322e-08,
      "loss": 2.5826,
      "step": 4710
    },
    {
      "epoch": 23.48258706467662,
      "grad_norm": 8.137096405029297,
      "learning_rate": 7.065298507462686e-08,
      "loss": 2.5859,
      "step": 4720
    },
    {
      "epoch": 23.53233830845771,
      "grad_norm": 13.478033065795898,
      "learning_rate": 7.059079601990048e-08,
      "loss": 2.6169,
      "step": 4730
    },
    {
      "epoch": 23.582089552238806,
      "grad_norm": 17.835979461669922,
      "learning_rate": 7.052860696517413e-08,
      "loss": 2.6384,
      "step": 4740
    },
    {
      "epoch": 23.6318407960199,
      "grad_norm": 8.193760871887207,
      "learning_rate": 7.046641791044776e-08,
      "loss": 2.7403,
      "step": 4750
    },
    {
      "epoch": 23.681592039800996,
      "grad_norm": 7.200317859649658,
      "learning_rate": 7.04042288557214e-08,
      "loss": 2.7137,
      "step": 4760
    },
    {
      "epoch": 23.73134328358209,
      "grad_norm": 9.045791625976562,
      "learning_rate": 7.034203980099502e-08,
      "loss": 2.7073,
      "step": 4770
    },
    {
      "epoch": 23.781094527363184,
      "grad_norm": 7.162771701812744,
      "learning_rate": 7.027985074626865e-08,
      "loss": 2.644,
      "step": 4780
    },
    {
      "epoch": 23.83084577114428,
      "grad_norm": 12.513699531555176,
      "learning_rate": 7.021766169154229e-08,
      "loss": 2.6943,
      "step": 4790
    },
    {
      "epoch": 23.880597014925375,
      "grad_norm": 12.753575325012207,
      "learning_rate": 7.015547263681591e-08,
      "loss": 2.6166,
      "step": 4800
    },
    {
      "epoch": 23.930348258706466,
      "grad_norm": 11.35549545288086,
      "learning_rate": 7.009328358208955e-08,
      "loss": 2.6053,
      "step": 4810
    },
    {
      "epoch": 23.980099502487562,
      "grad_norm": 18.095109939575195,
      "learning_rate": 7.003109452736317e-08,
      "loss": 2.7317,
      "step": 4820
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.7997440099716187,
      "eval_runtime": 55.2438,
      "eval_samples_per_second": 3.638,
      "eval_steps_per_second": 0.235,
      "eval_wer": 0.631667737789203,
      "step": 4824
    },
    {
      "epoch": 24.029850746268657,
      "grad_norm": 13.570301055908203,
      "learning_rate": 6.996890547263681e-08,
      "loss": 2.535,
      "step": 4830
    },
    {
      "epoch": 24.079601990049753,
      "grad_norm": 10.081156730651855,
      "learning_rate": 6.990671641791045e-08,
      "loss": 2.5317,
      "step": 4840
    },
    {
      "epoch": 24.129353233830845,
      "grad_norm": 11.467473030090332,
      "learning_rate": 6.984452736318408e-08,
      "loss": 2.7087,
      "step": 4850
    },
    {
      "epoch": 24.17910447761194,
      "grad_norm": 8.998106002807617,
      "learning_rate": 6.97823383084577e-08,
      "loss": 2.6891,
      "step": 4860
    },
    {
      "epoch": 24.228855721393035,
      "grad_norm": 7.155572891235352,
      "learning_rate": 6.972014925373134e-08,
      "loss": 2.5686,
      "step": 4870
    },
    {
      "epoch": 24.27860696517413,
      "grad_norm": 13.332143783569336,
      "learning_rate": 6.965796019900498e-08,
      "loss": 2.6213,
      "step": 4880
    },
    {
      "epoch": 24.328358208955223,
      "grad_norm": 13.36327838897705,
      "learning_rate": 6.95957711442786e-08,
      "loss": 2.6543,
      "step": 4890
    },
    {
      "epoch": 24.378109452736318,
      "grad_norm": 19.62424087524414,
      "learning_rate": 6.953358208955224e-08,
      "loss": 2.572,
      "step": 4900
    },
    {
      "epoch": 24.427860696517413,
      "grad_norm": 19.95665168762207,
      "learning_rate": 6.947139303482586e-08,
      "loss": 2.6816,
      "step": 4910
    },
    {
      "epoch": 24.47761194029851,
      "grad_norm": 9.953303337097168,
      "learning_rate": 6.94092039800995e-08,
      "loss": 2.7102,
      "step": 4920
    },
    {
      "epoch": 24.5273631840796,
      "grad_norm": 9.285727500915527,
      "learning_rate": 6.934701492537312e-08,
      "loss": 2.6116,
      "step": 4930
    },
    {
      "epoch": 24.577114427860696,
      "grad_norm": 22.130775451660156,
      "learning_rate": 6.928482587064677e-08,
      "loss": 2.6754,
      "step": 4940
    },
    {
      "epoch": 24.62686567164179,
      "grad_norm": 9.705073356628418,
      "learning_rate": 6.92226368159204e-08,
      "loss": 2.5815,
      "step": 4950
    },
    {
      "epoch": 24.676616915422887,
      "grad_norm": 14.361618995666504,
      "learning_rate": 6.916044776119403e-08,
      "loss": 2.5155,
      "step": 4960
    },
    {
      "epoch": 24.72636815920398,
      "grad_norm": 11.486882209777832,
      "learning_rate": 6.909825870646765e-08,
      "loss": 2.7283,
      "step": 4970
    },
    {
      "epoch": 24.776119402985074,
      "grad_norm": 8.410396575927734,
      "learning_rate": 6.903606965174129e-08,
      "loss": 2.6913,
      "step": 4980
    },
    {
      "epoch": 24.82587064676617,
      "grad_norm": 9.00861644744873,
      "learning_rate": 6.897388059701493e-08,
      "loss": 2.6361,
      "step": 4990
    },
    {
      "epoch": 24.875621890547265,
      "grad_norm": 17.330476760864258,
      "learning_rate": 6.891169154228855e-08,
      "loss": 2.6588,
      "step": 5000
    },
    {
      "epoch": 24.925373134328357,
      "grad_norm": 14.78081226348877,
      "learning_rate": 6.884950248756219e-08,
      "loss": 2.7023,
      "step": 5010
    },
    {
      "epoch": 24.975124378109452,
      "grad_norm": 12.674283981323242,
      "learning_rate": 6.878731343283581e-08,
      "loss": 2.7855,
      "step": 5020
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.7981959581375122,
      "eval_runtime": 55.055,
      "eval_samples_per_second": 3.651,
      "eval_steps_per_second": 0.236,
      "eval_wer": 0.6310250642673522,
      "step": 5025
    },
    {
      "epoch": 25.024875621890548,
      "grad_norm": 16.370162963867188,
      "learning_rate": 6.872512437810945e-08,
      "loss": 2.6893,
      "step": 5030
    },
    {
      "epoch": 25.074626865671643,
      "grad_norm": 9.400763511657715,
      "learning_rate": 6.866293532338308e-08,
      "loss": 2.6085,
      "step": 5040
    },
    {
      "epoch": 25.124378109452735,
      "grad_norm": 17.530288696289062,
      "learning_rate": 6.860074626865672e-08,
      "loss": 2.631,
      "step": 5050
    },
    {
      "epoch": 25.17412935323383,
      "grad_norm": 9.86412239074707,
      "learning_rate": 6.853855721393034e-08,
      "loss": 2.6309,
      "step": 5060
    },
    {
      "epoch": 25.223880597014926,
      "grad_norm": 28.03800392150879,
      "learning_rate": 6.847636815920398e-08,
      "loss": 2.709,
      "step": 5070
    },
    {
      "epoch": 25.27363184079602,
      "grad_norm": 28.003707885742188,
      "learning_rate": 6.84141791044776e-08,
      "loss": 2.542,
      "step": 5080
    },
    {
      "epoch": 25.323383084577113,
      "grad_norm": 10.297383308410645,
      "learning_rate": 6.835199004975124e-08,
      "loss": 2.6398,
      "step": 5090
    },
    {
      "epoch": 25.37313432835821,
      "grad_norm": 7.836884498596191,
      "learning_rate": 6.828980099502488e-08,
      "loss": 2.6622,
      "step": 5100
    },
    {
      "epoch": 25.422885572139304,
      "grad_norm": 9.284238815307617,
      "learning_rate": 6.82276119402985e-08,
      "loss": 2.7328,
      "step": 5110
    },
    {
      "epoch": 25.4726368159204,
      "grad_norm": 15.539955139160156,
      "learning_rate": 6.816542288557214e-08,
      "loss": 2.722,
      "step": 5120
    },
    {
      "epoch": 25.52238805970149,
      "grad_norm": 10.581624031066895,
      "learning_rate": 6.810323383084576e-08,
      "loss": 2.5888,
      "step": 5130
    },
    {
      "epoch": 25.572139303482587,
      "grad_norm": 14.005910873413086,
      "learning_rate": 6.804104477611941e-08,
      "loss": 2.6521,
      "step": 5140
    },
    {
      "epoch": 25.621890547263682,
      "grad_norm": 15.872185707092285,
      "learning_rate": 6.797885572139303e-08,
      "loss": 2.6455,
      "step": 5150
    },
    {
      "epoch": 25.671641791044777,
      "grad_norm": 6.8921427726745605,
      "learning_rate": 6.791666666666667e-08,
      "loss": 2.5949,
      "step": 5160
    },
    {
      "epoch": 25.72139303482587,
      "grad_norm": 18.93067741394043,
      "learning_rate": 6.785447761194029e-08,
      "loss": 2.6995,
      "step": 5170
    },
    {
      "epoch": 25.771144278606965,
      "grad_norm": 39.53017044067383,
      "learning_rate": 6.779228855721393e-08,
      "loss": 2.6593,
      "step": 5180
    },
    {
      "epoch": 25.82089552238806,
      "grad_norm": 13.698437690734863,
      "learning_rate": 6.773009950248755e-08,
      "loss": 2.6436,
      "step": 5190
    },
    {
      "epoch": 25.870646766169155,
      "grad_norm": 8.412054061889648,
      "learning_rate": 6.766791044776119e-08,
      "loss": 2.666,
      "step": 5200
    },
    {
      "epoch": 25.920398009950247,
      "grad_norm": 8.735100746154785,
      "learning_rate": 6.760572139303483e-08,
      "loss": 2.598,
      "step": 5210
    },
    {
      "epoch": 25.970149253731343,
      "grad_norm": 14.149540901184082,
      "learning_rate": 6.754353233830845e-08,
      "loss": 2.544,
      "step": 5220
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.7957767248153687,
      "eval_runtime": 55.8592,
      "eval_samples_per_second": 3.598,
      "eval_steps_per_second": 0.233,
      "eval_wer": 0.6304627249357326,
      "step": 5226
    },
    {
      "epoch": 26.019900497512438,
      "grad_norm": 35.88959884643555,
      "learning_rate": 6.748134328358209e-08,
      "loss": 2.6393,
      "step": 5230
    },
    {
      "epoch": 26.069651741293534,
      "grad_norm": 14.068130493164062,
      "learning_rate": 6.741915422885572e-08,
      "loss": 2.6366,
      "step": 5240
    },
    {
      "epoch": 26.119402985074625,
      "grad_norm": 9.715171813964844,
      "learning_rate": 6.735696517412936e-08,
      "loss": 2.5912,
      "step": 5250
    },
    {
      "epoch": 26.16915422885572,
      "grad_norm": 12.070926666259766,
      "learning_rate": 6.729477611940298e-08,
      "loss": 2.619,
      "step": 5260
    },
    {
      "epoch": 26.218905472636816,
      "grad_norm": 20.756362915039062,
      "learning_rate": 6.723258706467662e-08,
      "loss": 2.626,
      "step": 5270
    },
    {
      "epoch": 26.26865671641791,
      "grad_norm": 20.758230209350586,
      "learning_rate": 6.717039800995024e-08,
      "loss": 2.7485,
      "step": 5280
    },
    {
      "epoch": 26.318407960199004,
      "grad_norm": 9.456079483032227,
      "learning_rate": 6.710820895522388e-08,
      "loss": 2.682,
      "step": 5290
    },
    {
      "epoch": 26.3681592039801,
      "grad_norm": 9.880410194396973,
      "learning_rate": 6.704601990049751e-08,
      "loss": 2.6095,
      "step": 5300
    },
    {
      "epoch": 26.417910447761194,
      "grad_norm": 12.316943168640137,
      "learning_rate": 6.698383084577114e-08,
      "loss": 2.6288,
      "step": 5310
    },
    {
      "epoch": 26.46766169154229,
      "grad_norm": 16.045928955078125,
      "learning_rate": 6.692164179104477e-08,
      "loss": 2.5886,
      "step": 5320
    },
    {
      "epoch": 26.51741293532338,
      "grad_norm": 21.053909301757812,
      "learning_rate": 6.68594527363184e-08,
      "loss": 2.6618,
      "step": 5330
    },
    {
      "epoch": 26.567164179104477,
      "grad_norm": 11.22332763671875,
      "learning_rate": 6.679726368159205e-08,
      "loss": 2.6919,
      "step": 5340
    },
    {
      "epoch": 26.616915422885572,
      "grad_norm": 7.078033447265625,
      "learning_rate": 6.673507462686567e-08,
      "loss": 2.6274,
      "step": 5350
    },
    {
      "epoch": 26.666666666666668,
      "grad_norm": 12.237753868103027,
      "learning_rate": 6.667288557213931e-08,
      "loss": 2.6568,
      "step": 5360
    },
    {
      "epoch": 26.71641791044776,
      "grad_norm": 12.534139633178711,
      "learning_rate": 6.661069651741293e-08,
      "loss": 2.7289,
      "step": 5370
    },
    {
      "epoch": 26.766169154228855,
      "grad_norm": 52.787017822265625,
      "learning_rate": 6.654850746268657e-08,
      "loss": 2.5422,
      "step": 5380
    },
    {
      "epoch": 26.81592039800995,
      "grad_norm": 10.838604927062988,
      "learning_rate": 6.648631840796019e-08,
      "loss": 2.6815,
      "step": 5390
    },
    {
      "epoch": 26.865671641791046,
      "grad_norm": 9.170366287231445,
      "learning_rate": 6.642412935323383e-08,
      "loss": 2.7049,
      "step": 5400
    },
    {
      "epoch": 26.915422885572138,
      "grad_norm": 8.3944730758667,
      "learning_rate": 6.636194029850746e-08,
      "loss": 2.7013,
      "step": 5410
    },
    {
      "epoch": 26.965174129353233,
      "grad_norm": 11.281266212463379,
      "learning_rate": 6.629975124378109e-08,
      "loss": 2.6749,
      "step": 5420
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.7934943437576294,
      "eval_runtime": 54.6296,
      "eval_samples_per_second": 3.679,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6300610539845758,
      "step": 5427
    },
    {
      "epoch": 27.01492537313433,
      "grad_norm": 15.378066062927246,
      "learning_rate": 6.623756218905472e-08,
      "loss": 2.5592,
      "step": 5430
    },
    {
      "epoch": 27.064676616915424,
      "grad_norm": 9.342215538024902,
      "learning_rate": 6.617537313432836e-08,
      "loss": 2.5985,
      "step": 5440
    },
    {
      "epoch": 27.114427860696516,
      "grad_norm": 15.757919311523438,
      "learning_rate": 6.6113184079602e-08,
      "loss": 2.7102,
      "step": 5450
    },
    {
      "epoch": 27.16417910447761,
      "grad_norm": 11.894426345825195,
      "learning_rate": 6.605099502487562e-08,
      "loss": 2.7517,
      "step": 5460
    },
    {
      "epoch": 27.213930348258707,
      "grad_norm": 17.581579208374023,
      "learning_rate": 6.598880597014926e-08,
      "loss": 2.5164,
      "step": 5470
    },
    {
      "epoch": 27.263681592039802,
      "grad_norm": 10.667513847351074,
      "learning_rate": 6.592661691542288e-08,
      "loss": 2.6895,
      "step": 5480
    },
    {
      "epoch": 27.313432835820894,
      "grad_norm": 10.167531967163086,
      "learning_rate": 6.586442786069652e-08,
      "loss": 2.6312,
      "step": 5490
    },
    {
      "epoch": 27.36318407960199,
      "grad_norm": 18.45060920715332,
      "learning_rate": 6.580223880597014e-08,
      "loss": 2.5964,
      "step": 5500
    },
    {
      "epoch": 27.412935323383085,
      "grad_norm": 25.513273239135742,
      "learning_rate": 6.574004975124378e-08,
      "loss": 2.5926,
      "step": 5510
    },
    {
      "epoch": 27.46268656716418,
      "grad_norm": 5.766808032989502,
      "learning_rate": 6.567786069651741e-08,
      "loss": 2.6808,
      "step": 5520
    },
    {
      "epoch": 27.512437810945272,
      "grad_norm": 16.574054718017578,
      "learning_rate": 6.561567164179104e-08,
      "loss": 2.6392,
      "step": 5530
    },
    {
      "epoch": 27.562189054726367,
      "grad_norm": 11.464922904968262,
      "learning_rate": 6.555348258706468e-08,
      "loss": 2.6228,
      "step": 5540
    },
    {
      "epoch": 27.611940298507463,
      "grad_norm": 17.155290603637695,
      "learning_rate": 6.549129353233831e-08,
      "loss": 2.665,
      "step": 5550
    },
    {
      "epoch": 27.66169154228856,
      "grad_norm": 13.365473747253418,
      "learning_rate": 6.542910447761194e-08,
      "loss": 2.6814,
      "step": 5560
    },
    {
      "epoch": 27.71144278606965,
      "grad_norm": 10.925702095031738,
      "learning_rate": 6.536691542288557e-08,
      "loss": 2.6583,
      "step": 5570
    },
    {
      "epoch": 27.761194029850746,
      "grad_norm": 15.87096118927002,
      "learning_rate": 6.53047263681592e-08,
      "loss": 2.7166,
      "step": 5580
    },
    {
      "epoch": 27.81094527363184,
      "grad_norm": 10.825871467590332,
      "learning_rate": 6.524253731343283e-08,
      "loss": 2.5404,
      "step": 5590
    },
    {
      "epoch": 27.860696517412936,
      "grad_norm": 7.766041278839111,
      "learning_rate": 6.518034825870646e-08,
      "loss": 2.5369,
      "step": 5600
    },
    {
      "epoch": 27.91044776119403,
      "grad_norm": 12.348503112792969,
      "learning_rate": 6.511815920398009e-08,
      "loss": 2.6025,
      "step": 5610
    },
    {
      "epoch": 27.960199004975124,
      "grad_norm": 8.629049301147461,
      "learning_rate": 6.505597014925372e-08,
      "loss": 2.6632,
      "step": 5620
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.789375901222229,
      "eval_runtime": 54.314,
      "eval_samples_per_second": 3.701,
      "eval_steps_per_second": 0.239,
      "eval_wer": 0.6289363753213367,
      "step": 5628
    },
    {
      "epoch": 28.00995024875622,
      "grad_norm": 9.033918380737305,
      "learning_rate": 6.499378109452736e-08,
      "loss": 2.7478,
      "step": 5630
    },
    {
      "epoch": 28.059701492537314,
      "grad_norm": 20.10233497619629,
      "learning_rate": 6.4931592039801e-08,
      "loss": 2.6936,
      "step": 5640
    },
    {
      "epoch": 28.109452736318406,
      "grad_norm": 10.704511642456055,
      "learning_rate": 6.486940298507463e-08,
      "loss": 2.6338,
      "step": 5650
    },
    {
      "epoch": 28.1592039800995,
      "grad_norm": 10.21507740020752,
      "learning_rate": 6.480721393034826e-08,
      "loss": 2.6862,
      "step": 5660
    },
    {
      "epoch": 28.208955223880597,
      "grad_norm": 38.12730407714844,
      "learning_rate": 6.47450248756219e-08,
      "loss": 2.6528,
      "step": 5670
    },
    {
      "epoch": 28.258706467661693,
      "grad_norm": 8.49500846862793,
      "learning_rate": 6.468283582089552e-08,
      "loss": 2.6575,
      "step": 5680
    },
    {
      "epoch": 28.308457711442784,
      "grad_norm": 8.285677909851074,
      "learning_rate": 6.462064676616915e-08,
      "loss": 2.5881,
      "step": 5690
    },
    {
      "epoch": 28.35820895522388,
      "grad_norm": 16.21595573425293,
      "learning_rate": 6.455845771144278e-08,
      "loss": 2.6181,
      "step": 5700
    },
    {
      "epoch": 28.407960199004975,
      "grad_norm": 12.977173805236816,
      "learning_rate": 6.449626865671641e-08,
      "loss": 2.6419,
      "step": 5710
    },
    {
      "epoch": 28.45771144278607,
      "grad_norm": 20.23381996154785,
      "learning_rate": 6.443407960199005e-08,
      "loss": 2.6692,
      "step": 5720
    },
    {
      "epoch": 28.507462686567163,
      "grad_norm": 10.044376373291016,
      "learning_rate": 6.437189054726367e-08,
      "loss": 2.5744,
      "step": 5730
    },
    {
      "epoch": 28.557213930348258,
      "grad_norm": 7.872811317443848,
      "learning_rate": 6.430970149253731e-08,
      "loss": 2.6145,
      "step": 5740
    },
    {
      "epoch": 28.606965174129353,
      "grad_norm": 12.497659683227539,
      "learning_rate": 6.424751243781095e-08,
      "loss": 2.6226,
      "step": 5750
    },
    {
      "epoch": 28.65671641791045,
      "grad_norm": 11.626453399658203,
      "learning_rate": 6.418532338308458e-08,
      "loss": 2.6522,
      "step": 5760
    },
    {
      "epoch": 28.70646766169154,
      "grad_norm": 13.302453994750977,
      "learning_rate": 6.41231343283582e-08,
      "loss": 2.5603,
      "step": 5770
    },
    {
      "epoch": 28.756218905472636,
      "grad_norm": 12.99953556060791,
      "learning_rate": 6.406094527363184e-08,
      "loss": 2.5736,
      "step": 5780
    },
    {
      "epoch": 28.80597014925373,
      "grad_norm": 17.218032836914062,
      "learning_rate": 6.399875621890547e-08,
      "loss": 2.536,
      "step": 5790
    },
    {
      "epoch": 28.855721393034827,
      "grad_norm": 7.551255702972412,
      "learning_rate": 6.39365671641791e-08,
      "loss": 2.5772,
      "step": 5800
    },
    {
      "epoch": 28.90547263681592,
      "grad_norm": 11.540072441101074,
      "learning_rate": 6.387437810945273e-08,
      "loss": 2.7215,
      "step": 5810
    },
    {
      "epoch": 28.955223880597014,
      "grad_norm": 7.2066802978515625,
      "learning_rate": 6.381218905472636e-08,
      "loss": 2.6736,
      "step": 5820
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.7860918045043945,
      "eval_runtime": 55.165,
      "eval_samples_per_second": 3.644,
      "eval_steps_per_second": 0.236,
      "eval_wer": 0.6286150385604113,
      "step": 5829
    },
    {
      "epoch": 29.00497512437811,
      "grad_norm": 11.017903327941895,
      "learning_rate": 6.375e-08,
      "loss": 2.638,
      "step": 5830
    },
    {
      "epoch": 29.054726368159205,
      "grad_norm": 30.517723083496094,
      "learning_rate": 6.368781094527362e-08,
      "loss": 2.6948,
      "step": 5840
    },
    {
      "epoch": 29.104477611940297,
      "grad_norm": 7.036545753479004,
      "learning_rate": 6.362562189054727e-08,
      "loss": 2.6354,
      "step": 5850
    },
    {
      "epoch": 29.154228855721392,
      "grad_norm": 12.10776424407959,
      "learning_rate": 6.35634328358209e-08,
      "loss": 2.5357,
      "step": 5860
    },
    {
      "epoch": 29.203980099502488,
      "grad_norm": 6.785757541656494,
      "learning_rate": 6.350124378109453e-08,
      "loss": 2.4927,
      "step": 5870
    },
    {
      "epoch": 29.253731343283583,
      "grad_norm": 12.170220375061035,
      "learning_rate": 6.343905472636815e-08,
      "loss": 2.6458,
      "step": 5880
    },
    {
      "epoch": 29.303482587064675,
      "grad_norm": 13.616106033325195,
      "learning_rate": 6.337686567164179e-08,
      "loss": 2.6132,
      "step": 5890
    },
    {
      "epoch": 29.35323383084577,
      "grad_norm": 11.522642135620117,
      "learning_rate": 6.331467661691541e-08,
      "loss": 2.6397,
      "step": 5900
    },
    {
      "epoch": 29.402985074626866,
      "grad_norm": 10.218388557434082,
      "learning_rate": 6.325248756218905e-08,
      "loss": 2.6201,
      "step": 5910
    },
    {
      "epoch": 29.45273631840796,
      "grad_norm": 7.152180194854736,
      "learning_rate": 6.319029850746267e-08,
      "loss": 2.6611,
      "step": 5920
    },
    {
      "epoch": 29.502487562189053,
      "grad_norm": 10.831477165222168,
      "learning_rate": 6.312810945273631e-08,
      "loss": 2.7328,
      "step": 5930
    },
    {
      "epoch": 29.55223880597015,
      "grad_norm": 13.973175048828125,
      "learning_rate": 6.306592039800995e-08,
      "loss": 2.7043,
      "step": 5940
    },
    {
      "epoch": 29.601990049751244,
      "grad_norm": 17.929553985595703,
      "learning_rate": 6.300373134328358e-08,
      "loss": 2.6005,
      "step": 5950
    },
    {
      "epoch": 29.65174129353234,
      "grad_norm": 9.875418663024902,
      "learning_rate": 6.294154228855722e-08,
      "loss": 2.6735,
      "step": 5960
    },
    {
      "epoch": 29.701492537313435,
      "grad_norm": 9.753408432006836,
      "learning_rate": 6.287935323383084e-08,
      "loss": 2.6204,
      "step": 5970
    },
    {
      "epoch": 29.751243781094526,
      "grad_norm": 22.655977249145508,
      "learning_rate": 6.281716417910448e-08,
      "loss": 2.6646,
      "step": 5980
    },
    {
      "epoch": 29.800995024875622,
      "grad_norm": 13.50393295288086,
      "learning_rate": 6.27549751243781e-08,
      "loss": 2.6839,
      "step": 5990
    },
    {
      "epoch": 29.850746268656717,
      "grad_norm": 10.379850387573242,
      "learning_rate": 6.269278606965174e-08,
      "loss": 2.5679,
      "step": 6000
    },
    {
      "epoch": 29.90049751243781,
      "grad_norm": 16.894765853881836,
      "learning_rate": 6.263059701492536e-08,
      "loss": 2.653,
      "step": 6010
    },
    {
      "epoch": 29.950248756218905,
      "grad_norm": 7.231680870056152,
      "learning_rate": 6.2568407960199e-08,
      "loss": 2.6459,
      "step": 6020
    },
    {
      "epoch": 30.0,
      "grad_norm": 15.845741271972656,
      "learning_rate": 6.250621890547264e-08,
      "loss": 2.5445,
      "step": 6030
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.7834149599075317,
      "eval_runtime": 54.7346,
      "eval_samples_per_second": 3.672,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6286150385604113,
      "step": 6030
    },
    {
      "epoch": 30.049751243781095,
      "grad_norm": 7.755004405975342,
      "learning_rate": 6.244402985074626e-08,
      "loss": 2.6152,
      "step": 6040
    },
    {
      "epoch": 30.09950248756219,
      "grad_norm": 23.789615631103516,
      "learning_rate": 6.23818407960199e-08,
      "loss": 2.4707,
      "step": 6050
    },
    {
      "epoch": 30.149253731343283,
      "grad_norm": 10.21900463104248,
      "learning_rate": 6.231965174129353e-08,
      "loss": 2.6551,
      "step": 6060
    },
    {
      "epoch": 30.199004975124378,
      "grad_norm": 8.307355880737305,
      "learning_rate": 6.225746268656717e-08,
      "loss": 2.6533,
      "step": 6070
    },
    {
      "epoch": 30.248756218905474,
      "grad_norm": 12.767601013183594,
      "learning_rate": 6.219527363184079e-08,
      "loss": 2.6089,
      "step": 6080
    },
    {
      "epoch": 30.298507462686565,
      "grad_norm": 13.625951766967773,
      "learning_rate": 6.213308457711443e-08,
      "loss": 2.5697,
      "step": 6090
    },
    {
      "epoch": 30.34825870646766,
      "grad_norm": 11.216593742370605,
      "learning_rate": 6.207089552238805e-08,
      "loss": 2.6308,
      "step": 6100
    },
    {
      "epoch": 30.398009950248756,
      "grad_norm": 16.55826187133789,
      "learning_rate": 6.200870646766169e-08,
      "loss": 2.6401,
      "step": 6110
    },
    {
      "epoch": 30.44776119402985,
      "grad_norm": 11.379476547241211,
      "learning_rate": 6.194651741293531e-08,
      "loss": 2.6523,
      "step": 6120
    },
    {
      "epoch": 30.497512437810947,
      "grad_norm": 12.508578300476074,
      "learning_rate": 6.188432835820895e-08,
      "loss": 2.6333,
      "step": 6130
    },
    {
      "epoch": 30.54726368159204,
      "grad_norm": 7.892734527587891,
      "learning_rate": 6.182213930348258e-08,
      "loss": 2.6009,
      "step": 6140
    },
    {
      "epoch": 30.597014925373134,
      "grad_norm": 6.041556358337402,
      "learning_rate": 6.175995024875622e-08,
      "loss": 2.6566,
      "step": 6150
    },
    {
      "epoch": 30.64676616915423,
      "grad_norm": 23.970027923583984,
      "learning_rate": 6.169776119402986e-08,
      "loss": 2.6634,
      "step": 6160
    },
    {
      "epoch": 30.696517412935325,
      "grad_norm": 6.597024917602539,
      "learning_rate": 6.163557213930348e-08,
      "loss": 2.6435,
      "step": 6170
    },
    {
      "epoch": 30.746268656716417,
      "grad_norm": 21.87006187438965,
      "learning_rate": 6.157338308457712e-08,
      "loss": 2.7623,
      "step": 6180
    },
    {
      "epoch": 30.796019900497512,
      "grad_norm": 13.698311805725098,
      "learning_rate": 6.151119402985074e-08,
      "loss": 2.5823,
      "step": 6190
    },
    {
      "epoch": 30.845771144278608,
      "grad_norm": 29.407535552978516,
      "learning_rate": 6.144900497512438e-08,
      "loss": 2.6085,
      "step": 6200
    },
    {
      "epoch": 30.895522388059703,
      "grad_norm": 20.02312469482422,
      "learning_rate": 6.1386815920398e-08,
      "loss": 2.6209,
      "step": 6210
    },
    {
      "epoch": 30.945273631840795,
      "grad_norm": 22.38939666748047,
      "learning_rate": 6.132462686567164e-08,
      "loss": 2.659,
      "step": 6220
    },
    {
      "epoch": 30.99502487562189,
      "grad_norm": 13.520359992980957,
      "learning_rate": 6.126243781094526e-08,
      "loss": 2.6371,
      "step": 6230
    },
    {
      "epoch": 31.0,
      "eval_loss": 1.7834469079971313,
      "eval_runtime": 53.922,
      "eval_samples_per_second": 3.728,
      "eval_steps_per_second": 0.241,
      "eval_wer": 0.6270886889460154,
      "step": 6231
    },
    {
      "epoch": 31.044776119402986,
      "grad_norm": 21.206241607666016,
      "learning_rate": 6.12002487562189e-08,
      "loss": 2.6385,
      "step": 6240
    },
    {
      "epoch": 31.09452736318408,
      "grad_norm": 13.373249053955078,
      "learning_rate": 6.113805970149253e-08,
      "loss": 2.5887,
      "step": 6250
    },
    {
      "epoch": 31.144278606965173,
      "grad_norm": 10.399007797241211,
      "learning_rate": 6.107587064676617e-08,
      "loss": 2.6958,
      "step": 6260
    },
    {
      "epoch": 31.19402985074627,
      "grad_norm": 6.768477439880371,
      "learning_rate": 6.101368159203981e-08,
      "loss": 2.6362,
      "step": 6270
    },
    {
      "epoch": 31.243781094527364,
      "grad_norm": 25.239398956298828,
      "learning_rate": 6.095149253731343e-08,
      "loss": 2.6905,
      "step": 6280
    },
    {
      "epoch": 31.29353233830846,
      "grad_norm": 6.2428364753723145,
      "learning_rate": 6.088930348258707e-08,
      "loss": 2.5774,
      "step": 6290
    },
    {
      "epoch": 31.34328358208955,
      "grad_norm": 13.997161865234375,
      "learning_rate": 6.082711442786069e-08,
      "loss": 2.7437,
      "step": 6300
    },
    {
      "epoch": 31.393034825870647,
      "grad_norm": 12.655309677124023,
      "learning_rate": 6.076492537313433e-08,
      "loss": 2.642,
      "step": 6310
    },
    {
      "epoch": 31.442786069651742,
      "grad_norm": 15.60955810546875,
      "learning_rate": 6.070273631840795e-08,
      "loss": 2.5485,
      "step": 6320
    },
    {
      "epoch": 31.492537313432837,
      "grad_norm": 30.294824600219727,
      "learning_rate": 6.064054726368159e-08,
      "loss": 2.576,
      "step": 6330
    },
    {
      "epoch": 31.54228855721393,
      "grad_norm": 10.66919994354248,
      "learning_rate": 6.057835820895521e-08,
      "loss": 2.5681,
      "step": 6340
    },
    {
      "epoch": 31.592039800995025,
      "grad_norm": 16.006010055541992,
      "learning_rate": 6.051616915422886e-08,
      "loss": 2.6211,
      "step": 6350
    },
    {
      "epoch": 31.64179104477612,
      "grad_norm": 7.187403202056885,
      "learning_rate": 6.045398009950248e-08,
      "loss": 2.6664,
      "step": 6360
    },
    {
      "epoch": 31.691542288557216,
      "grad_norm": 14.802962303161621,
      "learning_rate": 6.039179104477612e-08,
      "loss": 2.5677,
      "step": 6370
    },
    {
      "epoch": 31.741293532338307,
      "grad_norm": 14.542182922363281,
      "learning_rate": 6.032960199004976e-08,
      "loss": 2.6346,
      "step": 6380
    },
    {
      "epoch": 31.791044776119403,
      "grad_norm": 9.996461868286133,
      "learning_rate": 6.026741293532338e-08,
      "loss": 2.6763,
      "step": 6390
    },
    {
      "epoch": 31.8407960199005,
      "grad_norm": 9.718449592590332,
      "learning_rate": 6.020522388059702e-08,
      "loss": 2.6163,
      "step": 6400
    },
    {
      "epoch": 31.890547263681594,
      "grad_norm": 9.190400123596191,
      "learning_rate": 6.014303482587064e-08,
      "loss": 2.5766,
      "step": 6410
    },
    {
      "epoch": 31.940298507462686,
      "grad_norm": 9.442203521728516,
      "learning_rate": 6.008084577114428e-08,
      "loss": 2.6649,
      "step": 6420
    },
    {
      "epoch": 31.99004975124378,
      "grad_norm": 6.169527053833008,
      "learning_rate": 6.00186567164179e-08,
      "loss": 2.5827,
      "step": 6430
    },
    {
      "epoch": 32.0,
      "eval_loss": 1.7814257144927979,
      "eval_runtime": 54.5647,
      "eval_samples_per_second": 3.684,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6271690231362468,
      "step": 6432
    },
    {
      "epoch": 32.039800995024876,
      "grad_norm": 8.211323738098145,
      "learning_rate": 5.995646766169153e-08,
      "loss": 2.5016,
      "step": 6440
    },
    {
      "epoch": 32.08955223880597,
      "grad_norm": 8.417597770690918,
      "learning_rate": 5.989427860696517e-08,
      "loss": 2.6739,
      "step": 6450
    },
    {
      "epoch": 32.13930348258707,
      "grad_norm": 8.343868255615234,
      "learning_rate": 5.983208955223881e-08,
      "loss": 2.6186,
      "step": 6460
    },
    {
      "epoch": 32.18905472636816,
      "grad_norm": 13.948975563049316,
      "learning_rate": 5.976990049751243e-08,
      "loss": 2.6114,
      "step": 6470
    },
    {
      "epoch": 32.23880597014925,
      "grad_norm": 7.683067798614502,
      "learning_rate": 5.970771144278607e-08,
      "loss": 2.6238,
      "step": 6480
    },
    {
      "epoch": 32.288557213930346,
      "grad_norm": 15.380043983459473,
      "learning_rate": 5.96455223880597e-08,
      "loss": 2.6743,
      "step": 6490
    },
    {
      "epoch": 32.33830845771144,
      "grad_norm": 11.964369773864746,
      "learning_rate": 5.958333333333333e-08,
      "loss": 2.6293,
      "step": 6500
    },
    {
      "epoch": 32.38805970149254,
      "grad_norm": 9.838786125183105,
      "learning_rate": 5.952114427860696e-08,
      "loss": 2.4903,
      "step": 6510
    },
    {
      "epoch": 32.43781094527363,
      "grad_norm": 9.00598430633545,
      "learning_rate": 5.9458955223880594e-08,
      "loss": 2.7181,
      "step": 6520
    },
    {
      "epoch": 32.48756218905473,
      "grad_norm": 10.58937931060791,
      "learning_rate": 5.9396766169154224e-08,
      "loss": 2.6293,
      "step": 6530
    },
    {
      "epoch": 32.53731343283582,
      "grad_norm": 12.814574241638184,
      "learning_rate": 5.9334577114427854e-08,
      "loss": 2.575,
      "step": 6540
    },
    {
      "epoch": 32.58706467661692,
      "grad_norm": 13.804804801940918,
      "learning_rate": 5.92723880597015e-08,
      "loss": 2.6696,
      "step": 6550
    },
    {
      "epoch": 32.63681592039801,
      "grad_norm": 11.429336547851562,
      "learning_rate": 5.921019900497513e-08,
      "loss": 2.5512,
      "step": 6560
    },
    {
      "epoch": 32.6865671641791,
      "grad_norm": 17.10361671447754,
      "learning_rate": 5.914800995024876e-08,
      "loss": 2.6054,
      "step": 6570
    },
    {
      "epoch": 32.7363184079602,
      "grad_norm": 11.651742935180664,
      "learning_rate": 5.9085820895522387e-08,
      "loss": 2.6426,
      "step": 6580
    },
    {
      "epoch": 32.78606965174129,
      "grad_norm": 14.436169624328613,
      "learning_rate": 5.9023631840796017e-08,
      "loss": 2.6463,
      "step": 6590
    },
    {
      "epoch": 32.83582089552239,
      "grad_norm": 26.198091506958008,
      "learning_rate": 5.8961442786069646e-08,
      "loss": 2.6467,
      "step": 6600
    },
    {
      "epoch": 32.885572139303484,
      "grad_norm": 9.945180892944336,
      "learning_rate": 5.8899253731343276e-08,
      "loss": 2.6199,
      "step": 6610
    },
    {
      "epoch": 32.93532338308458,
      "grad_norm": 12.96552562713623,
      "learning_rate": 5.883706467661691e-08,
      "loss": 2.6409,
      "step": 6620
    },
    {
      "epoch": 32.985074626865675,
      "grad_norm": 11.610391616821289,
      "learning_rate": 5.877487562189054e-08,
      "loss": 2.6586,
      "step": 6630
    },
    {
      "epoch": 33.0,
      "eval_loss": 1.7790011167526245,
      "eval_runtime": 54.585,
      "eval_samples_per_second": 3.682,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6269280205655527,
      "step": 6633
    },
    {
      "epoch": 33.03482587064676,
      "grad_norm": 10.035285949707031,
      "learning_rate": 5.871268656716417e-08,
      "loss": 2.6559,
      "step": 6640
    },
    {
      "epoch": 33.08457711442786,
      "grad_norm": 11.339408874511719,
      "learning_rate": 5.8650497512437816e-08,
      "loss": 2.6439,
      "step": 6650
    },
    {
      "epoch": 33.134328358208954,
      "grad_norm": 9.932672500610352,
      "learning_rate": 5.8588308457711446e-08,
      "loss": 2.6312,
      "step": 6660
    },
    {
      "epoch": 33.18407960199005,
      "grad_norm": 18.24337387084961,
      "learning_rate": 5.8526119402985076e-08,
      "loss": 2.6803,
      "step": 6670
    },
    {
      "epoch": 33.233830845771145,
      "grad_norm": 24.40298843383789,
      "learning_rate": 5.8463930348258705e-08,
      "loss": 2.5815,
      "step": 6680
    },
    {
      "epoch": 33.28358208955224,
      "grad_norm": 11.778960227966309,
      "learning_rate": 5.8401741293532335e-08,
      "loss": 2.5909,
      "step": 6690
    },
    {
      "epoch": 33.333333333333336,
      "grad_norm": 30.013906478881836,
      "learning_rate": 5.8339552238805965e-08,
      "loss": 2.5406,
      "step": 6700
    },
    {
      "epoch": 33.38308457711443,
      "grad_norm": 13.237077713012695,
      "learning_rate": 5.8277363184079595e-08,
      "loss": 2.6434,
      "step": 6710
    },
    {
      "epoch": 33.43283582089552,
      "grad_norm": 9.938166618347168,
      "learning_rate": 5.8215174129353225e-08,
      "loss": 2.6493,
      "step": 6720
    },
    {
      "epoch": 33.482587064676615,
      "grad_norm": 9.665926933288574,
      "learning_rate": 5.815298507462686e-08,
      "loss": 2.6049,
      "step": 6730
    },
    {
      "epoch": 33.53233830845771,
      "grad_norm": 11.094503402709961,
      "learning_rate": 5.809079601990049e-08,
      "loss": 2.5115,
      "step": 6740
    },
    {
      "epoch": 33.582089552238806,
      "grad_norm": 12.463808059692383,
      "learning_rate": 5.8028606965174135e-08,
      "loss": 2.651,
      "step": 6750
    },
    {
      "epoch": 33.6318407960199,
      "grad_norm": 11.108871459960938,
      "learning_rate": 5.7966417910447765e-08,
      "loss": 2.6281,
      "step": 6760
    },
    {
      "epoch": 33.681592039801,
      "grad_norm": 23.47173500061035,
      "learning_rate": 5.7904228855721394e-08,
      "loss": 2.6163,
      "step": 6770
    },
    {
      "epoch": 33.73134328358209,
      "grad_norm": 13.507646560668945,
      "learning_rate": 5.7842039800995024e-08,
      "loss": 2.6899,
      "step": 6780
    },
    {
      "epoch": 33.78109452736319,
      "grad_norm": 15.533875465393066,
      "learning_rate": 5.7779850746268654e-08,
      "loss": 2.5722,
      "step": 6790
    },
    {
      "epoch": 33.830845771144276,
      "grad_norm": 9.969374656677246,
      "learning_rate": 5.7717661691542284e-08,
      "loss": 2.7075,
      "step": 6800
    },
    {
      "epoch": 33.88059701492537,
      "grad_norm": 8.095727920532227,
      "learning_rate": 5.7655472636815914e-08,
      "loss": 2.6919,
      "step": 6810
    },
    {
      "epoch": 33.930348258706466,
      "grad_norm": 17.30046272277832,
      "learning_rate": 5.7593283582089544e-08,
      "loss": 2.6002,
      "step": 6820
    },
    {
      "epoch": 33.98009950248756,
      "grad_norm": 8.778377532958984,
      "learning_rate": 5.753109452736318e-08,
      "loss": 2.5789,
      "step": 6830
    },
    {
      "epoch": 34.0,
      "eval_loss": 1.7781848907470703,
      "eval_runtime": 57.9857,
      "eval_samples_per_second": 3.466,
      "eval_steps_per_second": 0.224,
      "eval_wer": 0.6264460154241646,
      "step": 6834
    },
    {
      "epoch": 34.02985074626866,
      "grad_norm": 8.551222801208496,
      "learning_rate": 5.746890547263681e-08,
      "loss": 2.6907,
      "step": 6840
    },
    {
      "epoch": 34.07960199004975,
      "grad_norm": 23.291643142700195,
      "learning_rate": 5.740671641791045e-08,
      "loss": 2.5548,
      "step": 6850
    },
    {
      "epoch": 34.12935323383085,
      "grad_norm": 7.490100860595703,
      "learning_rate": 5.7344527363184083e-08,
      "loss": 2.6282,
      "step": 6860
    },
    {
      "epoch": 34.17910447761194,
      "grad_norm": 9.54311752319336,
      "learning_rate": 5.728233830845771e-08,
      "loss": 2.6004,
      "step": 6870
    },
    {
      "epoch": 34.22885572139303,
      "grad_norm": 7.339991092681885,
      "learning_rate": 5.722014925373134e-08,
      "loss": 2.6966,
      "step": 6880
    },
    {
      "epoch": 34.27860696517413,
      "grad_norm": 8.284991264343262,
      "learning_rate": 5.715796019900497e-08,
      "loss": 2.7076,
      "step": 6890
    },
    {
      "epoch": 34.32835820895522,
      "grad_norm": 16.374467849731445,
      "learning_rate": 5.70957711442786e-08,
      "loss": 2.6649,
      "step": 6900
    },
    {
      "epoch": 34.37810945273632,
      "grad_norm": 8.314004898071289,
      "learning_rate": 5.703358208955223e-08,
      "loss": 2.5871,
      "step": 6910
    },
    {
      "epoch": 34.42786069651741,
      "grad_norm": 12.507616996765137,
      "learning_rate": 5.697139303482586e-08,
      "loss": 2.5749,
      "step": 6920
    },
    {
      "epoch": 34.47761194029851,
      "grad_norm": 13.54100513458252,
      "learning_rate": 5.69092039800995e-08,
      "loss": 2.4763,
      "step": 6930
    },
    {
      "epoch": 34.527363184079604,
      "grad_norm": 8.322944641113281,
      "learning_rate": 5.684701492537313e-08,
      "loss": 2.5705,
      "step": 6940
    },
    {
      "epoch": 34.5771144278607,
      "grad_norm": 9.9927978515625,
      "learning_rate": 5.6784825870646766e-08,
      "loss": 2.5588,
      "step": 6950
    },
    {
      "epoch": 34.62686567164179,
      "grad_norm": 10.265663146972656,
      "learning_rate": 5.67226368159204e-08,
      "loss": 2.6931,
      "step": 6960
    },
    {
      "epoch": 34.67661691542288,
      "grad_norm": 9.537723541259766,
      "learning_rate": 5.666044776119403e-08,
      "loss": 2.5702,
      "step": 6970
    },
    {
      "epoch": 34.72636815920398,
      "grad_norm": 28.04326057434082,
      "learning_rate": 5.659825870646766e-08,
      "loss": 2.6292,
      "step": 6980
    },
    {
      "epoch": 34.776119402985074,
      "grad_norm": 9.488061904907227,
      "learning_rate": 5.653606965174129e-08,
      "loss": 2.5863,
      "step": 6990
    },
    {
      "epoch": 34.82587064676617,
      "grad_norm": 9.19294261932373,
      "learning_rate": 5.647388059701492e-08,
      "loss": 2.6396,
      "step": 7000
    },
    {
      "epoch": 34.875621890547265,
      "grad_norm": 14.221977233886719,
      "learning_rate": 5.641169154228855e-08,
      "loss": 2.6069,
      "step": 7010
    },
    {
      "epoch": 34.92537313432836,
      "grad_norm": 8.435938835144043,
      "learning_rate": 5.634950248756218e-08,
      "loss": 2.5685,
      "step": 7020
    },
    {
      "epoch": 34.975124378109456,
      "grad_norm": 7.724143028259277,
      "learning_rate": 5.628731343283581e-08,
      "loss": 2.5473,
      "step": 7030
    },
    {
      "epoch": 35.0,
      "eval_loss": 1.7749300003051758,
      "eval_runtime": 55.2448,
      "eval_samples_per_second": 3.638,
      "eval_steps_per_second": 0.235,
      "eval_wer": 0.6254820051413882,
      "step": 7035
    },
    {
      "epoch": 35.024875621890544,
      "grad_norm": 29.045055389404297,
      "learning_rate": 5.622512437810945e-08,
      "loss": 2.5357,
      "step": 7040
    },
    {
      "epoch": 35.07462686567164,
      "grad_norm": 11.299015998840332,
      "learning_rate": 5.6162935323383085e-08,
      "loss": 2.6043,
      "step": 7050
    },
    {
      "epoch": 35.124378109452735,
      "grad_norm": 6.657975196838379,
      "learning_rate": 5.610074626865672e-08,
      "loss": 2.6824,
      "step": 7060
    },
    {
      "epoch": 35.17412935323383,
      "grad_norm": 6.705875873565674,
      "learning_rate": 5.603855721393035e-08,
      "loss": 2.5994,
      "step": 7070
    },
    {
      "epoch": 35.223880597014926,
      "grad_norm": 19.759552001953125,
      "learning_rate": 5.597636815920398e-08,
      "loss": 2.6544,
      "step": 7080
    },
    {
      "epoch": 35.27363184079602,
      "grad_norm": 20.49807357788086,
      "learning_rate": 5.591417910447761e-08,
      "loss": 2.5367,
      "step": 7090
    },
    {
      "epoch": 35.32338308457712,
      "grad_norm": 10.018224716186523,
      "learning_rate": 5.585199004975124e-08,
      "loss": 2.6304,
      "step": 7100
    },
    {
      "epoch": 35.37313432835821,
      "grad_norm": 28.945011138916016,
      "learning_rate": 5.578980099502487e-08,
      "loss": 2.4956,
      "step": 7110
    },
    {
      "epoch": 35.4228855721393,
      "grad_norm": 19.338193893432617,
      "learning_rate": 5.57276119402985e-08,
      "loss": 2.4899,
      "step": 7120
    },
    {
      "epoch": 35.472636815920396,
      "grad_norm": 8.161971092224121,
      "learning_rate": 5.566542288557213e-08,
      "loss": 2.6273,
      "step": 7130
    },
    {
      "epoch": 35.52238805970149,
      "grad_norm": 32.55045700073242,
      "learning_rate": 5.560323383084577e-08,
      "loss": 2.4829,
      "step": 7140
    },
    {
      "epoch": 35.57213930348259,
      "grad_norm": 26.211578369140625,
      "learning_rate": 5.5541044776119403e-08,
      "loss": 2.8085,
      "step": 7150
    },
    {
      "epoch": 35.62189054726368,
      "grad_norm": 12.987418174743652,
      "learning_rate": 5.5478855721393033e-08,
      "loss": 2.5186,
      "step": 7160
    },
    {
      "epoch": 35.67164179104478,
      "grad_norm": 10.110004425048828,
      "learning_rate": 5.541666666666667e-08,
      "loss": 2.6603,
      "step": 7170
    },
    {
      "epoch": 35.72139303482587,
      "grad_norm": 9.895519256591797,
      "learning_rate": 5.53544776119403e-08,
      "loss": 2.6604,
      "step": 7180
    },
    {
      "epoch": 35.77114427860697,
      "grad_norm": 22.647619247436523,
      "learning_rate": 5.529228855721393e-08,
      "loss": 2.6545,
      "step": 7190
    },
    {
      "epoch": 35.82089552238806,
      "grad_norm": 12.837747573852539,
      "learning_rate": 5.523009950248756e-08,
      "loss": 2.6206,
      "step": 7200
    },
    {
      "epoch": 35.87064676616915,
      "grad_norm": 6.581119060516357,
      "learning_rate": 5.516791044776119e-08,
      "loss": 2.5301,
      "step": 7210
    },
    {
      "epoch": 35.92039800995025,
      "grad_norm": 9.875165939331055,
      "learning_rate": 5.510572139303482e-08,
      "loss": 2.6098,
      "step": 7220
    },
    {
      "epoch": 35.97014925373134,
      "grad_norm": 7.493948936462402,
      "learning_rate": 5.504353233830845e-08,
      "loss": 2.5053,
      "step": 7230
    },
    {
      "epoch": 36.0,
      "eval_loss": 1.7739043235778809,
      "eval_runtime": 54.5522,
      "eval_samples_per_second": 3.685,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6254016709511568,
      "step": 7236
    },
    {
      "epoch": 36.01990049751244,
      "grad_norm": 6.66169548034668,
      "learning_rate": 5.498134328358208e-08,
      "loss": 2.5854,
      "step": 7240
    },
    {
      "epoch": 36.069651741293534,
      "grad_norm": 11.84201431274414,
      "learning_rate": 5.491915422885572e-08,
      "loss": 2.5888,
      "step": 7250
    },
    {
      "epoch": 36.11940298507463,
      "grad_norm": 10.041654586791992,
      "learning_rate": 5.485696517412935e-08,
      "loss": 2.5733,
      "step": 7260
    },
    {
      "epoch": 36.169154228855724,
      "grad_norm": 13.245490074157715,
      "learning_rate": 5.479477611940299e-08,
      "loss": 2.5197,
      "step": 7270
    },
    {
      "epoch": 36.21890547263681,
      "grad_norm": 10.990280151367188,
      "learning_rate": 5.473258706467662e-08,
      "loss": 2.6561,
      "step": 7280
    },
    {
      "epoch": 36.26865671641791,
      "grad_norm": 13.731064796447754,
      "learning_rate": 5.467039800995025e-08,
      "loss": 2.663,
      "step": 7290
    },
    {
      "epoch": 36.318407960199,
      "grad_norm": 9.296881675720215,
      "learning_rate": 5.460820895522388e-08,
      "loss": 2.555,
      "step": 7300
    },
    {
      "epoch": 36.3681592039801,
      "grad_norm": 7.5546159744262695,
      "learning_rate": 5.454601990049751e-08,
      "loss": 2.5404,
      "step": 7310
    },
    {
      "epoch": 36.417910447761194,
      "grad_norm": 5.693953514099121,
      "learning_rate": 5.448383084577114e-08,
      "loss": 2.5764,
      "step": 7320
    },
    {
      "epoch": 36.46766169154229,
      "grad_norm": 11.200671195983887,
      "learning_rate": 5.442164179104477e-08,
      "loss": 2.5816,
      "step": 7330
    },
    {
      "epoch": 36.517412935323385,
      "grad_norm": 11.895017623901367,
      "learning_rate": 5.43594527363184e-08,
      "loss": 2.5734,
      "step": 7340
    },
    {
      "epoch": 36.56716417910448,
      "grad_norm": 10.028855323791504,
      "learning_rate": 5.429726368159204e-08,
      "loss": 2.6415,
      "step": 7350
    },
    {
      "epoch": 36.61691542288557,
      "grad_norm": 36.357093811035156,
      "learning_rate": 5.423507462686567e-08,
      "loss": 2.7169,
      "step": 7360
    },
    {
      "epoch": 36.666666666666664,
      "grad_norm": 19.345359802246094,
      "learning_rate": 5.41728855721393e-08,
      "loss": 2.6078,
      "step": 7370
    },
    {
      "epoch": 36.71641791044776,
      "grad_norm": 13.65361213684082,
      "learning_rate": 5.411069651741294e-08,
      "loss": 2.6671,
      "step": 7380
    },
    {
      "epoch": 36.766169154228855,
      "grad_norm": 17.046337127685547,
      "learning_rate": 5.404850746268657e-08,
      "loss": 2.5982,
      "step": 7390
    },
    {
      "epoch": 36.81592039800995,
      "grad_norm": 7.155580520629883,
      "learning_rate": 5.39863184079602e-08,
      "loss": 2.6179,
      "step": 7400
    },
    {
      "epoch": 36.865671641791046,
      "grad_norm": 8.97148323059082,
      "learning_rate": 5.392412935323383e-08,
      "loss": 2.7015,
      "step": 7410
    },
    {
      "epoch": 36.91542288557214,
      "grad_norm": 8.860511779785156,
      "learning_rate": 5.386194029850746e-08,
      "loss": 2.6462,
      "step": 7420
    },
    {
      "epoch": 36.96517412935324,
      "grad_norm": 12.13713550567627,
      "learning_rate": 5.379975124378109e-08,
      "loss": 2.59,
      "step": 7430
    },
    {
      "epoch": 37.0,
      "eval_loss": 1.7715957164764404,
      "eval_runtime": 54.0715,
      "eval_samples_per_second": 3.717,
      "eval_steps_per_second": 0.24,
      "eval_wer": 0.6245179948586118,
      "step": 7437
    },
    {
      "epoch": 37.014925373134325,
      "grad_norm": 14.33958911895752,
      "learning_rate": 5.373756218905472e-08,
      "loss": 2.6599,
      "step": 7440
    },
    {
      "epoch": 37.06467661691542,
      "grad_norm": 11.556501388549805,
      "learning_rate": 5.367537313432836e-08,
      "loss": 2.6338,
      "step": 7450
    },
    {
      "epoch": 37.114427860696516,
      "grad_norm": 5.30551815032959,
      "learning_rate": 5.361318407960199e-08,
      "loss": 2.6261,
      "step": 7460
    },
    {
      "epoch": 37.16417910447761,
      "grad_norm": 25.035110473632812,
      "learning_rate": 5.355099502487562e-08,
      "loss": 2.5604,
      "step": 7470
    },
    {
      "epoch": 37.21393034825871,
      "grad_norm": 12.777660369873047,
      "learning_rate": 5.3488805970149256e-08,
      "loss": 2.61,
      "step": 7480
    },
    {
      "epoch": 37.2636815920398,
      "grad_norm": 7.921084403991699,
      "learning_rate": 5.3426616915422886e-08,
      "loss": 2.6131,
      "step": 7490
    },
    {
      "epoch": 37.3134328358209,
      "grad_norm": 8.848115921020508,
      "learning_rate": 5.3364427860696516e-08,
      "loss": 2.6988,
      "step": 7500
    },
    {
      "epoch": 37.36318407960199,
      "grad_norm": 9.519200325012207,
      "learning_rate": 5.3302238805970146e-08,
      "loss": 2.6078,
      "step": 7510
    },
    {
      "epoch": 37.41293532338308,
      "grad_norm": 11.4191255569458,
      "learning_rate": 5.3240049751243776e-08,
      "loss": 2.6555,
      "step": 7520
    },
    {
      "epoch": 37.46268656716418,
      "grad_norm": 15.671346664428711,
      "learning_rate": 5.3177860696517406e-08,
      "loss": 2.5401,
      "step": 7530
    },
    {
      "epoch": 37.51243781094527,
      "grad_norm": 12.50620174407959,
      "learning_rate": 5.3115671641791036e-08,
      "loss": 2.6972,
      "step": 7540
    },
    {
      "epoch": 37.56218905472637,
      "grad_norm": 9.408123970031738,
      "learning_rate": 5.305348258706468e-08,
      "loss": 2.6213,
      "step": 7550
    },
    {
      "epoch": 37.61194029850746,
      "grad_norm": 13.371488571166992,
      "learning_rate": 5.299129353233831e-08,
      "loss": 2.639,
      "step": 7560
    },
    {
      "epoch": 37.66169154228856,
      "grad_norm": 7.01920747756958,
      "learning_rate": 5.292910447761194e-08,
      "loss": 2.4953,
      "step": 7570
    },
    {
      "epoch": 37.711442786069654,
      "grad_norm": 13.524699211120605,
      "learning_rate": 5.286691542288557e-08,
      "loss": 2.674,
      "step": 7580
    },
    {
      "epoch": 37.76119402985075,
      "grad_norm": 11.392867088317871,
      "learning_rate": 5.2804726368159205e-08,
      "loss": 2.5873,
      "step": 7590
    },
    {
      "epoch": 37.81094527363184,
      "grad_norm": 11.586694717407227,
      "learning_rate": 5.2742537313432835e-08,
      "loss": 2.5432,
      "step": 7600
    },
    {
      "epoch": 37.86069651741293,
      "grad_norm": 9.420440673828125,
      "learning_rate": 5.2680348258706465e-08,
      "loss": 2.5209,
      "step": 7610
    },
    {
      "epoch": 37.91044776119403,
      "grad_norm": 12.850664138793945,
      "learning_rate": 5.2618159203980095e-08,
      "loss": 2.5078,
      "step": 7620
    },
    {
      "epoch": 37.960199004975124,
      "grad_norm": 10.710691452026367,
      "learning_rate": 5.2555970149253725e-08,
      "loss": 2.6736,
      "step": 7630
    },
    {
      "epoch": 38.0,
      "eval_loss": 1.7713477611541748,
      "eval_runtime": 54.6617,
      "eval_samples_per_second": 3.677,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6237949871465296,
      "step": 7638
    },
    {
      "epoch": 38.00995024875622,
      "grad_norm": 15.244569778442383,
      "learning_rate": 5.2493781094527355e-08,
      "loss": 2.6949,
      "step": 7640
    },
    {
      "epoch": 38.059701492537314,
      "grad_norm": 6.785576820373535,
      "learning_rate": 5.2431592039801e-08,
      "loss": 2.6604,
      "step": 7650
    },
    {
      "epoch": 38.10945273631841,
      "grad_norm": 8.344170570373535,
      "learning_rate": 5.236940298507463e-08,
      "loss": 2.6262,
      "step": 7660
    },
    {
      "epoch": 38.159203980099505,
      "grad_norm": 7.4182820320129395,
      "learning_rate": 5.230721393034826e-08,
      "loss": 2.5156,
      "step": 7670
    },
    {
      "epoch": 38.208955223880594,
      "grad_norm": 9.887770652770996,
      "learning_rate": 5.224502487562189e-08,
      "loss": 2.6387,
      "step": 7680
    },
    {
      "epoch": 38.25870646766169,
      "grad_norm": 20.201168060302734,
      "learning_rate": 5.2182835820895524e-08,
      "loss": 2.6249,
      "step": 7690
    },
    {
      "epoch": 38.308457711442784,
      "grad_norm": 14.051773071289062,
      "learning_rate": 5.2120646766169154e-08,
      "loss": 2.5171,
      "step": 7700
    },
    {
      "epoch": 38.35820895522388,
      "grad_norm": 9.108283042907715,
      "learning_rate": 5.2058457711442784e-08,
      "loss": 2.5912,
      "step": 7710
    },
    {
      "epoch": 38.407960199004975,
      "grad_norm": 14.901753425598145,
      "learning_rate": 5.1996268656716414e-08,
      "loss": 2.4957,
      "step": 7720
    },
    {
      "epoch": 38.45771144278607,
      "grad_norm": 25.13413429260254,
      "learning_rate": 5.1934079601990044e-08,
      "loss": 2.5837,
      "step": 7730
    },
    {
      "epoch": 38.507462686567166,
      "grad_norm": 13.404280662536621,
      "learning_rate": 5.1871890547263674e-08,
      "loss": 2.6431,
      "step": 7740
    },
    {
      "epoch": 38.55721393034826,
      "grad_norm": 18.8852596282959,
      "learning_rate": 5.1809701492537317e-08,
      "loss": 2.6733,
      "step": 7750
    },
    {
      "epoch": 38.60696517412935,
      "grad_norm": 7.614748001098633,
      "learning_rate": 5.1747512437810947e-08,
      "loss": 2.6894,
      "step": 7760
    },
    {
      "epoch": 38.656716417910445,
      "grad_norm": 7.619503498077393,
      "learning_rate": 5.1685323383084576e-08,
      "loss": 2.6872,
      "step": 7770
    },
    {
      "epoch": 38.70646766169154,
      "grad_norm": 15.364459037780762,
      "learning_rate": 5.1623134328358206e-08,
      "loss": 2.6142,
      "step": 7780
    },
    {
      "epoch": 38.756218905472636,
      "grad_norm": 9.843500137329102,
      "learning_rate": 5.1560945273631836e-08,
      "loss": 2.6048,
      "step": 7790
    },
    {
      "epoch": 38.80597014925373,
      "grad_norm": 7.6059088706970215,
      "learning_rate": 5.149875621890547e-08,
      "loss": 2.6447,
      "step": 7800
    },
    {
      "epoch": 38.85572139303483,
      "grad_norm": 7.434909343719482,
      "learning_rate": 5.14365671641791e-08,
      "loss": 2.5216,
      "step": 7810
    },
    {
      "epoch": 38.90547263681592,
      "grad_norm": 6.521846294403076,
      "learning_rate": 5.137437810945273e-08,
      "loss": 2.5741,
      "step": 7820
    },
    {
      "epoch": 38.95522388059702,
      "grad_norm": 11.867584228515625,
      "learning_rate": 5.131218905472636e-08,
      "loss": 2.5091,
      "step": 7830
    },
    {
      "epoch": 39.0,
      "eval_loss": 1.7694756984710693,
      "eval_runtime": 54.128,
      "eval_samples_per_second": 3.713,
      "eval_steps_per_second": 0.24,
      "eval_wer": 0.6237949871465296,
      "step": 7839
    },
    {
      "epoch": 39.004975124378106,
      "grad_norm": 8.254223823547363,
      "learning_rate": 5.124999999999999e-08,
      "loss": 2.6117,
      "step": 7840
    },
    {
      "epoch": 39.0547263681592,
      "grad_norm": 7.57152795791626,
      "learning_rate": 5.118781094527362e-08,
      "loss": 2.6483,
      "step": 7850
    },
    {
      "epoch": 39.1044776119403,
      "grad_norm": 6.916262626647949,
      "learning_rate": 5.1125621890547265e-08,
      "loss": 2.6146,
      "step": 7860
    },
    {
      "epoch": 39.15422885572139,
      "grad_norm": 7.728579998016357,
      "learning_rate": 5.1063432835820895e-08,
      "loss": 2.4881,
      "step": 7870
    },
    {
      "epoch": 39.20398009950249,
      "grad_norm": 11.557086944580078,
      "learning_rate": 5.1001243781094525e-08,
      "loss": 2.5356,
      "step": 7880
    },
    {
      "epoch": 39.25373134328358,
      "grad_norm": 10.882731437683105,
      "learning_rate": 5.0939054726368155e-08,
      "loss": 2.6269,
      "step": 7890
    },
    {
      "epoch": 39.30348258706468,
      "grad_norm": 7.17711877822876,
      "learning_rate": 5.087686567164179e-08,
      "loss": 2.5784,
      "step": 7900
    },
    {
      "epoch": 39.353233830845774,
      "grad_norm": 11.167913436889648,
      "learning_rate": 5.081467661691542e-08,
      "loss": 2.656,
      "step": 7910
    },
    {
      "epoch": 39.40298507462686,
      "grad_norm": 17.16898536682129,
      "learning_rate": 5.075248756218905e-08,
      "loss": 2.773,
      "step": 7920
    },
    {
      "epoch": 39.45273631840796,
      "grad_norm": 10.690255165100098,
      "learning_rate": 5.069029850746268e-08,
      "loss": 2.6261,
      "step": 7930
    },
    {
      "epoch": 39.50248756218905,
      "grad_norm": 12.012330055236816,
      "learning_rate": 5.062810945273631e-08,
      "loss": 2.5434,
      "step": 7940
    },
    {
      "epoch": 39.55223880597015,
      "grad_norm": 11.054511070251465,
      "learning_rate": 5.056592039800994e-08,
      "loss": 2.6118,
      "step": 7950
    },
    {
      "epoch": 39.601990049751244,
      "grad_norm": 13.902332305908203,
      "learning_rate": 5.0503731343283584e-08,
      "loss": 2.5177,
      "step": 7960
    },
    {
      "epoch": 39.65174129353234,
      "grad_norm": 10.393437385559082,
      "learning_rate": 5.0441542288557214e-08,
      "loss": 2.4995,
      "step": 7970
    },
    {
      "epoch": 39.701492537313435,
      "grad_norm": 10.515278816223145,
      "learning_rate": 5.0379353233830844e-08,
      "loss": 2.5018,
      "step": 7980
    },
    {
      "epoch": 39.75124378109453,
      "grad_norm": 9.571803092956543,
      "learning_rate": 5.0317164179104474e-08,
      "loss": 2.5845,
      "step": 7990
    },
    {
      "epoch": 39.80099502487562,
      "grad_norm": 15.755969047546387,
      "learning_rate": 5.025497512437811e-08,
      "loss": 2.6322,
      "step": 8000
    },
    {
      "epoch": 39.850746268656714,
      "grad_norm": 12.21865463256836,
      "learning_rate": 5.019278606965174e-08,
      "loss": 2.6342,
      "step": 8010
    },
    {
      "epoch": 39.90049751243781,
      "grad_norm": 10.643570899963379,
      "learning_rate": 5.013059701492537e-08,
      "loss": 2.5623,
      "step": 8020
    },
    {
      "epoch": 39.950248756218905,
      "grad_norm": 9.9536714553833,
      "learning_rate": 5.0068407960199e-08,
      "loss": 2.5331,
      "step": 8030
    },
    {
      "epoch": 40.0,
      "grad_norm": 23.034975051879883,
      "learning_rate": 5.000621890547263e-08,
      "loss": 2.6188,
      "step": 8040
    },
    {
      "epoch": 40.0,
      "eval_loss": 1.768253207206726,
      "eval_runtime": 54.3531,
      "eval_samples_per_second": 3.698,
      "eval_steps_per_second": 0.239,
      "eval_wer": 0.6237146529562982,
      "step": 8040
    },
    {
      "epoch": 40.049751243781095,
      "grad_norm": 19.82261085510254,
      "learning_rate": 4.9944029850746267e-08,
      "loss": 2.577,
      "step": 8050
    },
    {
      "epoch": 40.09950248756219,
      "grad_norm": 12.977036476135254,
      "learning_rate": 4.9881840796019897e-08,
      "loss": 2.5951,
      "step": 8060
    },
    {
      "epoch": 40.149253731343286,
      "grad_norm": 6.8516740798950195,
      "learning_rate": 4.9819651741293526e-08,
      "loss": 2.7179,
      "step": 8070
    },
    {
      "epoch": 40.19900497512438,
      "grad_norm": 10.055837631225586,
      "learning_rate": 4.975746268656716e-08,
      "loss": 2.562,
      "step": 8080
    },
    {
      "epoch": 40.24875621890547,
      "grad_norm": 10.728951454162598,
      "learning_rate": 4.969527363184079e-08,
      "loss": 2.5995,
      "step": 8090
    },
    {
      "epoch": 40.298507462686565,
      "grad_norm": 8.400613784790039,
      "learning_rate": 4.963308457711442e-08,
      "loss": 2.6827,
      "step": 8100
    },
    {
      "epoch": 40.34825870646766,
      "grad_norm": 21.744428634643555,
      "learning_rate": 4.957089552238806e-08,
      "loss": 2.5196,
      "step": 8110
    },
    {
      "epoch": 40.398009950248756,
      "grad_norm": 6.826544284820557,
      "learning_rate": 4.950870646766169e-08,
      "loss": 2.5473,
      "step": 8120
    },
    {
      "epoch": 40.44776119402985,
      "grad_norm": 11.173532485961914,
      "learning_rate": 4.9446517412935326e-08,
      "loss": 2.6123,
      "step": 8130
    },
    {
      "epoch": 40.49751243781095,
      "grad_norm": 16.997821807861328,
      "learning_rate": 4.9384328358208956e-08,
      "loss": 2.5188,
      "step": 8140
    },
    {
      "epoch": 40.54726368159204,
      "grad_norm": 10.882028579711914,
      "learning_rate": 4.9322139303482585e-08,
      "loss": 2.5946,
      "step": 8150
    },
    {
      "epoch": 40.59701492537313,
      "grad_norm": 12.027552604675293,
      "learning_rate": 4.9259950248756215e-08,
      "loss": 2.5162,
      "step": 8160
    },
    {
      "epoch": 40.646766169154226,
      "grad_norm": 7.966087818145752,
      "learning_rate": 4.9197761194029845e-08,
      "loss": 2.6202,
      "step": 8170
    },
    {
      "epoch": 40.69651741293532,
      "grad_norm": 14.17059326171875,
      "learning_rate": 4.913557213930348e-08,
      "loss": 2.632,
      "step": 8180
    },
    {
      "epoch": 40.74626865671642,
      "grad_norm": 16.643692016601562,
      "learning_rate": 4.907338308457711e-08,
      "loss": 2.6227,
      "step": 8190
    },
    {
      "epoch": 40.79601990049751,
      "grad_norm": 9.512494087219238,
      "learning_rate": 4.901119402985074e-08,
      "loss": 2.6017,
      "step": 8200
    },
    {
      "epoch": 40.84577114427861,
      "grad_norm": 10.458373069763184,
      "learning_rate": 4.894900497512438e-08,
      "loss": 2.5404,
      "step": 8210
    },
    {
      "epoch": 40.8955223880597,
      "grad_norm": 11.770461082458496,
      "learning_rate": 4.888681592039801e-08,
      "loss": 2.5127,
      "step": 8220
    },
    {
      "epoch": 40.9452736318408,
      "grad_norm": 10.336589813232422,
      "learning_rate": 4.8824626865671645e-08,
      "loss": 2.7179,
      "step": 8230
    },
    {
      "epoch": 40.995024875621894,
      "grad_norm": 18.809829711914062,
      "learning_rate": 4.8762437810945274e-08,
      "loss": 2.657,
      "step": 8240
    },
    {
      "epoch": 41.0,
      "eval_loss": 1.7625819444656372,
      "eval_runtime": 54.9536,
      "eval_samples_per_second": 3.658,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6231523136246787,
      "step": 8241
    },
    {
      "epoch": 41.04477611940298,
      "grad_norm": 7.562481880187988,
      "learning_rate": 4.8700248756218904e-08,
      "loss": 2.6378,
      "step": 8250
    },
    {
      "epoch": 41.09452736318408,
      "grad_norm": 14.728099822998047,
      "learning_rate": 4.8638059701492534e-08,
      "loss": 2.5844,
      "step": 8260
    },
    {
      "epoch": 41.14427860696517,
      "grad_norm": 9.671099662780762,
      "learning_rate": 4.8575870646766164e-08,
      "loss": 2.6546,
      "step": 8270
    },
    {
      "epoch": 41.19402985074627,
      "grad_norm": 14.143989562988281,
      "learning_rate": 4.85136815920398e-08,
      "loss": 2.7012,
      "step": 8280
    },
    {
      "epoch": 41.243781094527364,
      "grad_norm": 8.492281913757324,
      "learning_rate": 4.845149253731343e-08,
      "loss": 2.6677,
      "step": 8290
    },
    {
      "epoch": 41.29353233830846,
      "grad_norm": 8.88365650177002,
      "learning_rate": 4.838930348258706e-08,
      "loss": 2.6211,
      "step": 8300
    },
    {
      "epoch": 41.343283582089555,
      "grad_norm": 8.076998710632324,
      "learning_rate": 4.832711442786069e-08,
      "loss": 2.5308,
      "step": 8310
    },
    {
      "epoch": 41.39303482587065,
      "grad_norm": 10.965638160705566,
      "learning_rate": 4.826492537313433e-08,
      "loss": 2.6458,
      "step": 8320
    },
    {
      "epoch": 41.44278606965174,
      "grad_norm": 10.646464347839355,
      "learning_rate": 4.8202736318407963e-08,
      "loss": 2.6431,
      "step": 8330
    },
    {
      "epoch": 41.492537313432834,
      "grad_norm": 16.522573471069336,
      "learning_rate": 4.8140547263681593e-08,
      "loss": 2.6348,
      "step": 8340
    },
    {
      "epoch": 41.54228855721393,
      "grad_norm": 7.3186845779418945,
      "learning_rate": 4.807835820895522e-08,
      "loss": 2.7167,
      "step": 8350
    },
    {
      "epoch": 41.592039800995025,
      "grad_norm": 80.50212097167969,
      "learning_rate": 4.801616915422885e-08,
      "loss": 2.5117,
      "step": 8360
    },
    {
      "epoch": 41.64179104477612,
      "grad_norm": 11.847969055175781,
      "learning_rate": 4.795398009950248e-08,
      "loss": 2.5049,
      "step": 8370
    },
    {
      "epoch": 41.691542288557216,
      "grad_norm": 12.739986419677734,
      "learning_rate": 4.789179104477612e-08,
      "loss": 2.4574,
      "step": 8380
    },
    {
      "epoch": 41.74129353233831,
      "grad_norm": 11.049832344055176,
      "learning_rate": 4.782960199004975e-08,
      "loss": 2.6103,
      "step": 8390
    },
    {
      "epoch": 41.791044776119406,
      "grad_norm": 17.631473541259766,
      "learning_rate": 4.776741293532338e-08,
      "loss": 2.5074,
      "step": 8400
    },
    {
      "epoch": 41.840796019900495,
      "grad_norm": 20.159303665161133,
      "learning_rate": 4.770522388059701e-08,
      "loss": 2.553,
      "step": 8410
    },
    {
      "epoch": 41.89054726368159,
      "grad_norm": 13.263484001159668,
      "learning_rate": 4.7643034825870646e-08,
      "loss": 2.568,
      "step": 8420
    },
    {
      "epoch": 41.940298507462686,
      "grad_norm": 6.073016166687012,
      "learning_rate": 4.758084577114428e-08,
      "loss": 2.5583,
      "step": 8430
    },
    {
      "epoch": 41.99004975124378,
      "grad_norm": 35.04255294799805,
      "learning_rate": 4.751865671641791e-08,
      "loss": 2.593,
      "step": 8440
    },
    {
      "epoch": 42.0,
      "eval_loss": 1.7651928663253784,
      "eval_runtime": 55.2499,
      "eval_samples_per_second": 3.638,
      "eval_steps_per_second": 0.235,
      "eval_wer": 0.6222686375321337,
      "step": 8442
    },
    {
      "epoch": 42.039800995024876,
      "grad_norm": 8.35002613067627,
      "learning_rate": 4.745646766169154e-08,
      "loss": 2.5878,
      "step": 8450
    },
    {
      "epoch": 42.08955223880597,
      "grad_norm": 11.919656753540039,
      "learning_rate": 4.739427860696517e-08,
      "loss": 2.5615,
      "step": 8460
    },
    {
      "epoch": 42.13930348258707,
      "grad_norm": 6.592028617858887,
      "learning_rate": 4.73320895522388e-08,
      "loss": 2.5614,
      "step": 8470
    },
    {
      "epoch": 42.18905472636816,
      "grad_norm": 8.391692161560059,
      "learning_rate": 4.726990049751244e-08,
      "loss": 2.6088,
      "step": 8480
    },
    {
      "epoch": 42.23880597014925,
      "grad_norm": 10.03930950164795,
      "learning_rate": 4.720771144278607e-08,
      "loss": 2.603,
      "step": 8490
    },
    {
      "epoch": 42.288557213930346,
      "grad_norm": 8.587992668151855,
      "learning_rate": 4.71455223880597e-08,
      "loss": 2.7091,
      "step": 8500
    },
    {
      "epoch": 42.33830845771144,
      "grad_norm": 24.448368072509766,
      "learning_rate": 4.708333333333333e-08,
      "loss": 2.5079,
      "step": 8510
    },
    {
      "epoch": 42.38805970149254,
      "grad_norm": 6.534202575683594,
      "learning_rate": 4.702114427860696e-08,
      "loss": 2.6518,
      "step": 8520
    },
    {
      "epoch": 42.43781094527363,
      "grad_norm": 16.49262046813965,
      "learning_rate": 4.6958955223880595e-08,
      "loss": 2.5599,
      "step": 8530
    },
    {
      "epoch": 42.48756218905473,
      "grad_norm": 37.34288787841797,
      "learning_rate": 4.689676616915423e-08,
      "loss": 2.6466,
      "step": 8540
    },
    {
      "epoch": 42.53731343283582,
      "grad_norm": 10.444652557373047,
      "learning_rate": 4.683457711442786e-08,
      "loss": 2.652,
      "step": 8550
    },
    {
      "epoch": 42.58706467661692,
      "grad_norm": 36.933319091796875,
      "learning_rate": 4.677238805970149e-08,
      "loss": 2.6144,
      "step": 8560
    },
    {
      "epoch": 42.63681592039801,
      "grad_norm": 6.025495529174805,
      "learning_rate": 4.671019900497512e-08,
      "loss": 2.6558,
      "step": 8570
    },
    {
      "epoch": 42.6865671641791,
      "grad_norm": 6.34460973739624,
      "learning_rate": 4.664800995024876e-08,
      "loss": 2.6701,
      "step": 8580
    },
    {
      "epoch": 42.7363184079602,
      "grad_norm": 10.667281150817871,
      "learning_rate": 4.658582089552239e-08,
      "loss": 2.4924,
      "step": 8590
    },
    {
      "epoch": 42.78606965174129,
      "grad_norm": 7.483098030090332,
      "learning_rate": 4.652363184079602e-08,
      "loss": 2.5139,
      "step": 8600
    },
    {
      "epoch": 42.83582089552239,
      "grad_norm": 10.144134521484375,
      "learning_rate": 4.646144278606965e-08,
      "loss": 2.6817,
      "step": 8610
    },
    {
      "epoch": 42.885572139303484,
      "grad_norm": 15.742927551269531,
      "learning_rate": 4.639925373134328e-08,
      "loss": 2.5731,
      "step": 8620
    },
    {
      "epoch": 42.93532338308458,
      "grad_norm": 8.959490776062012,
      "learning_rate": 4.6337064676616913e-08,
      "loss": 2.5199,
      "step": 8630
    },
    {
      "epoch": 42.985074626865675,
      "grad_norm": 15.34212875366211,
      "learning_rate": 4.627487562189055e-08,
      "loss": 2.5926,
      "step": 8640
    },
    {
      "epoch": 43.0,
      "eval_loss": 1.7620965242385864,
      "eval_runtime": 53.9856,
      "eval_samples_per_second": 3.723,
      "eval_steps_per_second": 0.241,
      "eval_wer": 0.6224293059125964,
      "step": 8643
    },
    {
      "epoch": 43.03482587064676,
      "grad_norm": 12.714271545410156,
      "learning_rate": 4.621268656716418e-08,
      "loss": 2.5782,
      "step": 8650
    },
    {
      "epoch": 43.08457711442786,
      "grad_norm": 7.567221164703369,
      "learning_rate": 4.615049751243781e-08,
      "loss": 2.5581,
      "step": 8660
    },
    {
      "epoch": 43.134328358208954,
      "grad_norm": 6.921499729156494,
      "learning_rate": 4.608830845771144e-08,
      "loss": 2.6469,
      "step": 8670
    },
    {
      "epoch": 43.18407960199005,
      "grad_norm": 14.151759147644043,
      "learning_rate": 4.6026119402985076e-08,
      "loss": 2.5128,
      "step": 8680
    },
    {
      "epoch": 43.233830845771145,
      "grad_norm": 13.364418029785156,
      "learning_rate": 4.5963930348258706e-08,
      "loss": 2.4989,
      "step": 8690
    },
    {
      "epoch": 43.28358208955224,
      "grad_norm": 9.610870361328125,
      "learning_rate": 4.5901741293532336e-08,
      "loss": 2.5581,
      "step": 8700
    },
    {
      "epoch": 43.333333333333336,
      "grad_norm": 6.773524761199951,
      "learning_rate": 4.5839552238805966e-08,
      "loss": 2.5799,
      "step": 8710
    },
    {
      "epoch": 43.38308457711443,
      "grad_norm": 13.043240547180176,
      "learning_rate": 4.5777363184079596e-08,
      "loss": 2.6196,
      "step": 8720
    },
    {
      "epoch": 43.43283582089552,
      "grad_norm": 15.018447875976562,
      "learning_rate": 4.571517412935323e-08,
      "loss": 2.6866,
      "step": 8730
    },
    {
      "epoch": 43.482587064676615,
      "grad_norm": 12.412919044494629,
      "learning_rate": 4.565298507462686e-08,
      "loss": 2.5983,
      "step": 8740
    },
    {
      "epoch": 43.53233830845771,
      "grad_norm": 8.744331359863281,
      "learning_rate": 4.55907960199005e-08,
      "loss": 2.4799,
      "step": 8750
    },
    {
      "epoch": 43.582089552238806,
      "grad_norm": 7.037256240844727,
      "learning_rate": 4.552860696517413e-08,
      "loss": 2.6137,
      "step": 8760
    },
    {
      "epoch": 43.6318407960199,
      "grad_norm": 9.211565017700195,
      "learning_rate": 4.546641791044776e-08,
      "loss": 2.6631,
      "step": 8770
    },
    {
      "epoch": 43.681592039801,
      "grad_norm": 8.546202659606934,
      "learning_rate": 4.5404228855721395e-08,
      "loss": 2.643,
      "step": 8780
    },
    {
      "epoch": 43.73134328358209,
      "grad_norm": 17.960819244384766,
      "learning_rate": 4.5342039800995025e-08,
      "loss": 2.5205,
      "step": 8790
    },
    {
      "epoch": 43.78109452736319,
      "grad_norm": 8.615589141845703,
      "learning_rate": 4.5279850746268655e-08,
      "loss": 2.6609,
      "step": 8800
    },
    {
      "epoch": 43.830845771144276,
      "grad_norm": 10.838630676269531,
      "learning_rate": 4.5217661691542285e-08,
      "loss": 2.5811,
      "step": 8810
    },
    {
      "epoch": 43.88059701492537,
      "grad_norm": 34.71630859375,
      "learning_rate": 4.5155472636815915e-08,
      "loss": 2.5959,
      "step": 8820
    },
    {
      "epoch": 43.930348258706466,
      "grad_norm": 10.404635429382324,
      "learning_rate": 4.509328358208955e-08,
      "loss": 2.5888,
      "step": 8830
    },
    {
      "epoch": 43.98009950248756,
      "grad_norm": 16.79088592529297,
      "learning_rate": 4.503109452736318e-08,
      "loss": 2.8147,
      "step": 8840
    },
    {
      "epoch": 44.0,
      "eval_loss": 1.7635047435760498,
      "eval_runtime": 53.7984,
      "eval_samples_per_second": 3.736,
      "eval_steps_per_second": 0.242,
      "eval_wer": 0.6227506426735219,
      "step": 8844
    },
    {
      "epoch": 44.02985074626866,
      "grad_norm": 20.08549690246582,
      "learning_rate": 4.496890547263682e-08,
      "loss": 2.6996,
      "step": 8850
    },
    {
      "epoch": 44.07960199004975,
      "grad_norm": 9.893638610839844,
      "learning_rate": 4.490671641791045e-08,
      "loss": 2.6544,
      "step": 8860
    },
    {
      "epoch": 44.12935323383085,
      "grad_norm": 10.11038589477539,
      "learning_rate": 4.484452736318408e-08,
      "loss": 2.4978,
      "step": 8870
    },
    {
      "epoch": 44.17910447761194,
      "grad_norm": 31.81199073791504,
      "learning_rate": 4.478233830845771e-08,
      "loss": 2.5484,
      "step": 8880
    },
    {
      "epoch": 44.22885572139303,
      "grad_norm": 17.028667449951172,
      "learning_rate": 4.4720149253731344e-08,
      "loss": 2.6092,
      "step": 8890
    },
    {
      "epoch": 44.27860696517413,
      "grad_norm": 6.84855842590332,
      "learning_rate": 4.4657960199004974e-08,
      "loss": 2.5524,
      "step": 8900
    },
    {
      "epoch": 44.32835820895522,
      "grad_norm": 7.302009105682373,
      "learning_rate": 4.4595771144278604e-08,
      "loss": 2.5246,
      "step": 8910
    },
    {
      "epoch": 44.37810945273632,
      "grad_norm": 10.634467124938965,
      "learning_rate": 4.4533582089552233e-08,
      "loss": 2.5666,
      "step": 8920
    },
    {
      "epoch": 44.42786069651741,
      "grad_norm": 9.9918794631958,
      "learning_rate": 4.4471393034825863e-08,
      "loss": 2.5495,
      "step": 8930
    },
    {
      "epoch": 44.47761194029851,
      "grad_norm": 10.821551322937012,
      "learning_rate": 4.44092039800995e-08,
      "loss": 2.5331,
      "step": 8940
    },
    {
      "epoch": 44.527363184079604,
      "grad_norm": 6.784879207611084,
      "learning_rate": 4.434701492537313e-08,
      "loss": 2.6202,
      "step": 8950
    },
    {
      "epoch": 44.5771144278607,
      "grad_norm": 12.31251049041748,
      "learning_rate": 4.4284825870646766e-08,
      "loss": 2.671,
      "step": 8960
    },
    {
      "epoch": 44.62686567164179,
      "grad_norm": 8.916770935058594,
      "learning_rate": 4.4222636815920396e-08,
      "loss": 2.615,
      "step": 8970
    },
    {
      "epoch": 44.67661691542288,
      "grad_norm": 11.191627502441406,
      "learning_rate": 4.4160447761194026e-08,
      "loss": 2.5734,
      "step": 8980
    },
    {
      "epoch": 44.72636815920398,
      "grad_norm": 8.610555648803711,
      "learning_rate": 4.409825870646766e-08,
      "loss": 2.5774,
      "step": 8990
    },
    {
      "epoch": 44.776119402985074,
      "grad_norm": 9.895384788513184,
      "learning_rate": 4.403606965174129e-08,
      "loss": 2.6248,
      "step": 9000
    },
    {
      "epoch": 44.82587064676617,
      "grad_norm": 10.648443222045898,
      "learning_rate": 4.397388059701492e-08,
      "loss": 2.5109,
      "step": 9010
    },
    {
      "epoch": 44.875621890547265,
      "grad_norm": 9.163538932800293,
      "learning_rate": 4.391169154228855e-08,
      "loss": 2.5472,
      "step": 9020
    },
    {
      "epoch": 44.92537313432836,
      "grad_norm": 8.85579776763916,
      "learning_rate": 4.384950248756218e-08,
      "loss": 2.4744,
      "step": 9030
    },
    {
      "epoch": 44.975124378109456,
      "grad_norm": 16.694332122802734,
      "learning_rate": 4.378731343283582e-08,
      "loss": 2.6266,
      "step": 9040
    },
    {
      "epoch": 45.0,
      "eval_loss": 1.7602206468582153,
      "eval_runtime": 54.6402,
      "eval_samples_per_second": 3.679,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6213849614395887,
      "step": 9045
    },
    {
      "epoch": 45.024875621890544,
      "grad_norm": 17.135940551757812,
      "learning_rate": 4.372512437810945e-08,
      "loss": 2.6862,
      "step": 9050
    },
    {
      "epoch": 45.07462686567164,
      "grad_norm": 9.696233749389648,
      "learning_rate": 4.3662935323383085e-08,
      "loss": 2.5316,
      "step": 9060
    },
    {
      "epoch": 45.124378109452735,
      "grad_norm": 10.479519844055176,
      "learning_rate": 4.3600746268656715e-08,
      "loss": 2.6065,
      "step": 9070
    },
    {
      "epoch": 45.17412935323383,
      "grad_norm": 5.489598751068115,
      "learning_rate": 4.3538557213930345e-08,
      "loss": 2.5235,
      "step": 9080
    },
    {
      "epoch": 45.223880597014926,
      "grad_norm": 68.72357177734375,
      "learning_rate": 4.347636815920398e-08,
      "loss": 2.7754,
      "step": 9090
    },
    {
      "epoch": 45.27363184079602,
      "grad_norm": 7.730828285217285,
      "learning_rate": 4.341417910447761e-08,
      "loss": 2.5632,
      "step": 9100
    },
    {
      "epoch": 45.32338308457712,
      "grad_norm": 8.543156623840332,
      "learning_rate": 4.335199004975124e-08,
      "loss": 2.5454,
      "step": 9110
    },
    {
      "epoch": 45.37313432835821,
      "grad_norm": 9.452075004577637,
      "learning_rate": 4.328980099502487e-08,
      "loss": 2.6023,
      "step": 9120
    },
    {
      "epoch": 45.4228855721393,
      "grad_norm": 11.694985389709473,
      "learning_rate": 4.32276119402985e-08,
      "loss": 2.672,
      "step": 9130
    },
    {
      "epoch": 45.472636815920396,
      "grad_norm": 14.694646835327148,
      "learning_rate": 4.316542288557214e-08,
      "loss": 2.5476,
      "step": 9140
    },
    {
      "epoch": 45.52238805970149,
      "grad_norm": 8.559500694274902,
      "learning_rate": 4.310323383084577e-08,
      "loss": 2.5655,
      "step": 9150
    },
    {
      "epoch": 45.57213930348259,
      "grad_norm": 12.727283477783203,
      "learning_rate": 4.30410447761194e-08,
      "loss": 2.5579,
      "step": 9160
    },
    {
      "epoch": 45.62189054726368,
      "grad_norm": 7.866433143615723,
      "learning_rate": 4.2978855721393034e-08,
      "loss": 2.4874,
      "step": 9170
    },
    {
      "epoch": 45.67164179104478,
      "grad_norm": 23.087020874023438,
      "learning_rate": 4.2916666666666664e-08,
      "loss": 2.5807,
      "step": 9180
    },
    {
      "epoch": 45.72139303482587,
      "grad_norm": 17.880104064941406,
      "learning_rate": 4.28544776119403e-08,
      "loss": 2.5915,
      "step": 9190
    },
    {
      "epoch": 45.77114427860697,
      "grad_norm": 8.362702369689941,
      "learning_rate": 4.279228855721393e-08,
      "loss": 2.6003,
      "step": 9200
    },
    {
      "epoch": 45.82089552238806,
      "grad_norm": 8.519777297973633,
      "learning_rate": 4.273009950248756e-08,
      "loss": 2.5325,
      "step": 9210
    },
    {
      "epoch": 45.87064676616915,
      "grad_norm": 10.093082427978516,
      "learning_rate": 4.266791044776119e-08,
      "loss": 2.6761,
      "step": 9220
    },
    {
      "epoch": 45.92039800995025,
      "grad_norm": 7.656639099121094,
      "learning_rate": 4.260572139303482e-08,
      "loss": 2.5533,
      "step": 9230
    },
    {
      "epoch": 45.97014925373134,
      "grad_norm": 6.732161521911621,
      "learning_rate": 4.2543532338308456e-08,
      "loss": 2.6045,
      "step": 9240
    },
    {
      "epoch": 46.0,
      "eval_loss": 1.757639765739441,
      "eval_runtime": 54.4138,
      "eval_samples_per_second": 3.694,
      "eval_steps_per_second": 0.239,
      "eval_wer": 0.6213046272493573,
      "step": 9246
    },
    {
      "epoch": 46.01990049751244,
      "grad_norm": 9.837469100952148,
      "learning_rate": 4.2481343283582086e-08,
      "loss": 2.5311,
      "step": 9250
    },
    {
      "epoch": 46.069651741293534,
      "grad_norm": 16.120826721191406,
      "learning_rate": 4.2419154228855716e-08,
      "loss": 2.5559,
      "step": 9260
    },
    {
      "epoch": 46.11940298507463,
      "grad_norm": 19.996850967407227,
      "learning_rate": 4.235696517412935e-08,
      "loss": 2.6378,
      "step": 9270
    },
    {
      "epoch": 46.169154228855724,
      "grad_norm": 8.535684585571289,
      "learning_rate": 4.229477611940298e-08,
      "loss": 2.6291,
      "step": 9280
    },
    {
      "epoch": 46.21890547263681,
      "grad_norm": 9.023568153381348,
      "learning_rate": 4.223258706467662e-08,
      "loss": 2.5315,
      "step": 9290
    },
    {
      "epoch": 46.26865671641791,
      "grad_norm": 6.935903072357178,
      "learning_rate": 4.217039800995025e-08,
      "loss": 2.5554,
      "step": 9300
    },
    {
      "epoch": 46.318407960199,
      "grad_norm": 8.433433532714844,
      "learning_rate": 4.210820895522388e-08,
      "loss": 2.6076,
      "step": 9310
    },
    {
      "epoch": 46.3681592039801,
      "grad_norm": 18.005712509155273,
      "learning_rate": 4.204601990049751e-08,
      "loss": 2.6531,
      "step": 9320
    },
    {
      "epoch": 46.417910447761194,
      "grad_norm": 10.383423805236816,
      "learning_rate": 4.198383084577114e-08,
      "loss": 2.6178,
      "step": 9330
    },
    {
      "epoch": 46.46766169154229,
      "grad_norm": 18.976852416992188,
      "learning_rate": 4.1921641791044775e-08,
      "loss": 2.6249,
      "step": 9340
    },
    {
      "epoch": 46.517412935323385,
      "grad_norm": 28.999378204345703,
      "learning_rate": 4.1859452736318405e-08,
      "loss": 2.5759,
      "step": 9350
    },
    {
      "epoch": 46.56716417910448,
      "grad_norm": 8.132801055908203,
      "learning_rate": 4.1797263681592035e-08,
      "loss": 2.6163,
      "step": 9360
    },
    {
      "epoch": 46.61691542288557,
      "grad_norm": 9.745826721191406,
      "learning_rate": 4.173507462686567e-08,
      "loss": 2.6456,
      "step": 9370
    },
    {
      "epoch": 46.666666666666664,
      "grad_norm": 11.693495750427246,
      "learning_rate": 4.16728855721393e-08,
      "loss": 2.5679,
      "step": 9380
    },
    {
      "epoch": 46.71641791044776,
      "grad_norm": 10.979331970214844,
      "learning_rate": 4.161069651741294e-08,
      "loss": 2.6345,
      "step": 9390
    },
    {
      "epoch": 46.766169154228855,
      "grad_norm": 8.078340530395508,
      "learning_rate": 4.154850746268657e-08,
      "loss": 2.5511,
      "step": 9400
    },
    {
      "epoch": 46.81592039800995,
      "grad_norm": 6.83116340637207,
      "learning_rate": 4.14863184079602e-08,
      "loss": 2.5633,
      "step": 9410
    },
    {
      "epoch": 46.865671641791046,
      "grad_norm": 11.548575401306152,
      "learning_rate": 4.142412935323383e-08,
      "loss": 2.5237,
      "step": 9420
    },
    {
      "epoch": 46.91542288557214,
      "grad_norm": 29.394384384155273,
      "learning_rate": 4.136194029850746e-08,
      "loss": 2.6017,
      "step": 9430
    },
    {
      "epoch": 46.96517412935324,
      "grad_norm": 6.616682052612305,
      "learning_rate": 4.1299751243781094e-08,
      "loss": 2.5224,
      "step": 9440
    },
    {
      "epoch": 47.0,
      "eval_loss": 1.7561088800430298,
      "eval_runtime": 55.5791,
      "eval_samples_per_second": 3.616,
      "eval_steps_per_second": 0.234,
      "eval_wer": 0.6209832904884319,
      "step": 9447
    },
    {
      "epoch": 47.014925373134325,
      "grad_norm": 17.9504337310791,
      "learning_rate": 4.1237562189054724e-08,
      "loss": 2.6207,
      "step": 9450
    },
    {
      "epoch": 47.06467661691542,
      "grad_norm": 11.301321029663086,
      "learning_rate": 4.1175373134328354e-08,
      "loss": 2.5339,
      "step": 9460
    },
    {
      "epoch": 47.114427860696516,
      "grad_norm": 5.819350719451904,
      "learning_rate": 4.1113184079601984e-08,
      "loss": 2.6091,
      "step": 9470
    },
    {
      "epoch": 47.16417910447761,
      "grad_norm": 11.10993480682373,
      "learning_rate": 4.105099502487562e-08,
      "loss": 2.5724,
      "step": 9480
    },
    {
      "epoch": 47.21393034825871,
      "grad_norm": 11.914627075195312,
      "learning_rate": 4.098880597014926e-08,
      "loss": 2.5465,
      "step": 9490
    },
    {
      "epoch": 47.2636815920398,
      "grad_norm": 10.850359916687012,
      "learning_rate": 4.092661691542289e-08,
      "loss": 2.652,
      "step": 9500
    },
    {
      "epoch": 47.3134328358209,
      "grad_norm": 17.31947898864746,
      "learning_rate": 4.086442786069652e-08,
      "loss": 2.6892,
      "step": 9510
    },
    {
      "epoch": 47.36318407960199,
      "grad_norm": 30.618358612060547,
      "learning_rate": 4.0802238805970147e-08,
      "loss": 2.5604,
      "step": 9520
    },
    {
      "epoch": 47.41293532338308,
      "grad_norm": 19.249645233154297,
      "learning_rate": 4.0740049751243777e-08,
      "loss": 2.7393,
      "step": 9530
    },
    {
      "epoch": 47.46268656716418,
      "grad_norm": 8.032620429992676,
      "learning_rate": 4.067786069651741e-08,
      "loss": 2.6186,
      "step": 9540
    },
    {
      "epoch": 47.51243781094527,
      "grad_norm": 10.501465797424316,
      "learning_rate": 4.061567164179104e-08,
      "loss": 2.662,
      "step": 9550
    },
    {
      "epoch": 47.56218905472637,
      "grad_norm": 24.12517738342285,
      "learning_rate": 4.055348258706467e-08,
      "loss": 2.6599,
      "step": 9560
    },
    {
      "epoch": 47.61194029850746,
      "grad_norm": 12.935555458068848,
      "learning_rate": 4.04912935323383e-08,
      "loss": 2.5371,
      "step": 9570
    },
    {
      "epoch": 47.66169154228856,
      "grad_norm": 7.786722660064697,
      "learning_rate": 4.042910447761194e-08,
      "loss": 2.5962,
      "step": 9580
    },
    {
      "epoch": 47.711442786069654,
      "grad_norm": 11.588502883911133,
      "learning_rate": 4.036691542288557e-08,
      "loss": 2.5838,
      "step": 9590
    },
    {
      "epoch": 47.76119402985075,
      "grad_norm": 5.372027397155762,
      "learning_rate": 4.0304726368159206e-08,
      "loss": 2.5198,
      "step": 9600
    },
    {
      "epoch": 47.81094527363184,
      "grad_norm": 6.775544166564941,
      "learning_rate": 4.0242537313432836e-08,
      "loss": 2.4836,
      "step": 9610
    },
    {
      "epoch": 47.86069651741293,
      "grad_norm": 9.447962760925293,
      "learning_rate": 4.0180348258706466e-08,
      "loss": 2.5618,
      "step": 9620
    },
    {
      "epoch": 47.91044776119403,
      "grad_norm": 22.678546905517578,
      "learning_rate": 4.0118159203980095e-08,
      "loss": 2.6039,
      "step": 9630
    },
    {
      "epoch": 47.960199004975124,
      "grad_norm": 19.38692855834961,
      "learning_rate": 4.005597014925373e-08,
      "loss": 2.6262,
      "step": 9640
    },
    {
      "epoch": 48.0,
      "eval_loss": 1.7539596557617188,
      "eval_runtime": 54.6808,
      "eval_samples_per_second": 3.676,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6207422879177378,
      "step": 9648
    },
    {
      "epoch": 48.00995024875622,
      "grad_norm": 9.87736701965332,
      "learning_rate": 3.999378109452736e-08,
      "loss": 2.5957,
      "step": 9650
    },
    {
      "epoch": 48.059701492537314,
      "grad_norm": 14.208332061767578,
      "learning_rate": 3.993159203980099e-08,
      "loss": 2.6389,
      "step": 9660
    },
    {
      "epoch": 48.10945273631841,
      "grad_norm": 9.176698684692383,
      "learning_rate": 3.986940298507462e-08,
      "loss": 2.553,
      "step": 9670
    },
    {
      "epoch": 48.159203980099505,
      "grad_norm": 12.86655330657959,
      "learning_rate": 3.980721393034825e-08,
      "loss": 2.4932,
      "step": 9680
    },
    {
      "epoch": 48.208955223880594,
      "grad_norm": 9.559632301330566,
      "learning_rate": 3.974502487562189e-08,
      "loss": 2.5743,
      "step": 9690
    },
    {
      "epoch": 48.25870646766169,
      "grad_norm": 6.712464809417725,
      "learning_rate": 3.9682835820895525e-08,
      "loss": 2.5072,
      "step": 9700
    },
    {
      "epoch": 48.308457711442784,
      "grad_norm": 24.994613647460938,
      "learning_rate": 3.9620646766169154e-08,
      "loss": 2.5609,
      "step": 9710
    },
    {
      "epoch": 48.35820895522388,
      "grad_norm": 10.248614311218262,
      "learning_rate": 3.9558457711442784e-08,
      "loss": 2.7204,
      "step": 9720
    },
    {
      "epoch": 48.407960199004975,
      "grad_norm": 8.563825607299805,
      "learning_rate": 3.9496268656716414e-08,
      "loss": 2.547,
      "step": 9730
    },
    {
      "epoch": 48.45771144278607,
      "grad_norm": 11.920967102050781,
      "learning_rate": 3.943407960199005e-08,
      "loss": 2.5594,
      "step": 9740
    },
    {
      "epoch": 48.507462686567166,
      "grad_norm": 13.477784156799316,
      "learning_rate": 3.937189054726368e-08,
      "loss": 2.5858,
      "step": 9750
    },
    {
      "epoch": 48.55721393034826,
      "grad_norm": 8.87233829498291,
      "learning_rate": 3.930970149253731e-08,
      "loss": 2.579,
      "step": 9760
    },
    {
      "epoch": 48.60696517412935,
      "grad_norm": 8.362540245056152,
      "learning_rate": 3.924751243781094e-08,
      "loss": 2.5828,
      "step": 9770
    },
    {
      "epoch": 48.656716417910445,
      "grad_norm": 10.338394165039062,
      "learning_rate": 3.918532338308457e-08,
      "loss": 2.5895,
      "step": 9780
    },
    {
      "epoch": 48.70646766169154,
      "grad_norm": 7.628657341003418,
      "learning_rate": 3.912313432835821e-08,
      "loss": 2.5243,
      "step": 9790
    },
    {
      "epoch": 48.756218905472636,
      "grad_norm": 8.030197143554688,
      "learning_rate": 3.906094527363184e-08,
      "loss": 2.6206,
      "step": 9800
    },
    {
      "epoch": 48.80597014925373,
      "grad_norm": 9.025022506713867,
      "learning_rate": 3.8998756218905473e-08,
      "loss": 2.5338,
      "step": 9810
    },
    {
      "epoch": 48.85572139303483,
      "grad_norm": 6.33540153503418,
      "learning_rate": 3.89365671641791e-08,
      "loss": 2.5934,
      "step": 9820
    },
    {
      "epoch": 48.90547263681592,
      "grad_norm": 16.85621452331543,
      "learning_rate": 3.887437810945273e-08,
      "loss": 2.642,
      "step": 9830
    },
    {
      "epoch": 48.95522388059702,
      "grad_norm": 7.499173164367676,
      "learning_rate": 3.881218905472637e-08,
      "loss": 2.7404,
      "step": 9840
    },
    {
      "epoch": 49.0,
      "eval_loss": 1.7523908615112305,
      "eval_runtime": 54.698,
      "eval_samples_per_second": 3.675,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6209029562982005,
      "step": 9849
    },
    {
      "epoch": 49.004975124378106,
      "grad_norm": 8.809325218200684,
      "learning_rate": 3.875e-08,
      "loss": 2.5897,
      "step": 9850
    },
    {
      "epoch": 49.0547263681592,
      "grad_norm": 6.525244235992432,
      "learning_rate": 3.868781094527363e-08,
      "loss": 2.5537,
      "step": 9860
    },
    {
      "epoch": 49.1044776119403,
      "grad_norm": 11.229598999023438,
      "learning_rate": 3.862562189054726e-08,
      "loss": 2.5842,
      "step": 9870
    },
    {
      "epoch": 49.15422885572139,
      "grad_norm": 30.777326583862305,
      "learning_rate": 3.856343283582089e-08,
      "loss": 2.644,
      "step": 9880
    },
    {
      "epoch": 49.20398009950249,
      "grad_norm": 10.377021789550781,
      "learning_rate": 3.8501243781094526e-08,
      "loss": 2.5278,
      "step": 9890
    },
    {
      "epoch": 49.25373134328358,
      "grad_norm": 6.437005043029785,
      "learning_rate": 3.8439054726368156e-08,
      "loss": 2.4703,
      "step": 9900
    },
    {
      "epoch": 49.30348258706468,
      "grad_norm": 175.0415496826172,
      "learning_rate": 3.837686567164179e-08,
      "loss": 2.6327,
      "step": 9910
    },
    {
      "epoch": 49.353233830845774,
      "grad_norm": 17.40065574645996,
      "learning_rate": 3.831467661691542e-08,
      "loss": 2.5971,
      "step": 9920
    },
    {
      "epoch": 49.40298507462686,
      "grad_norm": 8.711833953857422,
      "learning_rate": 3.825248756218905e-08,
      "loss": 2.6587,
      "step": 9930
    },
    {
      "epoch": 49.45273631840796,
      "grad_norm": 9.453048706054688,
      "learning_rate": 3.819029850746269e-08,
      "loss": 2.5295,
      "step": 9940
    },
    {
      "epoch": 49.50248756218905,
      "grad_norm": 8.180929183959961,
      "learning_rate": 3.812810945273632e-08,
      "loss": 2.6623,
      "step": 9950
    },
    {
      "epoch": 49.55223880597015,
      "grad_norm": 7.765921592712402,
      "learning_rate": 3.806592039800995e-08,
      "loss": 2.6157,
      "step": 9960
    },
    {
      "epoch": 49.601990049751244,
      "grad_norm": 13.382482528686523,
      "learning_rate": 3.800373134328358e-08,
      "loss": 2.5767,
      "step": 9970
    },
    {
      "epoch": 49.65174129353234,
      "grad_norm": 11.386536598205566,
      "learning_rate": 3.794154228855721e-08,
      "loss": 2.5502,
      "step": 9980
    },
    {
      "epoch": 49.701492537313435,
      "grad_norm": 10.6873779296875,
      "learning_rate": 3.7879353233830845e-08,
      "loss": 2.5124,
      "step": 9990
    },
    {
      "epoch": 49.75124378109453,
      "grad_norm": 7.4751482009887695,
      "learning_rate": 3.7817164179104475e-08,
      "loss": 2.6225,
      "step": 10000
    },
    {
      "epoch": 49.80099502487562,
      "grad_norm": 7.006795406341553,
      "learning_rate": 3.775497512437811e-08,
      "loss": 2.6601,
      "step": 10010
    },
    {
      "epoch": 49.850746268656714,
      "grad_norm": 6.398087024688721,
      "learning_rate": 3.769278606965174e-08,
      "loss": 2.6341,
      "step": 10020
    },
    {
      "epoch": 49.90049751243781,
      "grad_norm": 8.657125473022461,
      "learning_rate": 3.763059701492537e-08,
      "loss": 2.6277,
      "step": 10030
    },
    {
      "epoch": 49.950248756218905,
      "grad_norm": 6.816653251647949,
      "learning_rate": 3.756840796019901e-08,
      "loss": 2.5022,
      "step": 10040
    },
    {
      "epoch": 50.0,
      "grad_norm": 12.82742977142334,
      "learning_rate": 3.750621890547264e-08,
      "loss": 2.4907,
      "step": 10050
    },
    {
      "epoch": 50.0,
      "eval_loss": 1.7521440982818604,
      "eval_runtime": 54.9217,
      "eval_samples_per_second": 3.66,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6209832904884319,
      "step": 10050
    },
    {
      "epoch": 50.049751243781095,
      "grad_norm": 8.854766845703125,
      "learning_rate": 3.744402985074627e-08,
      "loss": 2.5891,
      "step": 10060
    },
    {
      "epoch": 50.09950248756219,
      "grad_norm": 26.5128173828125,
      "learning_rate": 3.73818407960199e-08,
      "loss": 2.6045,
      "step": 10070
    },
    {
      "epoch": 50.149253731343286,
      "grad_norm": 16.678112030029297,
      "learning_rate": 3.731965174129353e-08,
      "loss": 2.5458,
      "step": 10080
    },
    {
      "epoch": 50.19900497512438,
      "grad_norm": 11.466429710388184,
      "learning_rate": 3.7257462686567164e-08,
      "loss": 2.6553,
      "step": 10090
    },
    {
      "epoch": 50.24875621890547,
      "grad_norm": 17.928722381591797,
      "learning_rate": 3.7195273631840793e-08,
      "loss": 2.5224,
      "step": 10100
    },
    {
      "epoch": 50.298507462686565,
      "grad_norm": 13.047065734863281,
      "learning_rate": 3.7133084577114423e-08,
      "loss": 2.6662,
      "step": 10110
    },
    {
      "epoch": 50.34825870646766,
      "grad_norm": 17.997425079345703,
      "learning_rate": 3.707089552238806e-08,
      "loss": 2.4898,
      "step": 10120
    },
    {
      "epoch": 50.398009950248756,
      "grad_norm": 9.546403884887695,
      "learning_rate": 3.700870646766169e-08,
      "loss": 2.554,
      "step": 10130
    },
    {
      "epoch": 50.44776119402985,
      "grad_norm": 7.161911487579346,
      "learning_rate": 3.6946517412935326e-08,
      "loss": 2.6206,
      "step": 10140
    },
    {
      "epoch": 50.49751243781095,
      "grad_norm": 30.477718353271484,
      "learning_rate": 3.6884328358208956e-08,
      "loss": 2.555,
      "step": 10150
    },
    {
      "epoch": 50.54726368159204,
      "grad_norm": 11.627469062805176,
      "learning_rate": 3.6822139303482586e-08,
      "loss": 2.6385,
      "step": 10160
    },
    {
      "epoch": 50.59701492537313,
      "grad_norm": 10.780057907104492,
      "learning_rate": 3.6759950248756216e-08,
      "loss": 2.4884,
      "step": 10170
    },
    {
      "epoch": 50.646766169154226,
      "grad_norm": 9.228099822998047,
      "learning_rate": 3.6697761194029846e-08,
      "loss": 2.5878,
      "step": 10180
    },
    {
      "epoch": 50.69651741293532,
      "grad_norm": 7.2856645584106445,
      "learning_rate": 3.663557213930348e-08,
      "loss": 2.5733,
      "step": 10190
    },
    {
      "epoch": 50.74626865671642,
      "grad_norm": 7.487781047821045,
      "learning_rate": 3.657338308457711e-08,
      "loss": 2.4847,
      "step": 10200
    },
    {
      "epoch": 50.79601990049751,
      "grad_norm": 23.63385772705078,
      "learning_rate": 3.651119402985074e-08,
      "loss": 2.6572,
      "step": 10210
    },
    {
      "epoch": 50.84577114427861,
      "grad_norm": 17.8817195892334,
      "learning_rate": 3.644900497512438e-08,
      "loss": 2.668,
      "step": 10220
    },
    {
      "epoch": 50.8955223880597,
      "grad_norm": 11.695464134216309,
      "learning_rate": 3.638681592039801e-08,
      "loss": 2.5962,
      "step": 10230
    },
    {
      "epoch": 50.9452736318408,
      "grad_norm": 13.905662536621094,
      "learning_rate": 3.6324626865671645e-08,
      "loss": 2.5712,
      "step": 10240
    },
    {
      "epoch": 50.995024875621894,
      "grad_norm": 11.561439514160156,
      "learning_rate": 3.6262437810945275e-08,
      "loss": 2.6221,
      "step": 10250
    },
    {
      "epoch": 51.0,
      "eval_loss": 1.7540991306304932,
      "eval_runtime": 54.8538,
      "eval_samples_per_second": 3.664,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6202602827763496,
      "step": 10251
    },
    {
      "epoch": 51.04477611940298,
      "grad_norm": 12.293326377868652,
      "learning_rate": 3.6200248756218905e-08,
      "loss": 2.5195,
      "step": 10260
    },
    {
      "epoch": 51.09452736318408,
      "grad_norm": 25.600051879882812,
      "learning_rate": 3.6138059701492535e-08,
      "loss": 2.6381,
      "step": 10270
    },
    {
      "epoch": 51.14427860696517,
      "grad_norm": 11.158759117126465,
      "learning_rate": 3.6075870646766165e-08,
      "loss": 2.5688,
      "step": 10280
    },
    {
      "epoch": 51.19402985074627,
      "grad_norm": 9.173240661621094,
      "learning_rate": 3.60136815920398e-08,
      "loss": 2.5563,
      "step": 10290
    },
    {
      "epoch": 51.243781094527364,
      "grad_norm": 15.36401081085205,
      "learning_rate": 3.595149253731343e-08,
      "loss": 2.5748,
      "step": 10300
    },
    {
      "epoch": 51.29353233830846,
      "grad_norm": 34.34877014160156,
      "learning_rate": 3.588930348258706e-08,
      "loss": 2.5853,
      "step": 10310
    },
    {
      "epoch": 51.343283582089555,
      "grad_norm": 10.34097671508789,
      "learning_rate": 3.582711442786069e-08,
      "loss": 2.5374,
      "step": 10320
    },
    {
      "epoch": 51.39303482587065,
      "grad_norm": 12.699339866638184,
      "learning_rate": 3.576492537313433e-08,
      "loss": 2.7056,
      "step": 10330
    },
    {
      "epoch": 51.44278606965174,
      "grad_norm": 8.438672065734863,
      "learning_rate": 3.5702736318407964e-08,
      "loss": 2.7014,
      "step": 10340
    },
    {
      "epoch": 51.492537313432834,
      "grad_norm": 32.58889389038086,
      "learning_rate": 3.5640547263681594e-08,
      "loss": 2.5093,
      "step": 10350
    },
    {
      "epoch": 51.54228855721393,
      "grad_norm": 5.145575046539307,
      "learning_rate": 3.5578358208955224e-08,
      "loss": 2.4997,
      "step": 10360
    },
    {
      "epoch": 51.592039800995025,
      "grad_norm": 8.35946273803711,
      "learning_rate": 3.5516169154228854e-08,
      "loss": 2.6157,
      "step": 10370
    },
    {
      "epoch": 51.64179104477612,
      "grad_norm": 20.869335174560547,
      "learning_rate": 3.5453980099502484e-08,
      "loss": 2.5297,
      "step": 10380
    },
    {
      "epoch": 51.691542288557216,
      "grad_norm": 11.483027458190918,
      "learning_rate": 3.539179104477612e-08,
      "loss": 2.5227,
      "step": 10390
    },
    {
      "epoch": 51.74129353233831,
      "grad_norm": 12.733041763305664,
      "learning_rate": 3.532960199004975e-08,
      "loss": 2.5433,
      "step": 10400
    },
    {
      "epoch": 51.791044776119406,
      "grad_norm": 9.099729537963867,
      "learning_rate": 3.526741293532338e-08,
      "loss": 2.504,
      "step": 10410
    },
    {
      "epoch": 51.840796019900495,
      "grad_norm": 6.862722873687744,
      "learning_rate": 3.520522388059701e-08,
      "loss": 2.5694,
      "step": 10420
    },
    {
      "epoch": 51.89054726368159,
      "grad_norm": 6.856381416320801,
      "learning_rate": 3.5143034825870646e-08,
      "loss": 2.6012,
      "step": 10430
    },
    {
      "epoch": 51.940298507462686,
      "grad_norm": 9.966936111450195,
      "learning_rate": 3.508084577114428e-08,
      "loss": 2.5833,
      "step": 10440
    },
    {
      "epoch": 51.99004975124378,
      "grad_norm": 14.3433198928833,
      "learning_rate": 3.501865671641791e-08,
      "loss": 2.6004,
      "step": 10450
    },
    {
      "epoch": 52.0,
      "eval_loss": 1.7523586750030518,
      "eval_runtime": 54.7298,
      "eval_samples_per_second": 3.673,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6196176092544987,
      "step": 10452
    },
    {
      "epoch": 52.039800995024876,
      "grad_norm": 7.625024795532227,
      "learning_rate": 3.495646766169154e-08,
      "loss": 2.5237,
      "step": 10460
    },
    {
      "epoch": 52.08955223880597,
      "grad_norm": 19.951099395751953,
      "learning_rate": 3.489427860696517e-08,
      "loss": 2.6395,
      "step": 10470
    },
    {
      "epoch": 52.13930348258707,
      "grad_norm": 26.71824836730957,
      "learning_rate": 3.48320895522388e-08,
      "loss": 2.5439,
      "step": 10480
    },
    {
      "epoch": 52.18905472636816,
      "grad_norm": 13.56948471069336,
      "learning_rate": 3.476990049751244e-08,
      "loss": 2.6582,
      "step": 10490
    },
    {
      "epoch": 52.23880597014925,
      "grad_norm": 9.240633010864258,
      "learning_rate": 3.470771144278607e-08,
      "loss": 2.546,
      "step": 10500
    },
    {
      "epoch": 52.288557213930346,
      "grad_norm": 16.366222381591797,
      "learning_rate": 3.46455223880597e-08,
      "loss": 2.5791,
      "step": 10510
    },
    {
      "epoch": 52.33830845771144,
      "grad_norm": 36.805789947509766,
      "learning_rate": 3.458333333333333e-08,
      "loss": 2.4447,
      "step": 10520
    },
    {
      "epoch": 52.38805970149254,
      "grad_norm": 12.066829681396484,
      "learning_rate": 3.452114427860696e-08,
      "loss": 2.5705,
      "step": 10530
    },
    {
      "epoch": 52.43781094527363,
      "grad_norm": 14.700385093688965,
      "learning_rate": 3.4458955223880595e-08,
      "loss": 2.5833,
      "step": 10540
    },
    {
      "epoch": 52.48756218905473,
      "grad_norm": 8.25156307220459,
      "learning_rate": 3.439676616915423e-08,
      "loss": 2.642,
      "step": 10550
    },
    {
      "epoch": 52.53731343283582,
      "grad_norm": 9.769615173339844,
      "learning_rate": 3.433457711442786e-08,
      "loss": 2.6086,
      "step": 10560
    },
    {
      "epoch": 52.58706467661692,
      "grad_norm": 7.214608669281006,
      "learning_rate": 3.427238805970149e-08,
      "loss": 2.6009,
      "step": 10570
    },
    {
      "epoch": 52.63681592039801,
      "grad_norm": 20.022762298583984,
      "learning_rate": 3.421019900497512e-08,
      "loss": 2.5921,
      "step": 10580
    },
    {
      "epoch": 52.6865671641791,
      "grad_norm": 7.232862949371338,
      "learning_rate": 3.414800995024876e-08,
      "loss": 2.4921,
      "step": 10590
    },
    {
      "epoch": 52.7363184079602,
      "grad_norm": 6.897913455963135,
      "learning_rate": 3.408582089552239e-08,
      "loss": 2.5437,
      "step": 10600
    },
    {
      "epoch": 52.78606965174129,
      "grad_norm": 16.16379165649414,
      "learning_rate": 3.402363184079602e-08,
      "loss": 2.6519,
      "step": 10610
    },
    {
      "epoch": 52.83582089552239,
      "grad_norm": 9.148743629455566,
      "learning_rate": 3.396144278606965e-08,
      "loss": 2.6751,
      "step": 10620
    },
    {
      "epoch": 52.885572139303484,
      "grad_norm": 12.422295570373535,
      "learning_rate": 3.389925373134328e-08,
      "loss": 2.566,
      "step": 10630
    },
    {
      "epoch": 52.93532338308458,
      "grad_norm": 15.240989685058594,
      "learning_rate": 3.3837064676616914e-08,
      "loss": 2.4735,
      "step": 10640
    },
    {
      "epoch": 52.985074626865675,
      "grad_norm": 20.612228393554688,
      "learning_rate": 3.377487562189055e-08,
      "loss": 2.6308,
      "step": 10650
    },
    {
      "epoch": 53.0,
      "eval_loss": 1.7495089769363403,
      "eval_runtime": 55.8641,
      "eval_samples_per_second": 3.598,
      "eval_steps_per_second": 0.233,
      "eval_wer": 0.6193766066838047,
      "step": 10653
    },
    {
      "epoch": 53.03482587064676,
      "grad_norm": 6.417850971221924,
      "learning_rate": 3.371268656716418e-08,
      "loss": 2.5754,
      "step": 10660
    },
    {
      "epoch": 53.08457711442786,
      "grad_norm": 9.175419807434082,
      "learning_rate": 3.365049751243781e-08,
      "loss": 2.5819,
      "step": 10670
    },
    {
      "epoch": 53.134328358208954,
      "grad_norm": 11.451496124267578,
      "learning_rate": 3.358830845771144e-08,
      "loss": 2.6149,
      "step": 10680
    },
    {
      "epoch": 53.18407960199005,
      "grad_norm": 8.347162246704102,
      "learning_rate": 3.352611940298508e-08,
      "loss": 2.5029,
      "step": 10690
    },
    {
      "epoch": 53.233830845771145,
      "grad_norm": 8.13878345489502,
      "learning_rate": 3.3463930348258707e-08,
      "loss": 2.4689,
      "step": 10700
    },
    {
      "epoch": 53.28358208955224,
      "grad_norm": 17.524560928344727,
      "learning_rate": 3.3401741293532337e-08,
      "loss": 2.6004,
      "step": 10710
    },
    {
      "epoch": 53.333333333333336,
      "grad_norm": 7.564151763916016,
      "learning_rate": 3.3339552238805966e-08,
      "loss": 2.567,
      "step": 10720
    },
    {
      "epoch": 53.38308457711443,
      "grad_norm": 20.036561965942383,
      "learning_rate": 3.3277363184079596e-08,
      "loss": 2.6018,
      "step": 10730
    },
    {
      "epoch": 53.43283582089552,
      "grad_norm": 15.810239791870117,
      "learning_rate": 3.321517412935323e-08,
      "loss": 2.6901,
      "step": 10740
    },
    {
      "epoch": 53.482587064676615,
      "grad_norm": 14.316540718078613,
      "learning_rate": 3.315298507462686e-08,
      "loss": 2.5774,
      "step": 10750
    },
    {
      "epoch": 53.53233830845771,
      "grad_norm": 7.984283924102783,
      "learning_rate": 3.30907960199005e-08,
      "loss": 2.6138,
      "step": 10760
    },
    {
      "epoch": 53.582089552238806,
      "grad_norm": 14.144639015197754,
      "learning_rate": 3.302860696517413e-08,
      "loss": 2.5662,
      "step": 10770
    },
    {
      "epoch": 53.6318407960199,
      "grad_norm": 7.084502696990967,
      "learning_rate": 3.296641791044776e-08,
      "loss": 2.398,
      "step": 10780
    },
    {
      "epoch": 53.681592039801,
      "grad_norm": 10.917149543762207,
      "learning_rate": 3.2904228855721396e-08,
      "loss": 2.5554,
      "step": 10790
    },
    {
      "epoch": 53.73134328358209,
      "grad_norm": 7.584003448486328,
      "learning_rate": 3.2842039800995025e-08,
      "loss": 2.5435,
      "step": 10800
    },
    {
      "epoch": 53.78109452736319,
      "grad_norm": 8.901392936706543,
      "learning_rate": 3.2779850746268655e-08,
      "loss": 2.6701,
      "step": 10810
    },
    {
      "epoch": 53.830845771144276,
      "grad_norm": 11.119505882263184,
      "learning_rate": 3.2717661691542285e-08,
      "loss": 2.5197,
      "step": 10820
    },
    {
      "epoch": 53.88059701492537,
      "grad_norm": 14.367993354797363,
      "learning_rate": 3.2655472636815915e-08,
      "loss": 2.6968,
      "step": 10830
    },
    {
      "epoch": 53.930348258706466,
      "grad_norm": 9.780805587768555,
      "learning_rate": 3.259328358208955e-08,
      "loss": 2.5794,
      "step": 10840
    },
    {
      "epoch": 53.98009950248756,
      "grad_norm": 24.563343048095703,
      "learning_rate": 3.253109452736318e-08,
      "loss": 2.6568,
      "step": 10850
    },
    {
      "epoch": 54.0,
      "eval_loss": 1.749178171157837,
      "eval_runtime": 54.9134,
      "eval_samples_per_second": 3.66,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6188946015424165,
      "step": 10854
    },
    {
      "epoch": 54.02985074626866,
      "grad_norm": 7.558012962341309,
      "learning_rate": 3.246890547263682e-08,
      "loss": 2.5845,
      "step": 10860
    },
    {
      "epoch": 54.07960199004975,
      "grad_norm": 6.353367805480957,
      "learning_rate": 3.240671641791045e-08,
      "loss": 2.5773,
      "step": 10870
    },
    {
      "epoch": 54.12935323383085,
      "grad_norm": 18.8171443939209,
      "learning_rate": 3.234452736318408e-08,
      "loss": 2.7107,
      "step": 10880
    },
    {
      "epoch": 54.17910447761194,
      "grad_norm": 60.3653450012207,
      "learning_rate": 3.228233830845771e-08,
      "loss": 2.5626,
      "step": 10890
    },
    {
      "epoch": 54.22885572139303,
      "grad_norm": 9.016311645507812,
      "learning_rate": 3.2220149253731344e-08,
      "loss": 2.5425,
      "step": 10900
    },
    {
      "epoch": 54.27860696517413,
      "grad_norm": 6.275454044342041,
      "learning_rate": 3.2157960199004974e-08,
      "loss": 2.5485,
      "step": 10910
    },
    {
      "epoch": 54.32835820895522,
      "grad_norm": 11.084681510925293,
      "learning_rate": 3.2095771144278604e-08,
      "loss": 2.5738,
      "step": 10920
    },
    {
      "epoch": 54.37810945273632,
      "grad_norm": 10.92772388458252,
      "learning_rate": 3.2033582089552234e-08,
      "loss": 2.5329,
      "step": 10930
    },
    {
      "epoch": 54.42786069651741,
      "grad_norm": 13.158965110778809,
      "learning_rate": 3.1971393034825864e-08,
      "loss": 2.5295,
      "step": 10940
    },
    {
      "epoch": 54.47761194029851,
      "grad_norm": 9.488323211669922,
      "learning_rate": 3.19092039800995e-08,
      "loss": 2.6283,
      "step": 10950
    },
    {
      "epoch": 54.527363184079604,
      "grad_norm": 26.248994827270508,
      "learning_rate": 3.184701492537313e-08,
      "loss": 2.4663,
      "step": 10960
    },
    {
      "epoch": 54.5771144278607,
      "grad_norm": 13.182022094726562,
      "learning_rate": 3.178482587064677e-08,
      "loss": 2.5628,
      "step": 10970
    },
    {
      "epoch": 54.62686567164179,
      "grad_norm": 27.162668228149414,
      "learning_rate": 3.17226368159204e-08,
      "loss": 2.5723,
      "step": 10980
    },
    {
      "epoch": 54.67661691542288,
      "grad_norm": 19.664016723632812,
      "learning_rate": 3.1660447761194027e-08,
      "loss": 2.5341,
      "step": 10990
    },
    {
      "epoch": 54.72636815920398,
      "grad_norm": 8.001737594604492,
      "learning_rate": 3.159825870646766e-08,
      "loss": 2.5951,
      "step": 11000
    },
    {
      "epoch": 54.776119402985074,
      "grad_norm": 11.996772766113281,
      "learning_rate": 3.153606965174129e-08,
      "loss": 2.5642,
      "step": 11010
    },
    {
      "epoch": 54.82587064676617,
      "grad_norm": 18.365863800048828,
      "learning_rate": 3.147388059701492e-08,
      "loss": 2.4993,
      "step": 11020
    },
    {
      "epoch": 54.875621890547265,
      "grad_norm": 6.205034255981445,
      "learning_rate": 3.141169154228855e-08,
      "loss": 2.5481,
      "step": 11030
    },
    {
      "epoch": 54.92537313432836,
      "grad_norm": 10.987139701843262,
      "learning_rate": 3.134950248756218e-08,
      "loss": 2.5812,
      "step": 11040
    },
    {
      "epoch": 54.975124378109456,
      "grad_norm": 13.153599739074707,
      "learning_rate": 3.128731343283582e-08,
      "loss": 2.6299,
      "step": 11050
    },
    {
      "epoch": 55.0,
      "eval_loss": 1.7501827478408813,
      "eval_runtime": 54.8918,
      "eval_samples_per_second": 3.662,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6188142673521851,
      "step": 11055
    },
    {
      "epoch": 55.024875621890544,
      "grad_norm": 14.104415893554688,
      "learning_rate": 3.122512437810945e-08,
      "loss": 2.5327,
      "step": 11060
    },
    {
      "epoch": 55.07462686567164,
      "grad_norm": 8.46965503692627,
      "learning_rate": 3.1162935323383086e-08,
      "loss": 2.6371,
      "step": 11070
    },
    {
      "epoch": 55.124378109452735,
      "grad_norm": 25.761262893676758,
      "learning_rate": 3.1100746268656716e-08,
      "loss": 2.6406,
      "step": 11080
    },
    {
      "epoch": 55.17412935323383,
      "grad_norm": 8.845752716064453,
      "learning_rate": 3.1038557213930346e-08,
      "loss": 2.5234,
      "step": 11090
    },
    {
      "epoch": 55.223880597014926,
      "grad_norm": 30.214744567871094,
      "learning_rate": 3.097636815920398e-08,
      "loss": 2.6178,
      "step": 11100
    },
    {
      "epoch": 55.27363184079602,
      "grad_norm": 7.388950824737549,
      "learning_rate": 3.091417910447761e-08,
      "loss": 2.4596,
      "step": 11110
    },
    {
      "epoch": 55.32338308457712,
      "grad_norm": 17.60306739807129,
      "learning_rate": 3.085199004975124e-08,
      "loss": 2.4973,
      "step": 11120
    },
    {
      "epoch": 55.37313432835821,
      "grad_norm": 5.830843925476074,
      "learning_rate": 3.078980099502487e-08,
      "loss": 2.5962,
      "step": 11130
    },
    {
      "epoch": 55.4228855721393,
      "grad_norm": 19.407270431518555,
      "learning_rate": 3.07276119402985e-08,
      "loss": 2.6734,
      "step": 11140
    },
    {
      "epoch": 55.472636815920396,
      "grad_norm": 15.401960372924805,
      "learning_rate": 3.066542288557214e-08,
      "loss": 2.5303,
      "step": 11150
    },
    {
      "epoch": 55.52238805970149,
      "grad_norm": 7.348367691040039,
      "learning_rate": 3.060323383084577e-08,
      "loss": 2.5228,
      "step": 11160
    },
    {
      "epoch": 55.57213930348259,
      "grad_norm": 9.59393310546875,
      "learning_rate": 3.05410447761194e-08,
      "loss": 2.5946,
      "step": 11170
    },
    {
      "epoch": 55.62189054726368,
      "grad_norm": 16.848892211914062,
      "learning_rate": 3.0478855721393035e-08,
      "loss": 2.5991,
      "step": 11180
    },
    {
      "epoch": 55.67164179104478,
      "grad_norm": 22.154544830322266,
      "learning_rate": 3.0416666666666664e-08,
      "loss": 2.5961,
      "step": 11190
    },
    {
      "epoch": 55.72139303482587,
      "grad_norm": 10.064751625061035,
      "learning_rate": 3.03544776119403e-08,
      "loss": 2.5726,
      "step": 11200
    },
    {
      "epoch": 55.77114427860697,
      "grad_norm": 9.420202255249023,
      "learning_rate": 3.029228855721393e-08,
      "loss": 2.6095,
      "step": 11210
    },
    {
      "epoch": 55.82089552238806,
      "grad_norm": 7.899473190307617,
      "learning_rate": 3.023009950248756e-08,
      "loss": 2.6314,
      "step": 11220
    },
    {
      "epoch": 55.87064676616915,
      "grad_norm": 10.311331748962402,
      "learning_rate": 3.016791044776119e-08,
      "loss": 2.5798,
      "step": 11230
    },
    {
      "epoch": 55.92039800995025,
      "grad_norm": 9.717779159545898,
      "learning_rate": 3.010572139303482e-08,
      "loss": 2.707,
      "step": 11240
    },
    {
      "epoch": 55.97014925373134,
      "grad_norm": 16.78072738647461,
      "learning_rate": 3.004353233830846e-08,
      "loss": 2.6353,
      "step": 11250
    },
    {
      "epoch": 56.0,
      "eval_loss": 1.748745322227478,
      "eval_runtime": 54.4735,
      "eval_samples_per_second": 3.69,
      "eval_steps_per_second": 0.239,
      "eval_wer": 0.6190552699228792,
      "step": 11256
    },
    {
      "epoch": 56.01990049751244,
      "grad_norm": 8.201872825622559,
      "learning_rate": 2.998134328358209e-08,
      "loss": 2.5257,
      "step": 11260
    },
    {
      "epoch": 56.069651741293534,
      "grad_norm": 8.637201309204102,
      "learning_rate": 2.991915422885572e-08,
      "loss": 2.5107,
      "step": 11270
    },
    {
      "epoch": 56.11940298507463,
      "grad_norm": 9.807394027709961,
      "learning_rate": 2.9856965174129353e-08,
      "loss": 2.5655,
      "step": 11280
    },
    {
      "epoch": 56.169154228855724,
      "grad_norm": 12.135688781738281,
      "learning_rate": 2.979477611940298e-08,
      "loss": 2.6395,
      "step": 11290
    },
    {
      "epoch": 56.21890547263681,
      "grad_norm": 8.879673957824707,
      "learning_rate": 2.9732587064676616e-08,
      "loss": 2.5252,
      "step": 11300
    },
    {
      "epoch": 56.26865671641791,
      "grad_norm": 11.03530216217041,
      "learning_rate": 2.967039800995025e-08,
      "loss": 2.5727,
      "step": 11310
    },
    {
      "epoch": 56.318407960199,
      "grad_norm": 19.822418212890625,
      "learning_rate": 2.960820895522388e-08,
      "loss": 2.6712,
      "step": 11320
    },
    {
      "epoch": 56.3681592039801,
      "grad_norm": 11.386688232421875,
      "learning_rate": 2.954601990049751e-08,
      "loss": 2.5054,
      "step": 11330
    },
    {
      "epoch": 56.417910447761194,
      "grad_norm": 9.514450073242188,
      "learning_rate": 2.948383084577114e-08,
      "loss": 2.5223,
      "step": 11340
    },
    {
      "epoch": 56.46766169154229,
      "grad_norm": 11.15783405303955,
      "learning_rate": 2.9421641791044776e-08,
      "loss": 2.5233,
      "step": 11350
    },
    {
      "epoch": 56.517412935323385,
      "grad_norm": 9.415477752685547,
      "learning_rate": 2.9359452736318406e-08,
      "loss": 2.6399,
      "step": 11360
    },
    {
      "epoch": 56.56716417910448,
      "grad_norm": 7.081172943115234,
      "learning_rate": 2.929726368159204e-08,
      "loss": 2.6476,
      "step": 11370
    },
    {
      "epoch": 56.61691542288557,
      "grad_norm": 12.114727020263672,
      "learning_rate": 2.923507462686567e-08,
      "loss": 2.6228,
      "step": 11380
    },
    {
      "epoch": 56.666666666666664,
      "grad_norm": 7.31442928314209,
      "learning_rate": 2.91728855721393e-08,
      "loss": 2.5578,
      "step": 11390
    },
    {
      "epoch": 56.71641791044776,
      "grad_norm": 8.435174942016602,
      "learning_rate": 2.9110696517412935e-08,
      "loss": 2.6119,
      "step": 11400
    },
    {
      "epoch": 56.766169154228855,
      "grad_norm": 14.848917961120605,
      "learning_rate": 2.9048507462686565e-08,
      "loss": 2.5304,
      "step": 11410
    },
    {
      "epoch": 56.81592039800995,
      "grad_norm": 10.452244758605957,
      "learning_rate": 2.89863184079602e-08,
      "loss": 2.595,
      "step": 11420
    },
    {
      "epoch": 56.865671641791046,
      "grad_norm": 6.924099445343018,
      "learning_rate": 2.892412935323383e-08,
      "loss": 2.5042,
      "step": 11430
    },
    {
      "epoch": 56.91542288557214,
      "grad_norm": 19.8043155670166,
      "learning_rate": 2.8861940298507458e-08,
      "loss": 2.6327,
      "step": 11440
    },
    {
      "epoch": 56.96517412935324,
      "grad_norm": 10.387885093688965,
      "learning_rate": 2.8799751243781095e-08,
      "loss": 2.5613,
      "step": 11450
    },
    {
      "epoch": 57.0,
      "eval_loss": 1.7468613386154175,
      "eval_runtime": 54.854,
      "eval_samples_per_second": 3.664,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6184929305912596,
      "step": 11457
    },
    {
      "epoch": 57.014925373134325,
      "grad_norm": 20.224504470825195,
      "learning_rate": 2.8737562189054725e-08,
      "loss": 2.6364,
      "step": 11460
    },
    {
      "epoch": 57.06467661691542,
      "grad_norm": 10.989253044128418,
      "learning_rate": 2.8675373134328358e-08,
      "loss": 2.6906,
      "step": 11470
    },
    {
      "epoch": 57.114427860696516,
      "grad_norm": 8.516094207763672,
      "learning_rate": 2.8613184079601988e-08,
      "loss": 2.4741,
      "step": 11480
    },
    {
      "epoch": 57.16417910447761,
      "grad_norm": 15.357333183288574,
      "learning_rate": 2.8550995024875618e-08,
      "loss": 2.5126,
      "step": 11490
    },
    {
      "epoch": 57.21393034825871,
      "grad_norm": 8.459786415100098,
      "learning_rate": 2.8488805970149254e-08,
      "loss": 2.5389,
      "step": 11500
    },
    {
      "epoch": 57.2636815920398,
      "grad_norm": 16.64356231689453,
      "learning_rate": 2.8426616915422884e-08,
      "loss": 2.5851,
      "step": 11510
    },
    {
      "epoch": 57.3134328358209,
      "grad_norm": 10.695073127746582,
      "learning_rate": 2.8364427860696517e-08,
      "loss": 2.591,
      "step": 11520
    },
    {
      "epoch": 57.36318407960199,
      "grad_norm": 7.525224685668945,
      "learning_rate": 2.8302238805970147e-08,
      "loss": 2.5454,
      "step": 11530
    },
    {
      "epoch": 57.41293532338308,
      "grad_norm": 7.552605628967285,
      "learning_rate": 2.8240049751243777e-08,
      "loss": 2.6522,
      "step": 11540
    },
    {
      "epoch": 57.46268656716418,
      "grad_norm": 9.797518730163574,
      "learning_rate": 2.8177860696517414e-08,
      "loss": 2.65,
      "step": 11550
    },
    {
      "epoch": 57.51243781094527,
      "grad_norm": 21.356613159179688,
      "learning_rate": 2.8115671641791044e-08,
      "loss": 2.6535,
      "step": 11560
    },
    {
      "epoch": 57.56218905472637,
      "grad_norm": 25.04976463317871,
      "learning_rate": 2.8053482587064677e-08,
      "loss": 2.5822,
      "step": 11570
    },
    {
      "epoch": 57.61194029850746,
      "grad_norm": 6.021686553955078,
      "learning_rate": 2.7991293532338307e-08,
      "loss": 2.5624,
      "step": 11580
    },
    {
      "epoch": 57.66169154228856,
      "grad_norm": 6.424306869506836,
      "learning_rate": 2.7929104477611937e-08,
      "loss": 2.6662,
      "step": 11590
    },
    {
      "epoch": 57.711442786069654,
      "grad_norm": 5.5406670570373535,
      "learning_rate": 2.7866915422885573e-08,
      "loss": 2.5326,
      "step": 11600
    },
    {
      "epoch": 57.76119402985075,
      "grad_norm": 13.694026947021484,
      "learning_rate": 2.7804726368159203e-08,
      "loss": 2.5656,
      "step": 11610
    },
    {
      "epoch": 57.81094527363184,
      "grad_norm": 19.315473556518555,
      "learning_rate": 2.7742537313432833e-08,
      "loss": 2.5051,
      "step": 11620
    },
    {
      "epoch": 57.86069651741293,
      "grad_norm": 6.130607604980469,
      "learning_rate": 2.7680348258706466e-08,
      "loss": 2.4952,
      "step": 11630
    },
    {
      "epoch": 57.91044776119403,
      "grad_norm": 7.370049953460693,
      "learning_rate": 2.7618159203980096e-08,
      "loss": 2.5593,
      "step": 11640
    },
    {
      "epoch": 57.960199004975124,
      "grad_norm": 9.466297149658203,
      "learning_rate": 2.7555970149253733e-08,
      "loss": 2.5871,
      "step": 11650
    },
    {
      "epoch": 58.0,
      "eval_loss": 1.7486165761947632,
      "eval_runtime": 55.0678,
      "eval_samples_per_second": 3.65,
      "eval_steps_per_second": 0.236,
      "eval_wer": 0.6182519280205655,
      "step": 11658
    },
    {
      "epoch": 58.00995024875622,
      "grad_norm": 15.277399063110352,
      "learning_rate": 2.7493781094527362e-08,
      "loss": 2.5785,
      "step": 11660
    },
    {
      "epoch": 58.059701492537314,
      "grad_norm": 11.762680053710938,
      "learning_rate": 2.7431592039800992e-08,
      "loss": 2.4622,
      "step": 11670
    },
    {
      "epoch": 58.10945273631841,
      "grad_norm": 18.00480842590332,
      "learning_rate": 2.7369402985074626e-08,
      "loss": 2.6753,
      "step": 11680
    },
    {
      "epoch": 58.159203980099505,
      "grad_norm": 11.721050262451172,
      "learning_rate": 2.7307213930348255e-08,
      "loss": 2.6925,
      "step": 11690
    },
    {
      "epoch": 58.208955223880594,
      "grad_norm": 5.994294166564941,
      "learning_rate": 2.7245024875621892e-08,
      "loss": 2.5429,
      "step": 11700
    },
    {
      "epoch": 58.25870646766169,
      "grad_norm": 8.834815979003906,
      "learning_rate": 2.7182835820895522e-08,
      "loss": 2.5003,
      "step": 11710
    },
    {
      "epoch": 58.308457711442784,
      "grad_norm": 7.928008079528809,
      "learning_rate": 2.7120646766169152e-08,
      "loss": 2.6413,
      "step": 11720
    },
    {
      "epoch": 58.35820895522388,
      "grad_norm": 14.583810806274414,
      "learning_rate": 2.7058457711442785e-08,
      "loss": 2.5746,
      "step": 11730
    },
    {
      "epoch": 58.407960199004975,
      "grad_norm": 7.777055740356445,
      "learning_rate": 2.6996268656716415e-08,
      "loss": 2.6674,
      "step": 11740
    },
    {
      "epoch": 58.45771144278607,
      "grad_norm": 5.594944000244141,
      "learning_rate": 2.693407960199005e-08,
      "loss": 2.552,
      "step": 11750
    },
    {
      "epoch": 58.507462686567166,
      "grad_norm": 14.366602897644043,
      "learning_rate": 2.687189054726368e-08,
      "loss": 2.5968,
      "step": 11760
    },
    {
      "epoch": 58.55721393034826,
      "grad_norm": 9.839268684387207,
      "learning_rate": 2.680970149253731e-08,
      "loss": 2.6787,
      "step": 11770
    },
    {
      "epoch": 58.60696517412935,
      "grad_norm": 19.1937313079834,
      "learning_rate": 2.6747512437810944e-08,
      "loss": 2.5334,
      "step": 11780
    },
    {
      "epoch": 58.656716417910445,
      "grad_norm": 11.663838386535645,
      "learning_rate": 2.6685323383084574e-08,
      "loss": 2.5159,
      "step": 11790
    },
    {
      "epoch": 58.70646766169154,
      "grad_norm": 6.68789529800415,
      "learning_rate": 2.662313432835821e-08,
      "loss": 2.5169,
      "step": 11800
    },
    {
      "epoch": 58.756218905472636,
      "grad_norm": 83.05772399902344,
      "learning_rate": 2.656094527363184e-08,
      "loss": 2.6412,
      "step": 11810
    },
    {
      "epoch": 58.80597014925373,
      "grad_norm": 7.809956073760986,
      "learning_rate": 2.649875621890547e-08,
      "loss": 2.672,
      "step": 11820
    },
    {
      "epoch": 58.85572139303483,
      "grad_norm": 10.411999702453613,
      "learning_rate": 2.64365671641791e-08,
      "loss": 2.43,
      "step": 11830
    },
    {
      "epoch": 58.90547263681592,
      "grad_norm": 10.811592102050781,
      "learning_rate": 2.6374378109452734e-08,
      "loss": 2.597,
      "step": 11840
    },
    {
      "epoch": 58.95522388059702,
      "grad_norm": 12.399470329284668,
      "learning_rate": 2.631218905472637e-08,
      "loss": 2.6172,
      "step": 11850
    },
    {
      "epoch": 59.0,
      "eval_loss": 1.743545413017273,
      "eval_runtime": 54.6989,
      "eval_samples_per_second": 3.675,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6178502570694088,
      "step": 11859
    },
    {
      "epoch": 59.004975124378106,
      "grad_norm": 15.90406322479248,
      "learning_rate": 2.625e-08,
      "loss": 2.5664,
      "step": 11860
    },
    {
      "epoch": 59.0547263681592,
      "grad_norm": 7.497384071350098,
      "learning_rate": 2.618781094527363e-08,
      "loss": 2.56,
      "step": 11870
    },
    {
      "epoch": 59.1044776119403,
      "grad_norm": 10.447818756103516,
      "learning_rate": 2.612562189054726e-08,
      "loss": 2.5321,
      "step": 11880
    },
    {
      "epoch": 59.15422885572139,
      "grad_norm": 8.943414688110352,
      "learning_rate": 2.6063432835820893e-08,
      "loss": 2.4896,
      "step": 11890
    },
    {
      "epoch": 59.20398009950249,
      "grad_norm": 20.120027542114258,
      "learning_rate": 2.600124378109453e-08,
      "loss": 2.5615,
      "step": 11900
    },
    {
      "epoch": 59.25373134328358,
      "grad_norm": 20.207679748535156,
      "learning_rate": 2.593905472636816e-08,
      "loss": 2.5886,
      "step": 11910
    },
    {
      "epoch": 59.30348258706468,
      "grad_norm": 8.405997276306152,
      "learning_rate": 2.587686567164179e-08,
      "loss": 2.4711,
      "step": 11920
    },
    {
      "epoch": 59.353233830845774,
      "grad_norm": 8.604598999023438,
      "learning_rate": 2.581467661691542e-08,
      "loss": 2.6638,
      "step": 11930
    },
    {
      "epoch": 59.40298507462686,
      "grad_norm": 27.402830123901367,
      "learning_rate": 2.5752487562189053e-08,
      "loss": 2.6603,
      "step": 11940
    },
    {
      "epoch": 59.45273631840796,
      "grad_norm": 6.1326985359191895,
      "learning_rate": 2.569029850746269e-08,
      "loss": 2.516,
      "step": 11950
    },
    {
      "epoch": 59.50248756218905,
      "grad_norm": 7.769306182861328,
      "learning_rate": 2.562810945273632e-08,
      "loss": 2.5337,
      "step": 11960
    },
    {
      "epoch": 59.55223880597015,
      "grad_norm": 9.29305648803711,
      "learning_rate": 2.556592039800995e-08,
      "loss": 2.5397,
      "step": 11970
    },
    {
      "epoch": 59.601990049751244,
      "grad_norm": 9.563431739807129,
      "learning_rate": 2.550373134328358e-08,
      "loss": 2.743,
      "step": 11980
    },
    {
      "epoch": 59.65174129353234,
      "grad_norm": 18.39816665649414,
      "learning_rate": 2.5441542288557212e-08,
      "loss": 2.582,
      "step": 11990
    },
    {
      "epoch": 59.701492537313435,
      "grad_norm": 11.258341789245605,
      "learning_rate": 2.537935323383085e-08,
      "loss": 2.5986,
      "step": 12000
    },
    {
      "epoch": 59.75124378109453,
      "grad_norm": 10.614891052246094,
      "learning_rate": 2.531716417910448e-08,
      "loss": 2.4768,
      "step": 12010
    },
    {
      "epoch": 59.80099502487562,
      "grad_norm": 27.88168716430664,
      "learning_rate": 2.5254975124378108e-08,
      "loss": 2.6851,
      "step": 12020
    },
    {
      "epoch": 59.850746268656714,
      "grad_norm": 17.957326889038086,
      "learning_rate": 2.5192786069651738e-08,
      "loss": 2.6185,
      "step": 12030
    },
    {
      "epoch": 59.90049751243781,
      "grad_norm": 7.207729816436768,
      "learning_rate": 2.513059701492537e-08,
      "loss": 2.6123,
      "step": 12040
    },
    {
      "epoch": 59.950248756218905,
      "grad_norm": 10.947501182556152,
      "learning_rate": 2.5068407960199005e-08,
      "loss": 2.5128,
      "step": 12050
    },
    {
      "epoch": 60.0,
      "grad_norm": 8.449090957641602,
      "learning_rate": 2.5006218905472638e-08,
      "loss": 2.5693,
      "step": 12060
    },
    {
      "epoch": 60.0,
      "eval_loss": 1.744126796722412,
      "eval_runtime": 54.8616,
      "eval_samples_per_second": 3.664,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6169665809768637,
      "step": 12060
    },
    {
      "epoch": 60.049751243781095,
      "grad_norm": 8.531131744384766,
      "learning_rate": 2.4944029850746268e-08,
      "loss": 2.5889,
      "step": 12070
    },
    {
      "epoch": 60.09950248756219,
      "grad_norm": 20.713947296142578,
      "learning_rate": 2.4881840796019898e-08,
      "loss": 2.4247,
      "step": 12080
    },
    {
      "epoch": 60.149253731343286,
      "grad_norm": 27.689748764038086,
      "learning_rate": 2.481965174129353e-08,
      "loss": 2.6186,
      "step": 12090
    },
    {
      "epoch": 60.19900497512438,
      "grad_norm": 7.655657768249512,
      "learning_rate": 2.4757462686567164e-08,
      "loss": 2.5031,
      "step": 12100
    },
    {
      "epoch": 60.24875621890547,
      "grad_norm": 10.93252944946289,
      "learning_rate": 2.4695273631840797e-08,
      "loss": 2.6026,
      "step": 12110
    },
    {
      "epoch": 60.298507462686565,
      "grad_norm": 9.224724769592285,
      "learning_rate": 2.4633084577114427e-08,
      "loss": 2.6054,
      "step": 12120
    },
    {
      "epoch": 60.34825870646766,
      "grad_norm": 20.740947723388672,
      "learning_rate": 2.4570895522388057e-08,
      "loss": 2.6748,
      "step": 12130
    },
    {
      "epoch": 60.398009950248756,
      "grad_norm": 7.200096607208252,
      "learning_rate": 2.450870646766169e-08,
      "loss": 2.6877,
      "step": 12140
    },
    {
      "epoch": 60.44776119402985,
      "grad_norm": 8.647165298461914,
      "learning_rate": 2.4446517412935324e-08,
      "loss": 2.6142,
      "step": 12150
    },
    {
      "epoch": 60.49751243781095,
      "grad_norm": 4.834940433502197,
      "learning_rate": 2.4384328358208957e-08,
      "loss": 2.536,
      "step": 12160
    },
    {
      "epoch": 60.54726368159204,
      "grad_norm": 9.500268936157227,
      "learning_rate": 2.4322139303482587e-08,
      "loss": 2.5204,
      "step": 12170
    },
    {
      "epoch": 60.59701492537313,
      "grad_norm": 13.47887134552002,
      "learning_rate": 2.4259950248756217e-08,
      "loss": 2.6136,
      "step": 12180
    },
    {
      "epoch": 60.646766169154226,
      "grad_norm": 8.616329193115234,
      "learning_rate": 2.419776119402985e-08,
      "loss": 2.5944,
      "step": 12190
    },
    {
      "epoch": 60.69651741293532,
      "grad_norm": 14.079524040222168,
      "learning_rate": 2.413557213930348e-08,
      "loss": 2.578,
      "step": 12200
    },
    {
      "epoch": 60.74626865671642,
      "grad_norm": 13.70647144317627,
      "learning_rate": 2.4073383084577116e-08,
      "loss": 2.5133,
      "step": 12210
    },
    {
      "epoch": 60.79601990049751,
      "grad_norm": 7.544736862182617,
      "learning_rate": 2.4011194029850746e-08,
      "loss": 2.5766,
      "step": 12220
    },
    {
      "epoch": 60.84577114427861,
      "grad_norm": 18.91160011291504,
      "learning_rate": 2.3949004975124376e-08,
      "loss": 2.5878,
      "step": 12230
    },
    {
      "epoch": 60.8955223880597,
      "grad_norm": 8.78632640838623,
      "learning_rate": 2.388681592039801e-08,
      "loss": 2.5529,
      "step": 12240
    },
    {
      "epoch": 60.9452736318408,
      "grad_norm": 6.471785545349121,
      "learning_rate": 2.382462686567164e-08,
      "loss": 2.5562,
      "step": 12250
    },
    {
      "epoch": 60.995024875621894,
      "grad_norm": 8.703667640686035,
      "learning_rate": 2.3762437810945272e-08,
      "loss": 2.6599,
      "step": 12260
    },
    {
      "epoch": 61.0,
      "eval_loss": 1.7473539113998413,
      "eval_runtime": 54.8921,
      "eval_samples_per_second": 3.662,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.616805912596401,
      "step": 12261
    },
    {
      "epoch": 61.04477611940298,
      "grad_norm": 9.112048149108887,
      "learning_rate": 2.3700248756218905e-08,
      "loss": 2.5529,
      "step": 12270
    },
    {
      "epoch": 61.09452736318408,
      "grad_norm": 14.736770629882812,
      "learning_rate": 2.3638059701492535e-08,
      "loss": 2.5183,
      "step": 12280
    },
    {
      "epoch": 61.14427860696517,
      "grad_norm": 13.727509498596191,
      "learning_rate": 2.357587064676617e-08,
      "loss": 2.6214,
      "step": 12290
    },
    {
      "epoch": 61.19402985074627,
      "grad_norm": 13.325010299682617,
      "learning_rate": 2.35136815920398e-08,
      "loss": 2.4764,
      "step": 12300
    },
    {
      "epoch": 61.243781094527364,
      "grad_norm": 15.089162826538086,
      "learning_rate": 2.3451492537313432e-08,
      "loss": 2.5369,
      "step": 12310
    },
    {
      "epoch": 61.29353233830846,
      "grad_norm": 10.684605598449707,
      "learning_rate": 2.3389303482587065e-08,
      "loss": 2.6123,
      "step": 12320
    },
    {
      "epoch": 61.343283582089555,
      "grad_norm": 9.243149757385254,
      "learning_rate": 2.3327114427860695e-08,
      "loss": 2.5695,
      "step": 12330
    },
    {
      "epoch": 61.39303482587065,
      "grad_norm": 21.109241485595703,
      "learning_rate": 2.3264925373134328e-08,
      "loss": 2.5194,
      "step": 12340
    },
    {
      "epoch": 61.44278606965174,
      "grad_norm": 8.906634330749512,
      "learning_rate": 2.3202736318407958e-08,
      "loss": 2.5706,
      "step": 12350
    },
    {
      "epoch": 61.492537313432834,
      "grad_norm": 6.351418972015381,
      "learning_rate": 2.314054726368159e-08,
      "loss": 2.693,
      "step": 12360
    },
    {
      "epoch": 61.54228855721393,
      "grad_norm": 12.735198974609375,
      "learning_rate": 2.3078358208955224e-08,
      "loss": 2.5893,
      "step": 12370
    },
    {
      "epoch": 61.592039800995025,
      "grad_norm": 6.510805606842041,
      "learning_rate": 2.3016169154228854e-08,
      "loss": 2.5805,
      "step": 12380
    },
    {
      "epoch": 61.64179104477612,
      "grad_norm": 5.415046215057373,
      "learning_rate": 2.2953980099502487e-08,
      "loss": 2.5146,
      "step": 12390
    },
    {
      "epoch": 61.691542288557216,
      "grad_norm": 7.286680698394775,
      "learning_rate": 2.2891791044776117e-08,
      "loss": 2.7166,
      "step": 12400
    },
    {
      "epoch": 61.74129353233831,
      "grad_norm": 12.407971382141113,
      "learning_rate": 2.2829601990049747e-08,
      "loss": 2.6882,
      "step": 12410
    },
    {
      "epoch": 61.791044776119406,
      "grad_norm": 13.886772155761719,
      "learning_rate": 2.2767412935323384e-08,
      "loss": 2.5478,
      "step": 12420
    },
    {
      "epoch": 61.840796019900495,
      "grad_norm": 12.121258735656738,
      "learning_rate": 2.2705223880597014e-08,
      "loss": 2.5977,
      "step": 12430
    },
    {
      "epoch": 61.89054726368159,
      "grad_norm": 5.717807292938232,
      "learning_rate": 2.2643034825870647e-08,
      "loss": 2.6503,
      "step": 12440
    },
    {
      "epoch": 61.940298507462686,
      "grad_norm": 14.006301879882812,
      "learning_rate": 2.2580845771144277e-08,
      "loss": 2.6052,
      "step": 12450
    },
    {
      "epoch": 61.99004975124378,
      "grad_norm": 8.788012504577637,
      "learning_rate": 2.2518656716417907e-08,
      "loss": 2.5563,
      "step": 12460
    },
    {
      "epoch": 62.0,
      "eval_loss": 1.7437655925750732,
      "eval_runtime": 54.4237,
      "eval_samples_per_second": 3.693,
      "eval_steps_per_second": 0.239,
      "eval_wer": 0.6164845758354756,
      "step": 12462
    },
    {
      "epoch": 62.039800995024876,
      "grad_norm": 9.205987930297852,
      "learning_rate": 2.2456467661691543e-08,
      "loss": 2.514,
      "step": 12470
    },
    {
      "epoch": 62.08955223880597,
      "grad_norm": 10.214042663574219,
      "learning_rate": 2.2394278606965173e-08,
      "loss": 2.5735,
      "step": 12480
    },
    {
      "epoch": 62.13930348258707,
      "grad_norm": 8.81572151184082,
      "learning_rate": 2.2332089552238806e-08,
      "loss": 2.5175,
      "step": 12490
    },
    {
      "epoch": 62.18905472636816,
      "grad_norm": 11.915310859680176,
      "learning_rate": 2.2269900497512436e-08,
      "loss": 2.6015,
      "step": 12500
    },
    {
      "epoch": 62.23880597014925,
      "grad_norm": 11.43213176727295,
      "learning_rate": 2.2207711442786066e-08,
      "loss": 2.6539,
      "step": 12510
    },
    {
      "epoch": 62.288557213930346,
      "grad_norm": 7.585986137390137,
      "learning_rate": 2.21455223880597e-08,
      "loss": 2.5709,
      "step": 12520
    },
    {
      "epoch": 62.33830845771144,
      "grad_norm": 9.862466812133789,
      "learning_rate": 2.2083333333333333e-08,
      "loss": 2.6109,
      "step": 12530
    },
    {
      "epoch": 62.38805970149254,
      "grad_norm": 13.263501167297363,
      "learning_rate": 2.2021144278606966e-08,
      "loss": 2.6519,
      "step": 12540
    },
    {
      "epoch": 62.43781094527363,
      "grad_norm": 11.51648998260498,
      "learning_rate": 2.1958955223880596e-08,
      "loss": 2.5267,
      "step": 12550
    },
    {
      "epoch": 62.48756218905473,
      "grad_norm": 7.9603424072265625,
      "learning_rate": 2.1896766169154226e-08,
      "loss": 2.5075,
      "step": 12560
    },
    {
      "epoch": 62.53731343283582,
      "grad_norm": 6.826436519622803,
      "learning_rate": 2.183457711442786e-08,
      "loss": 2.521,
      "step": 12570
    },
    {
      "epoch": 62.58706467661692,
      "grad_norm": 10.825465202331543,
      "learning_rate": 2.1772388059701492e-08,
      "loss": 2.44,
      "step": 12580
    },
    {
      "epoch": 62.63681592039801,
      "grad_norm": 10.833799362182617,
      "learning_rate": 2.1710199004975125e-08,
      "loss": 2.5179,
      "step": 12590
    },
    {
      "epoch": 62.6865671641791,
      "grad_norm": 15.645363807678223,
      "learning_rate": 2.1648009950248755e-08,
      "loss": 2.5927,
      "step": 12600
    },
    {
      "epoch": 62.7363184079602,
      "grad_norm": 11.259777069091797,
      "learning_rate": 2.1585820895522385e-08,
      "loss": 2.5062,
      "step": 12610
    },
    {
      "epoch": 62.78606965174129,
      "grad_norm": 10.059137344360352,
      "learning_rate": 2.1523631840796018e-08,
      "loss": 2.574,
      "step": 12620
    },
    {
      "epoch": 62.83582089552239,
      "grad_norm": 7.127941608428955,
      "learning_rate": 2.146144278606965e-08,
      "loss": 2.5662,
      "step": 12630
    },
    {
      "epoch": 62.885572139303484,
      "grad_norm": 14.10925579071045,
      "learning_rate": 2.1399253731343285e-08,
      "loss": 2.539,
      "step": 12640
    },
    {
      "epoch": 62.93532338308458,
      "grad_norm": 6.9202775955200195,
      "learning_rate": 2.1337064676616915e-08,
      "loss": 2.5436,
      "step": 12650
    },
    {
      "epoch": 62.985074626865675,
      "grad_norm": 9.7735013961792,
      "learning_rate": 2.1274875621890544e-08,
      "loss": 2.5295,
      "step": 12660
    },
    {
      "epoch": 63.0,
      "eval_loss": 1.742337703704834,
      "eval_runtime": 55.892,
      "eval_samples_per_second": 3.596,
      "eval_steps_per_second": 0.233,
      "eval_wer": 0.6163239074550129,
      "step": 12663
    },
    {
      "epoch": 63.03482587064676,
      "grad_norm": 18.45573616027832,
      "learning_rate": 2.1212686567164178e-08,
      "loss": 2.6423,
      "step": 12670
    },
    {
      "epoch": 63.08457711442786,
      "grad_norm": 13.707225799560547,
      "learning_rate": 2.115049751243781e-08,
      "loss": 2.7091,
      "step": 12680
    },
    {
      "epoch": 63.134328358208954,
      "grad_norm": 10.176962852478027,
      "learning_rate": 2.1088308457711444e-08,
      "loss": 2.512,
      "step": 12690
    },
    {
      "epoch": 63.18407960199005,
      "grad_norm": 19.230669021606445,
      "learning_rate": 2.1026119402985074e-08,
      "loss": 2.538,
      "step": 12700
    },
    {
      "epoch": 63.233830845771145,
      "grad_norm": 12.015585899353027,
      "learning_rate": 2.0963930348258704e-08,
      "loss": 2.5893,
      "step": 12710
    },
    {
      "epoch": 63.28358208955224,
      "grad_norm": 29.32846450805664,
      "learning_rate": 2.0901741293532337e-08,
      "loss": 2.6236,
      "step": 12720
    },
    {
      "epoch": 63.333333333333336,
      "grad_norm": 21.052616119384766,
      "learning_rate": 2.0839552238805967e-08,
      "loss": 2.5384,
      "step": 12730
    },
    {
      "epoch": 63.38308457711443,
      "grad_norm": 8.911526679992676,
      "learning_rate": 2.0777363184079603e-08,
      "loss": 2.5825,
      "step": 12740
    },
    {
      "epoch": 63.43283582089552,
      "grad_norm": 9.202142715454102,
      "learning_rate": 2.0715174129353233e-08,
      "loss": 2.564,
      "step": 12750
    },
    {
      "epoch": 63.482587064676615,
      "grad_norm": 8.352992057800293,
      "learning_rate": 2.0652985074626863e-08,
      "loss": 2.6189,
      "step": 12760
    },
    {
      "epoch": 63.53233830845771,
      "grad_norm": 9.696529388427734,
      "learning_rate": 2.0590796019900497e-08,
      "loss": 2.5432,
      "step": 12770
    },
    {
      "epoch": 63.582089552238806,
      "grad_norm": 9.517684936523438,
      "learning_rate": 2.0528606965174126e-08,
      "loss": 2.5369,
      "step": 12780
    },
    {
      "epoch": 63.6318407960199,
      "grad_norm": 13.405790328979492,
      "learning_rate": 2.0466417910447763e-08,
      "loss": 2.5327,
      "step": 12790
    },
    {
      "epoch": 63.681592039801,
      "grad_norm": 11.517354011535645,
      "learning_rate": 2.0404228855721393e-08,
      "loss": 2.5732,
      "step": 12800
    },
    {
      "epoch": 63.73134328358209,
      "grad_norm": 12.193466186523438,
      "learning_rate": 2.0342039800995023e-08,
      "loss": 2.5672,
      "step": 12810
    },
    {
      "epoch": 63.78109452736319,
      "grad_norm": 10.074650764465332,
      "learning_rate": 2.0279850746268656e-08,
      "loss": 2.4221,
      "step": 12820
    },
    {
      "epoch": 63.830845771144276,
      "grad_norm": 22.188018798828125,
      "learning_rate": 2.0217661691542286e-08,
      "loss": 2.6098,
      "step": 12830
    },
    {
      "epoch": 63.88059701492537,
      "grad_norm": 30.722929000854492,
      "learning_rate": 2.015547263681592e-08,
      "loss": 2.5599,
      "step": 12840
    },
    {
      "epoch": 63.930348258706466,
      "grad_norm": 10.658068656921387,
      "learning_rate": 2.0093283582089552e-08,
      "loss": 2.6133,
      "step": 12850
    },
    {
      "epoch": 63.98009950248756,
      "grad_norm": 9.639474868774414,
      "learning_rate": 2.0031094527363182e-08,
      "loss": 2.4958,
      "step": 12860
    },
    {
      "epoch": 64.0,
      "eval_loss": 1.7430044412612915,
      "eval_runtime": 54.6078,
      "eval_samples_per_second": 3.681,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6158419023136247,
      "step": 12864
    },
    {
      "epoch": 64.02985074626865,
      "grad_norm": 7.691218376159668,
      "learning_rate": 1.9968905472636815e-08,
      "loss": 2.62,
      "step": 12870
    },
    {
      "epoch": 64.07960199004975,
      "grad_norm": 12.850301742553711,
      "learning_rate": 1.9906716417910445e-08,
      "loss": 2.5936,
      "step": 12880
    },
    {
      "epoch": 64.12935323383084,
      "grad_norm": 8.763204574584961,
      "learning_rate": 1.984452736318408e-08,
      "loss": 2.6107,
      "step": 12890
    },
    {
      "epoch": 64.17910447761194,
      "grad_norm": 8.416143417358398,
      "learning_rate": 1.9782338308457712e-08,
      "loss": 2.516,
      "step": 12900
    },
    {
      "epoch": 64.22885572139303,
      "grad_norm": 8.768400192260742,
      "learning_rate": 1.972014925373134e-08,
      "loss": 2.6308,
      "step": 12910
    },
    {
      "epoch": 64.27860696517413,
      "grad_norm": 54.61814880371094,
      "learning_rate": 1.9657960199004975e-08,
      "loss": 2.5628,
      "step": 12920
    },
    {
      "epoch": 64.32835820895522,
      "grad_norm": 10.313943862915039,
      "learning_rate": 1.9595771144278605e-08,
      "loss": 2.5101,
      "step": 12930
    },
    {
      "epoch": 64.37810945273633,
      "grad_norm": 8.643865585327148,
      "learning_rate": 1.9533582089552238e-08,
      "loss": 2.5409,
      "step": 12940
    },
    {
      "epoch": 64.42786069651741,
      "grad_norm": 26.34177589416504,
      "learning_rate": 1.947139303482587e-08,
      "loss": 2.5596,
      "step": 12950
    },
    {
      "epoch": 64.4776119402985,
      "grad_norm": 10.992036819458008,
      "learning_rate": 1.94092039800995e-08,
      "loss": 2.4806,
      "step": 12960
    },
    {
      "epoch": 64.5273631840796,
      "grad_norm": 11.057731628417969,
      "learning_rate": 1.9347014925373134e-08,
      "loss": 2.5276,
      "step": 12970
    },
    {
      "epoch": 64.57711442786069,
      "grad_norm": 6.7040791511535645,
      "learning_rate": 1.9284825870646764e-08,
      "loss": 2.594,
      "step": 12980
    },
    {
      "epoch": 64.6268656716418,
      "grad_norm": 15.049653053283691,
      "learning_rate": 1.9222636815920397e-08,
      "loss": 2.4632,
      "step": 12990
    },
    {
      "epoch": 64.67661691542288,
      "grad_norm": 8.907625198364258,
      "learning_rate": 1.916044776119403e-08,
      "loss": 2.5894,
      "step": 13000
    },
    {
      "epoch": 64.72636815920399,
      "grad_norm": 7.142899513244629,
      "learning_rate": 1.909825870646766e-08,
      "loss": 2.4959,
      "step": 13010
    },
    {
      "epoch": 64.77611940298507,
      "grad_norm": 11.325250625610352,
      "learning_rate": 1.9036069651741294e-08,
      "loss": 2.5485,
      "step": 13020
    },
    {
      "epoch": 64.82587064676616,
      "grad_norm": 9.208454132080078,
      "learning_rate": 1.8973880597014924e-08,
      "loss": 2.6299,
      "step": 13030
    },
    {
      "epoch": 64.87562189054727,
      "grad_norm": 12.75424575805664,
      "learning_rate": 1.8911691542288557e-08,
      "loss": 2.4982,
      "step": 13040
    },
    {
      "epoch": 64.92537313432835,
      "grad_norm": 9.460578918457031,
      "learning_rate": 1.8849502487562187e-08,
      "loss": 2.5192,
      "step": 13050
    },
    {
      "epoch": 64.97512437810946,
      "grad_norm": 10.297282218933105,
      "learning_rate": 1.878731343283582e-08,
      "loss": 2.7346,
      "step": 13060
    },
    {
      "epoch": 65.0,
      "eval_loss": 1.742141604423523,
      "eval_runtime": 55.117,
      "eval_samples_per_second": 3.647,
      "eval_steps_per_second": 0.236,
      "eval_wer": 0.6152795629820051,
      "step": 13065
    },
    {
      "epoch": 65.02487562189054,
      "grad_norm": 12.852729797363281,
      "learning_rate": 1.8725124378109453e-08,
      "loss": 2.5357,
      "step": 13070
    },
    {
      "epoch": 65.07462686567165,
      "grad_norm": 11.744860649108887,
      "learning_rate": 1.8662935323383083e-08,
      "loss": 2.5069,
      "step": 13080
    },
    {
      "epoch": 65.12437810945273,
      "grad_norm": 11.611035346984863,
      "learning_rate": 1.8600746268656716e-08,
      "loss": 2.6326,
      "step": 13090
    },
    {
      "epoch": 65.17412935323384,
      "grad_norm": 9.88477897644043,
      "learning_rate": 1.8538557213930346e-08,
      "loss": 2.5828,
      "step": 13100
    },
    {
      "epoch": 65.22388059701493,
      "grad_norm": 8.389058113098145,
      "learning_rate": 1.847636815920398e-08,
      "loss": 2.5766,
      "step": 13110
    },
    {
      "epoch": 65.27363184079601,
      "grad_norm": 14.536524772644043,
      "learning_rate": 1.8414179104477613e-08,
      "loss": 2.5772,
      "step": 13120
    },
    {
      "epoch": 65.32338308457712,
      "grad_norm": 16.251541137695312,
      "learning_rate": 1.8351990049751242e-08,
      "loss": 2.6056,
      "step": 13130
    },
    {
      "epoch": 65.3731343283582,
      "grad_norm": 6.321529865264893,
      "learning_rate": 1.8289800995024876e-08,
      "loss": 2.5745,
      "step": 13140
    },
    {
      "epoch": 65.42288557213931,
      "grad_norm": 7.776747703552246,
      "learning_rate": 1.8227611940298506e-08,
      "loss": 2.5119,
      "step": 13150
    },
    {
      "epoch": 65.4726368159204,
      "grad_norm": 41.907562255859375,
      "learning_rate": 1.816542288557214e-08,
      "loss": 2.6065,
      "step": 13160
    },
    {
      "epoch": 65.5223880597015,
      "grad_norm": 8.825675010681152,
      "learning_rate": 1.8103233830845772e-08,
      "loss": 2.4504,
      "step": 13170
    },
    {
      "epoch": 65.57213930348259,
      "grad_norm": 10.01198673248291,
      "learning_rate": 1.8041044776119402e-08,
      "loss": 2.585,
      "step": 13180
    },
    {
      "epoch": 65.62189054726367,
      "grad_norm": 7.660529613494873,
      "learning_rate": 1.7978855721393035e-08,
      "loss": 2.5031,
      "step": 13190
    },
    {
      "epoch": 65.67164179104478,
      "grad_norm": 15.603519439697266,
      "learning_rate": 1.7916666666666665e-08,
      "loss": 2.4833,
      "step": 13200
    },
    {
      "epoch": 65.72139303482587,
      "grad_norm": 11.654173851013184,
      "learning_rate": 1.7854477611940298e-08,
      "loss": 2.5308,
      "step": 13210
    },
    {
      "epoch": 65.77114427860697,
      "grad_norm": 9.056235313415527,
      "learning_rate": 1.779228855721393e-08,
      "loss": 2.7019,
      "step": 13220
    },
    {
      "epoch": 65.82089552238806,
      "grad_norm": 7.776330947875977,
      "learning_rate": 1.773009950248756e-08,
      "loss": 2.71,
      "step": 13230
    },
    {
      "epoch": 65.87064676616916,
      "grad_norm": 38.4303092956543,
      "learning_rate": 1.7667910447761195e-08,
      "loss": 2.6503,
      "step": 13240
    },
    {
      "epoch": 65.92039800995025,
      "grad_norm": 10.079785346984863,
      "learning_rate": 1.7605721393034824e-08,
      "loss": 2.5343,
      "step": 13250
    },
    {
      "epoch": 65.97014925373135,
      "grad_norm": 18.14642333984375,
      "learning_rate": 1.7543532338308458e-08,
      "loss": 2.5522,
      "step": 13260
    },
    {
      "epoch": 66.0,
      "eval_loss": 1.7402900457382202,
      "eval_runtime": 55.8136,
      "eval_samples_per_second": 3.601,
      "eval_steps_per_second": 0.233,
      "eval_wer": 0.6151188946015425,
      "step": 13266
    },
    {
      "epoch": 66.01990049751244,
      "grad_norm": 8.577829360961914,
      "learning_rate": 1.748134328358209e-08,
      "loss": 2.5581,
      "step": 13270
    },
    {
      "epoch": 66.06965174129353,
      "grad_norm": 12.502483367919922,
      "learning_rate": 1.741915422885572e-08,
      "loss": 2.544,
      "step": 13280
    },
    {
      "epoch": 66.11940298507463,
      "grad_norm": 10.153834342956543,
      "learning_rate": 1.7356965174129354e-08,
      "loss": 2.5379,
      "step": 13290
    },
    {
      "epoch": 66.16915422885572,
      "grad_norm": 5.907919406890869,
      "learning_rate": 1.7294776119402984e-08,
      "loss": 2.5306,
      "step": 13300
    },
    {
      "epoch": 66.21890547263682,
      "grad_norm": 12.142834663391113,
      "learning_rate": 1.7232587064676614e-08,
      "loss": 2.6013,
      "step": 13310
    },
    {
      "epoch": 66.26865671641791,
      "grad_norm": 10.217930793762207,
      "learning_rate": 1.717039800995025e-08,
      "loss": 2.6105,
      "step": 13320
    },
    {
      "epoch": 66.31840796019901,
      "grad_norm": 21.405675888061523,
      "learning_rate": 1.710820895522388e-08,
      "loss": 2.4787,
      "step": 13330
    },
    {
      "epoch": 66.3681592039801,
      "grad_norm": 6.916533946990967,
      "learning_rate": 1.7046019900497513e-08,
      "loss": 2.6214,
      "step": 13340
    },
    {
      "epoch": 66.41791044776119,
      "grad_norm": 9.810046195983887,
      "learning_rate": 1.6983830845771143e-08,
      "loss": 2.6559,
      "step": 13350
    },
    {
      "epoch": 66.46766169154229,
      "grad_norm": 13.18017292022705,
      "learning_rate": 1.6921641791044773e-08,
      "loss": 2.5317,
      "step": 13360
    },
    {
      "epoch": 66.51741293532338,
      "grad_norm": 5.967436790466309,
      "learning_rate": 1.6859452736318406e-08,
      "loss": 2.4594,
      "step": 13370
    },
    {
      "epoch": 66.56716417910448,
      "grad_norm": 10.577871322631836,
      "learning_rate": 1.679726368159204e-08,
      "loss": 2.5978,
      "step": 13380
    },
    {
      "epoch": 66.61691542288557,
      "grad_norm": 6.941184043884277,
      "learning_rate": 1.673507462686567e-08,
      "loss": 2.5136,
      "step": 13390
    },
    {
      "epoch": 66.66666666666667,
      "grad_norm": 8.412035942077637,
      "learning_rate": 1.6672885572139303e-08,
      "loss": 2.6843,
      "step": 13400
    },
    {
      "epoch": 66.71641791044776,
      "grad_norm": 13.322736740112305,
      "learning_rate": 1.6610696517412933e-08,
      "loss": 2.5376,
      "step": 13410
    },
    {
      "epoch": 66.76616915422886,
      "grad_norm": 11.565679550170898,
      "learning_rate": 1.6548507462686566e-08,
      "loss": 2.5263,
      "step": 13420
    },
    {
      "epoch": 66.81592039800995,
      "grad_norm": 6.565865516662598,
      "learning_rate": 1.64863184079602e-08,
      "loss": 2.5308,
      "step": 13430
    },
    {
      "epoch": 66.86567164179104,
      "grad_norm": 9.573969841003418,
      "learning_rate": 1.642412935323383e-08,
      "loss": 2.5057,
      "step": 13440
    },
    {
      "epoch": 66.91542288557214,
      "grad_norm": 6.998996257781982,
      "learning_rate": 1.6361940298507462e-08,
      "loss": 2.6491,
      "step": 13450
    },
    {
      "epoch": 66.96517412935323,
      "grad_norm": 11.908072471618652,
      "learning_rate": 1.6299751243781092e-08,
      "loss": 2.4728,
      "step": 13460
    },
    {
      "epoch": 67.0,
      "eval_loss": 1.7406916618347168,
      "eval_runtime": 54.9713,
      "eval_samples_per_second": 3.656,
      "eval_steps_per_second": 0.236,
      "eval_wer": 0.6151992287917738,
      "step": 13467
    },
    {
      "epoch": 67.01492537313433,
      "grad_norm": 10.757588386535645,
      "learning_rate": 1.6237562189054725e-08,
      "loss": 2.5427,
      "step": 13470
    },
    {
      "epoch": 67.06467661691542,
      "grad_norm": 10.512118339538574,
      "learning_rate": 1.617537313432836e-08,
      "loss": 2.5458,
      "step": 13480
    },
    {
      "epoch": 67.11442786069652,
      "grad_norm": 8.055963516235352,
      "learning_rate": 1.611318407960199e-08,
      "loss": 2.5309,
      "step": 13490
    },
    {
      "epoch": 67.16417910447761,
      "grad_norm": 9.446562767028809,
      "learning_rate": 1.605099502487562e-08,
      "loss": 2.5152,
      "step": 13500
    },
    {
      "epoch": 67.2139303482587,
      "grad_norm": 10.214380264282227,
      "learning_rate": 1.598880597014925e-08,
      "loss": 2.4765,
      "step": 13510
    },
    {
      "epoch": 67.2636815920398,
      "grad_norm": 8.427699089050293,
      "learning_rate": 1.5926616915422885e-08,
      "loss": 2.629,
      "step": 13520
    },
    {
      "epoch": 67.31343283582089,
      "grad_norm": 13.367751121520996,
      "learning_rate": 1.5864427860696518e-08,
      "loss": 2.5924,
      "step": 13530
    },
    {
      "epoch": 67.363184079602,
      "grad_norm": 9.018622398376465,
      "learning_rate": 1.5802238805970148e-08,
      "loss": 2.6349,
      "step": 13540
    },
    {
      "epoch": 67.41293532338308,
      "grad_norm": 8.349079132080078,
      "learning_rate": 1.574004975124378e-08,
      "loss": 2.532,
      "step": 13550
    },
    {
      "epoch": 67.46268656716418,
      "grad_norm": 8.290576934814453,
      "learning_rate": 1.567786069651741e-08,
      "loss": 2.619,
      "step": 13560
    },
    {
      "epoch": 67.51243781094527,
      "grad_norm": 10.096277236938477,
      "learning_rate": 1.5615671641791044e-08,
      "loss": 2.6057,
      "step": 13570
    },
    {
      "epoch": 67.56218905472637,
      "grad_norm": 9.768614768981934,
      "learning_rate": 1.5553482587064677e-08,
      "loss": 2.5259,
      "step": 13580
    },
    {
      "epoch": 67.61194029850746,
      "grad_norm": 16.763952255249023,
      "learning_rate": 1.5491293532338307e-08,
      "loss": 2.6223,
      "step": 13590
    },
    {
      "epoch": 67.66169154228855,
      "grad_norm": 5.84976863861084,
      "learning_rate": 1.542910447761194e-08,
      "loss": 2.5581,
      "step": 13600
    },
    {
      "epoch": 67.71144278606965,
      "grad_norm": 10.306370735168457,
      "learning_rate": 1.536691542288557e-08,
      "loss": 2.5161,
      "step": 13610
    },
    {
      "epoch": 67.76119402985074,
      "grad_norm": 11.754392623901367,
      "learning_rate": 1.5304726368159204e-08,
      "loss": 2.5212,
      "step": 13620
    },
    {
      "epoch": 67.81094527363184,
      "grad_norm": 5.4289164543151855,
      "learning_rate": 1.5242537313432833e-08,
      "loss": 2.6241,
      "step": 13630
    },
    {
      "epoch": 67.86069651741293,
      "grad_norm": 9.68454360961914,
      "learning_rate": 1.5180348258706467e-08,
      "loss": 2.5589,
      "step": 13640
    },
    {
      "epoch": 67.91044776119404,
      "grad_norm": 13.947064399719238,
      "learning_rate": 1.51181592039801e-08,
      "loss": 2.5228,
      "step": 13650
    },
    {
      "epoch": 67.96019900497512,
      "grad_norm": 14.983333587646484,
      "learning_rate": 1.505597014925373e-08,
      "loss": 2.5151,
      "step": 13660
    },
    {
      "epoch": 68.0,
      "eval_loss": 1.7409394979476929,
      "eval_runtime": 54.9263,
      "eval_samples_per_second": 3.659,
      "eval_steps_per_second": 0.237,
      "eval_wer": 0.6151992287917738,
      "step": 13668
    },
    {
      "epoch": 68.00995024875621,
      "grad_norm": 7.30391263961792,
      "learning_rate": 1.4993781094527363e-08,
      "loss": 2.6109,
      "step": 13670
    },
    {
      "epoch": 68.05970149253731,
      "grad_norm": 13.92562198638916,
      "learning_rate": 1.4931592039800993e-08,
      "loss": 2.577,
      "step": 13680
    },
    {
      "epoch": 68.1094527363184,
      "grad_norm": 9.011327743530273,
      "learning_rate": 1.4869402985074624e-08,
      "loss": 2.4483,
      "step": 13690
    },
    {
      "epoch": 68.1592039800995,
      "grad_norm": 8.188272476196289,
      "learning_rate": 1.480721393034826e-08,
      "loss": 2.5483,
      "step": 13700
    },
    {
      "epoch": 68.2089552238806,
      "grad_norm": 6.463874340057373,
      "learning_rate": 1.474502487562189e-08,
      "loss": 2.5441,
      "step": 13710
    },
    {
      "epoch": 68.2587064676617,
      "grad_norm": 5.949731826782227,
      "learning_rate": 1.4682835820895522e-08,
      "loss": 2.5594,
      "step": 13720
    },
    {
      "epoch": 68.30845771144278,
      "grad_norm": 11.53824234008789,
      "learning_rate": 1.4620646766169154e-08,
      "loss": 2.586,
      "step": 13730
    },
    {
      "epoch": 68.35820895522389,
      "grad_norm": 9.681018829345703,
      "learning_rate": 1.4558457711442784e-08,
      "loss": 2.6287,
      "step": 13740
    },
    {
      "epoch": 68.40796019900498,
      "grad_norm": 5.467864513397217,
      "learning_rate": 1.4496268656716417e-08,
      "loss": 2.3949,
      "step": 13750
    },
    {
      "epoch": 68.45771144278606,
      "grad_norm": 8.050409317016602,
      "learning_rate": 1.4434079601990049e-08,
      "loss": 2.5789,
      "step": 13760
    },
    {
      "epoch": 68.50746268656717,
      "grad_norm": 10.135811805725098,
      "learning_rate": 1.4371890547263682e-08,
      "loss": 2.5337,
      "step": 13770
    },
    {
      "epoch": 68.55721393034825,
      "grad_norm": 10.170740127563477,
      "learning_rate": 1.4309701492537313e-08,
      "loss": 2.6496,
      "step": 13780
    },
    {
      "epoch": 68.60696517412936,
      "grad_norm": 26.366044998168945,
      "learning_rate": 1.4247512437810943e-08,
      "loss": 2.5551,
      "step": 13790
    },
    {
      "epoch": 68.65671641791045,
      "grad_norm": 9.468135833740234,
      "learning_rate": 1.4185323383084577e-08,
      "loss": 2.5036,
      "step": 13800
    },
    {
      "epoch": 68.70646766169155,
      "grad_norm": 32.40653610229492,
      "learning_rate": 1.4123134328358208e-08,
      "loss": 2.6069,
      "step": 13810
    },
    {
      "epoch": 68.75621890547264,
      "grad_norm": 25.42793846130371,
      "learning_rate": 1.4060945273631841e-08,
      "loss": 2.54,
      "step": 13820
    },
    {
      "epoch": 68.80597014925372,
      "grad_norm": 6.925323486328125,
      "learning_rate": 1.3998756218905471e-08,
      "loss": 2.6093,
      "step": 13830
    },
    {
      "epoch": 68.85572139303483,
      "grad_norm": 10.612593650817871,
      "learning_rate": 1.3936567164179103e-08,
      "loss": 2.695,
      "step": 13840
    },
    {
      "epoch": 68.90547263681592,
      "grad_norm": 9.576706886291504,
      "learning_rate": 1.3874378109452736e-08,
      "loss": 2.5984,
      "step": 13850
    },
    {
      "epoch": 68.95522388059702,
      "grad_norm": 9.606842994689941,
      "learning_rate": 1.3812189054726367e-08,
      "loss": 2.6277,
      "step": 13860
    },
    {
      "epoch": 69.0,
      "eval_loss": 1.7395751476287842,
      "eval_runtime": 54.663,
      "eval_samples_per_second": 3.677,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6145565552699229,
      "step": 13869
    },
    {
      "epoch": 69.0049751243781,
      "grad_norm": 13.332077980041504,
      "learning_rate": 1.375e-08,
      "loss": 2.5126,
      "step": 13870
    },
    {
      "epoch": 69.05472636815921,
      "grad_norm": 15.91581916809082,
      "learning_rate": 1.368781094527363e-08,
      "loss": 2.5753,
      "step": 13880
    },
    {
      "epoch": 69.1044776119403,
      "grad_norm": 8.394647598266602,
      "learning_rate": 1.3625621890547262e-08,
      "loss": 2.5015,
      "step": 13890
    },
    {
      "epoch": 69.1542288557214,
      "grad_norm": 13.87378215789795,
      "learning_rate": 1.3563432835820895e-08,
      "loss": 2.6362,
      "step": 13900
    },
    {
      "epoch": 69.20398009950249,
      "grad_norm": 5.291336536407471,
      "learning_rate": 1.3501243781094527e-08,
      "loss": 2.4634,
      "step": 13910
    },
    {
      "epoch": 69.25373134328358,
      "grad_norm": 7.125029563903809,
      "learning_rate": 1.343905472636816e-08,
      "loss": 2.5464,
      "step": 13920
    },
    {
      "epoch": 69.30348258706468,
      "grad_norm": 8.515119552612305,
      "learning_rate": 1.337686567164179e-08,
      "loss": 2.6479,
      "step": 13930
    },
    {
      "epoch": 69.35323383084577,
      "grad_norm": 18.809791564941406,
      "learning_rate": 1.3314676616915422e-08,
      "loss": 2.5765,
      "step": 13940
    },
    {
      "epoch": 69.40298507462687,
      "grad_norm": 9.486112594604492,
      "learning_rate": 1.3252487562189055e-08,
      "loss": 2.5754,
      "step": 13950
    },
    {
      "epoch": 69.45273631840796,
      "grad_norm": 8.319880485534668,
      "learning_rate": 1.3190298507462685e-08,
      "loss": 2.5867,
      "step": 13960
    },
    {
      "epoch": 69.50248756218906,
      "grad_norm": 16.797121047973633,
      "learning_rate": 1.312810945273632e-08,
      "loss": 2.4993,
      "step": 13970
    },
    {
      "epoch": 69.55223880597015,
      "grad_norm": 9.861861228942871,
      "learning_rate": 1.306592039800995e-08,
      "loss": 2.6045,
      "step": 13980
    },
    {
      "epoch": 69.60199004975124,
      "grad_norm": 11.714680671691895,
      "learning_rate": 1.3003731343283581e-08,
      "loss": 2.5488,
      "step": 13990
    },
    {
      "epoch": 69.65174129353234,
      "grad_norm": 9.665568351745605,
      "learning_rate": 1.2941542288557214e-08,
      "loss": 2.4751,
      "step": 14000
    },
    {
      "epoch": 69.70149253731343,
      "grad_norm": 11.752007484436035,
      "learning_rate": 1.2879353233830844e-08,
      "loss": 2.5375,
      "step": 14010
    },
    {
      "epoch": 69.75124378109453,
      "grad_norm": 8.607183456420898,
      "learning_rate": 1.2817164179104479e-08,
      "loss": 2.5368,
      "step": 14020
    },
    {
      "epoch": 69.80099502487562,
      "grad_norm": 13.305367469787598,
      "learning_rate": 1.2754975124378109e-08,
      "loss": 2.5985,
      "step": 14030
    },
    {
      "epoch": 69.85074626865672,
      "grad_norm": 8.38629150390625,
      "learning_rate": 1.269278606965174e-08,
      "loss": 2.5934,
      "step": 14040
    },
    {
      "epoch": 69.90049751243781,
      "grad_norm": 7.372270107269287,
      "learning_rate": 1.2630597014925374e-08,
      "loss": 2.7104,
      "step": 14050
    },
    {
      "epoch": 69.95024875621891,
      "grad_norm": 9.173103332519531,
      "learning_rate": 1.2568407960199004e-08,
      "loss": 2.5426,
      "step": 14060
    },
    {
      "epoch": 70.0,
      "grad_norm": 37.0217399597168,
      "learning_rate": 1.2506218905472637e-08,
      "loss": 2.5574,
      "step": 14070
    },
    {
      "epoch": 70.0,
      "eval_loss": 1.7401745319366455,
      "eval_runtime": 55.248,
      "eval_samples_per_second": 3.638,
      "eval_steps_per_second": 0.235,
      "eval_wer": 0.6142352185089974,
      "step": 14070
    },
    {
      "epoch": 70.04975124378109,
      "grad_norm": 12.378738403320312,
      "learning_rate": 1.2444029850746268e-08,
      "loss": 2.5267,
      "step": 14080
    },
    {
      "epoch": 70.09950248756219,
      "grad_norm": 11.141202926635742,
      "learning_rate": 1.23818407960199e-08,
      "loss": 2.5378,
      "step": 14090
    },
    {
      "epoch": 70.14925373134328,
      "grad_norm": 16.540374755859375,
      "learning_rate": 1.2319651741293533e-08,
      "loss": 2.5064,
      "step": 14100
    },
    {
      "epoch": 70.19900497512438,
      "grad_norm": 18.693967819213867,
      "learning_rate": 1.2257462686567163e-08,
      "loss": 2.6372,
      "step": 14110
    },
    {
      "epoch": 70.24875621890547,
      "grad_norm": 6.880332946777344,
      "learning_rate": 1.2195273631840795e-08,
      "loss": 2.4502,
      "step": 14120
    },
    {
      "epoch": 70.29850746268657,
      "grad_norm": 11.50917911529541,
      "learning_rate": 1.2133084577114428e-08,
      "loss": 2.6186,
      "step": 14130
    },
    {
      "epoch": 70.34825870646766,
      "grad_norm": 7.34707498550415,
      "learning_rate": 1.207089552238806e-08,
      "loss": 2.5882,
      "step": 14140
    },
    {
      "epoch": 70.39800995024876,
      "grad_norm": 8.658184051513672,
      "learning_rate": 1.2008706467661691e-08,
      "loss": 2.5781,
      "step": 14150
    },
    {
      "epoch": 70.44776119402985,
      "grad_norm": 7.70871639251709,
      "learning_rate": 1.1946517412935322e-08,
      "loss": 2.5841,
      "step": 14160
    },
    {
      "epoch": 70.49751243781094,
      "grad_norm": 6.072415828704834,
      "learning_rate": 1.1884328358208954e-08,
      "loss": 2.5718,
      "step": 14170
    },
    {
      "epoch": 70.54726368159204,
      "grad_norm": 9.727291107177734,
      "learning_rate": 1.1822139303482587e-08,
      "loss": 2.6119,
      "step": 14180
    },
    {
      "epoch": 70.59701492537313,
      "grad_norm": 17.09806251525879,
      "learning_rate": 1.1759950248756219e-08,
      "loss": 2.5276,
      "step": 14190
    },
    {
      "epoch": 70.64676616915423,
      "grad_norm": 7.614607334136963,
      "learning_rate": 1.169776119402985e-08,
      "loss": 2.5659,
      "step": 14200
    },
    {
      "epoch": 70.69651741293532,
      "grad_norm": 11.782506942749023,
      "learning_rate": 1.1635572139303482e-08,
      "loss": 2.6113,
      "step": 14210
    },
    {
      "epoch": 70.74626865671642,
      "grad_norm": 7.2796525955200195,
      "learning_rate": 1.1573383084577113e-08,
      "loss": 2.679,
      "step": 14220
    },
    {
      "epoch": 70.79601990049751,
      "grad_norm": 6.0928874015808105,
      "learning_rate": 1.1511194029850747e-08,
      "loss": 2.5291,
      "step": 14230
    },
    {
      "epoch": 70.8457711442786,
      "grad_norm": 11.400750160217285,
      "learning_rate": 1.1449004975124378e-08,
      "loss": 2.5246,
      "step": 14240
    },
    {
      "epoch": 70.8955223880597,
      "grad_norm": 8.87453842163086,
      "learning_rate": 1.1386815920398008e-08,
      "loss": 2.5362,
      "step": 14250
    },
    {
      "epoch": 70.94527363184079,
      "grad_norm": 19.92848777770996,
      "learning_rate": 1.1324626865671641e-08,
      "loss": 2.6468,
      "step": 14260
    },
    {
      "epoch": 70.9950248756219,
      "grad_norm": 32.75180435180664,
      "learning_rate": 1.1262437810945273e-08,
      "loss": 2.6993,
      "step": 14270
    },
    {
      "epoch": 71.0,
      "eval_loss": 1.739559531211853,
      "eval_runtime": 55.9336,
      "eval_samples_per_second": 3.594,
      "eval_steps_per_second": 0.232,
      "eval_wer": 0.6140745501285347,
      "step": 14271
    },
    {
      "epoch": 71.04477611940298,
      "grad_norm": 10.419812202453613,
      "learning_rate": 1.1200248756218904e-08,
      "loss": 2.503,
      "step": 14280
    },
    {
      "epoch": 71.09452736318408,
      "grad_norm": 9.835759162902832,
      "learning_rate": 1.1138059701492538e-08,
      "loss": 2.5285,
      "step": 14290
    },
    {
      "epoch": 71.14427860696517,
      "grad_norm": 4.598819732666016,
      "learning_rate": 1.1075870646766168e-08,
      "loss": 2.6559,
      "step": 14300
    },
    {
      "epoch": 71.19402985074628,
      "grad_norm": 6.195279121398926,
      "learning_rate": 1.10136815920398e-08,
      "loss": 2.5603,
      "step": 14310
    },
    {
      "epoch": 71.24378109452736,
      "grad_norm": 8.81418228149414,
      "learning_rate": 1.0951492537313432e-08,
      "loss": 2.528,
      "step": 14320
    },
    {
      "epoch": 71.29353233830845,
      "grad_norm": 9.251811027526855,
      "learning_rate": 1.0889303482587064e-08,
      "loss": 2.6964,
      "step": 14330
    },
    {
      "epoch": 71.34328358208955,
      "grad_norm": 10.44333553314209,
      "learning_rate": 1.0827114427860697e-08,
      "loss": 2.5675,
      "step": 14340
    },
    {
      "epoch": 71.39303482587064,
      "grad_norm": 18.723167419433594,
      "learning_rate": 1.0764925373134327e-08,
      "loss": 2.6047,
      "step": 14350
    },
    {
      "epoch": 71.44278606965175,
      "grad_norm": 6.919525146484375,
      "learning_rate": 1.070273631840796e-08,
      "loss": 2.5655,
      "step": 14360
    },
    {
      "epoch": 71.49253731343283,
      "grad_norm": 7.407499313354492,
      "learning_rate": 1.0640547263681592e-08,
      "loss": 2.5472,
      "step": 14370
    },
    {
      "epoch": 71.54228855721394,
      "grad_norm": 28.923593521118164,
      "learning_rate": 1.0578358208955223e-08,
      "loss": 2.4894,
      "step": 14380
    },
    {
      "epoch": 71.59203980099502,
      "grad_norm": 11.64416790008545,
      "learning_rate": 1.0516169154228856e-08,
      "loss": 2.6087,
      "step": 14390
    },
    {
      "epoch": 71.64179104477611,
      "grad_norm": 8.705493927001953,
      "learning_rate": 1.0453980099502486e-08,
      "loss": 2.5289,
      "step": 14400
    },
    {
      "epoch": 71.69154228855722,
      "grad_norm": 5.689556121826172,
      "learning_rate": 1.0391791044776118e-08,
      "loss": 2.5115,
      "step": 14410
    },
    {
      "epoch": 71.7412935323383,
      "grad_norm": 10.475946426391602,
      "learning_rate": 1.0329601990049751e-08,
      "loss": 2.6632,
      "step": 14420
    },
    {
      "epoch": 71.7910447761194,
      "grad_norm": 10.263591766357422,
      "learning_rate": 1.0267412935323383e-08,
      "loss": 2.6082,
      "step": 14430
    },
    {
      "epoch": 71.8407960199005,
      "grad_norm": 14.977537155151367,
      "learning_rate": 1.0205223880597014e-08,
      "loss": 2.579,
      "step": 14440
    },
    {
      "epoch": 71.8905472636816,
      "grad_norm": 6.177733421325684,
      "learning_rate": 1.0143034825870646e-08,
      "loss": 2.5469,
      "step": 14450
    },
    {
      "epoch": 71.94029850746269,
      "grad_norm": 12.785229682922363,
      "learning_rate": 1.0080845771144277e-08,
      "loss": 2.4309,
      "step": 14460
    },
    {
      "epoch": 71.99004975124379,
      "grad_norm": 10.459708213806152,
      "learning_rate": 1.001865671641791e-08,
      "loss": 2.6581,
      "step": 14470
    },
    {
      "epoch": 72.0,
      "eval_loss": 1.737435221672058,
      "eval_runtime": 54.7265,
      "eval_samples_per_second": 3.673,
      "eval_steps_per_second": 0.238,
      "eval_wer": 0.6136728791773779,
      "step": 14472
    },
    {
      "epoch": 72.03980099502488,
      "grad_norm": 14.806185722351074,
      "learning_rate": 9.956467661691542e-09,
      "loss": 2.5915,
      "step": 14480
    },
    {
      "epoch": 72.08955223880596,
      "grad_norm": 8.43320369720459,
      "learning_rate": 9.894278606965174e-09,
      "loss": 2.4567,
      "step": 14490
    },
    {
      "epoch": 72.13930348258707,
      "grad_norm": 9.135769844055176,
      "learning_rate": 9.832089552238805e-09,
      "loss": 2.6051,
      "step": 14500
    },
    {
      "epoch": 72.18905472636816,
      "grad_norm": 11.830820083618164,
      "learning_rate": 9.769900497512437e-09,
      "loss": 2.6597,
      "step": 14510
    },
    {
      "epoch": 72.23880597014926,
      "grad_norm": 11.759239196777344,
      "learning_rate": 9.70771144278607e-09,
      "loss": 2.602,
      "step": 14520
    },
    {
      "epoch": 72.28855721393035,
      "grad_norm": 14.436872482299805,
      "learning_rate": 9.645522388059702e-09,
      "loss": 2.5257,
      "step": 14530
    },
    {
      "epoch": 72.33830845771145,
      "grad_norm": 16.087528228759766,
      "learning_rate": 9.583333333333333e-09,
      "loss": 2.5246,
      "step": 14540
    },
    {
      "epoch": 72.38805970149254,
      "grad_norm": 11.028712272644043,
      "learning_rate": 9.521144278606965e-09,
      "loss": 2.5553,
      "step": 14550
    },
    {
      "epoch": 72.43781094527363,
      "grad_norm": 6.939203262329102,
      "learning_rate": 9.458955223880596e-09,
      "loss": 2.6146,
      "step": 14560
    },
    {
      "epoch": 72.48756218905473,
      "grad_norm": 9.49596881866455,
      "learning_rate": 9.396766169154228e-09,
      "loss": 2.5718,
      "step": 14570
    },
    {
      "epoch": 72.53731343283582,
      "grad_norm": 11.613102912902832,
      "learning_rate": 9.334577114427861e-09,
      "loss": 2.5133,
      "step": 14580
    },
    {
      "epoch": 72.58706467661692,
      "grad_norm": 6.702709674835205,
      "learning_rate": 9.272388059701493e-09,
      "loss": 2.6765,
      "step": 14590
    },
    {
      "epoch": 72.636815920398,
      "grad_norm": 8.741586685180664,
      "learning_rate": 9.210199004975124e-09,
      "loss": 2.6263,
      "step": 14600
    },
    {
      "epoch": 72.68656716417911,
      "grad_norm": 13.361675262451172,
      "learning_rate": 9.148009950248756e-09,
      "loss": 2.5258,
      "step": 14610
    },
    {
      "epoch": 72.7363184079602,
      "grad_norm": 6.564762592315674,
      "learning_rate": 9.085820895522387e-09,
      "loss": 2.5973,
      "step": 14620
    },
    {
      "epoch": 72.7860696517413,
      "grad_norm": 9.35444450378418,
      "learning_rate": 9.02363184079602e-09,
      "loss": 2.6,
      "step": 14630
    },
    {
      "epoch": 72.83582089552239,
      "grad_norm": 16.028287887573242,
      "learning_rate": 8.96144278606965e-09,
      "loss": 2.5228,
      "step": 14640
    },
    {
      "epoch": 72.88557213930348,
      "grad_norm": 8.40113353729248,
      "learning_rate": 8.899253731343284e-09,
      "loss": 2.5826,
      "step": 14650
    },
    {
      "epoch": 72.93532338308458,
      "grad_norm": 10.655759811401367,
      "learning_rate": 8.837064676616915e-09,
      "loss": 2.572,
      "step": 14660
    },
    {
      "epoch": 72.98507462686567,
      "grad_norm": 10.79667854309082,
      "learning_rate": 8.774875621890547e-09,
      "loss": 2.604,
      "step": 14670
    },
    {
      "epoch": 73.0,
      "eval_loss": 1.7419610023498535,
      "eval_runtime": 55.0499,
      "eval_samples_per_second": 3.651,
      "eval_steps_per_second": 0.236,
      "eval_wer": 0.6136728791773779,
      "step": 14673
    },
    {
      "epoch": 73.03482587064677,
      "grad_norm": 6.929107189178467,
      "learning_rate": 8.71268656716418e-09,
      "loss": 2.6816,
      "step": 14680
    },
    {
      "epoch": 73.08457711442786,
      "grad_norm": 9.656874656677246,
      "learning_rate": 8.65049751243781e-09,
      "loss": 2.5999,
      "step": 14690
    },
    {
      "epoch": 73.13432835820896,
      "grad_norm": 7.445315361022949,
      "learning_rate": 8.588308457711441e-09,
      "loss": 2.4596,
      "step": 14700
    },
    {
      "epoch": 73.18407960199005,
      "grad_norm": 8.528217315673828,
      "learning_rate": 8.526119402985075e-09,
      "loss": 2.4553,
      "step": 14710
    },
    {
      "epoch": 73.23383084577114,
      "grad_norm": 16.958526611328125,
      "learning_rate": 8.463930348258706e-09,
      "loss": 2.6244,
      "step": 14720
    },
    {
      "epoch": 73.28358208955224,
      "grad_norm": 6.130399703979492,
      "learning_rate": 8.401741293532338e-09,
      "loss": 2.5728,
      "step": 14730
    },
    {
      "epoch": 73.33333333333333,
      "grad_norm": 9.969183921813965,
      "learning_rate": 8.33955223880597e-09,
      "loss": 2.5284,
      "step": 14740
    },
    {
      "epoch": 73.38308457711443,
      "grad_norm": 9.083782196044922,
      "learning_rate": 8.2773631840796e-09,
      "loss": 2.5597,
      "step": 14750
    },
    {
      "epoch": 73.43283582089552,
      "grad_norm": 11.892850875854492,
      "learning_rate": 8.215174129353234e-09,
      "loss": 2.6335,
      "step": 14760
    },
    {
      "epoch": 73.48258706467662,
      "grad_norm": 8.07524585723877,
      "learning_rate": 8.152985074626866e-09,
      "loss": 2.5446,
      "step": 14770
    },
    {
      "epoch": 73.53233830845771,
      "grad_norm": 19.038414001464844,
      "learning_rate": 8.090796019900497e-09,
      "loss": 2.5538,
      "step": 14780
    },
    {
      "epoch": 73.58208955223881,
      "grad_norm": 7.7516584396362305,
      "learning_rate": 8.028606965174129e-09,
      "loss": 2.5593,
      "step": 14790
    },
    {
      "epoch": 73.6318407960199,
      "grad_norm": 39.36531448364258,
      "learning_rate": 7.96641791044776e-09,
      "loss": 2.5931,
      "step": 14800
    },
    {
      "epoch": 73.68159203980099,
      "grad_norm": 7.516993522644043,
      "learning_rate": 7.904228855721393e-09,
      "loss": 2.4994,
      "step": 14810
    },
    {
      "epoch": 73.73134328358209,
      "grad_norm": 9.355527877807617,
      "learning_rate": 7.842039800995025e-09,
      "loss": 2.5712,
      "step": 14820
    },
    {
      "epoch": 73.78109452736318,
      "grad_norm": 13.099994659423828,
      "learning_rate": 7.779850746268657e-09,
      "loss": 2.6208,
      "step": 14830
    },
    {
      "epoch": 73.83084577114428,
      "grad_norm": 7.985240936279297,
      "learning_rate": 7.717661691542288e-09,
      "loss": 2.7137,
      "step": 14840
    },
    {
      "epoch": 73.88059701492537,
      "grad_norm": 10.240762710571289,
      "learning_rate": 7.65547263681592e-09,
      "loss": 2.6802,
      "step": 14850
    },
    {
      "epoch": 73.93034825870647,
      "grad_norm": 7.332944869995117,
      "learning_rate": 7.593283582089551e-09,
      "loss": 2.5632,
      "step": 14860
    },
    {
      "epoch": 73.98009950248756,
      "grad_norm": 7.122368812561035,
      "learning_rate": 7.531094527363184e-09,
      "loss": 2.5064,
      "step": 14870
    },
    {
      "epoch": 74.0,
      "eval_loss": 1.7404417991638184,
      "eval_runtime": 55.2884,
      "eval_samples_per_second": 3.635,
      "eval_steps_per_second": 0.235,
      "eval_wer": 0.6134318766066839,
      "step": 14874
    },
    {
      "epoch": 74.02985074626865,
      "grad_norm": 13.066670417785645,
      "learning_rate": 7.468905472636816e-09,
      "loss": 2.4791,
      "step": 14880
    },
    {
      "epoch": 74.07960199004975,
      "grad_norm": 9.552101135253906,
      "learning_rate": 7.406716417910447e-09,
      "loss": 2.5641,
      "step": 14890
    },
    {
      "epoch": 74.12935323383084,
      "grad_norm": 11.660626411437988,
      "learning_rate": 7.344527363184079e-09,
      "loss": 2.5067,
      "step": 14900
    },
    {
      "epoch": 74.17910447761194,
      "grad_norm": 5.803769111633301,
      "learning_rate": 7.2823383084577114e-09,
      "loss": 2.4776,
      "step": 14910
    },
    {
      "epoch": 74.22885572139303,
      "grad_norm": 9.190707206726074,
      "learning_rate": 7.220149253731343e-09,
      "loss": 2.6106,
      "step": 14920
    },
    {
      "epoch": 74.27860696517413,
      "grad_norm": 12.540715217590332,
      "learning_rate": 7.157960199004975e-09,
      "loss": 2.6301,
      "step": 14930
    },
    {
      "epoch": 74.32835820895522,
      "grad_norm": 12.136575698852539,
      "learning_rate": 7.095771144278606e-09,
      "loss": 2.5866,
      "step": 14940
    },
    {
      "epoch": 74.37810945273633,
      "grad_norm": 11.850789070129395,
      "learning_rate": 7.0335820895522385e-09,
      "loss": 2.5663,
      "step": 14950
    },
    {
      "epoch": 74.42786069651741,
      "grad_norm": 11.95090103149414,
      "learning_rate": 6.97139303482587e-09,
      "loss": 2.587,
      "step": 14960
    },
    {
      "epoch": 74.4776119402985,
      "grad_norm": 12.766063690185547,
      "learning_rate": 6.9092039800995024e-09,
      "loss": 2.5805,
      "step": 14970
    },
    {
      "epoch": 74.5273631840796,
      "grad_norm": 7.3379411697387695,
      "learning_rate": 6.847014925373135e-09,
      "loss": 2.5825,
      "step": 14980
    },
    {
      "epoch": 74.57711442786069,
      "grad_norm": 8.643304824829102,
      "learning_rate": 6.7848258706467655e-09,
      "loss": 2.5235,
      "step": 14990
    },
    {
      "epoch": 74.6268656716418,
      "grad_norm": 8.473143577575684,
      "learning_rate": 6.722636815920398e-09,
      "loss": 2.4882,
      "step": 15000
    },
    {
      "epoch": 74.67661691542288,
      "grad_norm": 25.491809844970703,
      "learning_rate": 6.6604477611940295e-09,
      "loss": 2.5377,
      "step": 15010
    },
    {
      "epoch": 74.72636815920399,
      "grad_norm": 32.12715530395508,
      "learning_rate": 6.598258706467662e-09,
      "loss": 2.6332,
      "step": 15020
    },
    {
      "epoch": 74.77611940298507,
      "grad_norm": 14.962933540344238,
      "learning_rate": 6.536069651741294e-09,
      "loss": 2.4629,
      "step": 15030
    },
    {
      "epoch": 74.82587064676616,
      "grad_norm": 9.430265426635742,
      "learning_rate": 6.473880597014925e-09,
      "loss": 2.6308,
      "step": 15040
    },
    {
      "epoch": 74.87562189054727,
      "grad_norm": 6.576960563659668,
      "learning_rate": 6.4116915422885565e-09,
      "loss": 2.5713,
      "step": 15050
    },
    {
      "epoch": 74.92537313432835,
      "grad_norm": 8.850163459777832,
      "learning_rate": 6.349502487562189e-09,
      "loss": 2.5429,
      "step": 15060
    },
    {
      "epoch": 74.97512437810946,
      "grad_norm": 9.619879722595215,
      "learning_rate": 6.287313432835821e-09,
      "loss": 2.653,
      "step": 15070
    },
    {
      "epoch": 75.0,
      "eval_loss": 1.738396167755127,
      "eval_runtime": 54.9981,
      "eval_samples_per_second": 3.655,
      "eval_steps_per_second": 0.236,
      "eval_wer": 0.6133515424164524,
      "step": 15075
    },
    {
      "epoch": 75.02487562189054,
      "grad_norm": 6.729069232940674,
      "learning_rate": 6.225124378109453e-09,
      "loss": 2.5841,
      "step": 15080
    },
    {
      "epoch": 75.07462686567165,
      "grad_norm": 7.031573295593262,
      "learning_rate": 6.162935323383084e-09,
      "loss": 2.5336,
      "step": 15090
    },
    {
      "epoch": 75.12437810945273,
      "grad_norm": 7.628549575805664,
      "learning_rate": 6.100746268656716e-09,
      "loss": 2.6047,
      "step": 15100
    },
    {
      "epoch": 75.17412935323384,
      "grad_norm": 10.497118949890137,
      "learning_rate": 6.038557213930348e-09,
      "loss": 2.5886,
      "step": 15110
    },
    {
      "epoch": 75.22388059701493,
      "grad_norm": 9.966863632202148,
      "learning_rate": 5.97636815920398e-09,
      "loss": 2.5766,
      "step": 15120
    },
    {
      "epoch": 75.27363184079601,
      "grad_norm": 10.412978172302246,
      "learning_rate": 5.9141791044776114e-09,
      "loss": 2.5326,
      "step": 15130
    },
    {
      "epoch": 75.32338308457712,
      "grad_norm": 22.785381317138672,
      "learning_rate": 5.851990049751244e-09,
      "loss": 2.4857,
      "step": 15140
    },
    {
      "epoch": 75.3731343283582,
      "grad_norm": 8.023524284362793,
      "learning_rate": 5.789800995024875e-09,
      "loss": 2.6133,
      "step": 15150
    },
    {
      "epoch": 75.42288557213931,
      "grad_norm": 9.6243257522583,
      "learning_rate": 5.727611940298508e-09,
      "loss": 2.5069,
      "step": 15160
    },
    {
      "epoch": 75.4726368159204,
      "grad_norm": 20.41996955871582,
      "learning_rate": 5.6654228855721385e-09,
      "loss": 2.5361,
      "step": 15170
    },
    {
      "epoch": 75.5223880597015,
      "grad_norm": 9.456984519958496,
      "learning_rate": 5.603233830845771e-09,
      "loss": 2.5731,
      "step": 15180
    },
    {
      "epoch": 75.57213930348259,
      "grad_norm": 9.658447265625,
      "learning_rate": 5.541044776119403e-09,
      "loss": 2.5959,
      "step": 15190
    },
    {
      "epoch": 75.62189054726367,
      "grad_norm": 8.306246757507324,
      "learning_rate": 5.478855721393035e-09,
      "loss": 2.5371,
      "step": 15200
    },
    {
      "epoch": 75.67164179104478,
      "grad_norm": 10.628049850463867,
      "learning_rate": 5.416666666666666e-09,
      "loss": 2.5815,
      "step": 15210
    },
    {
      "epoch": 75.72139303482587,
      "grad_norm": 9.090507507324219,
      "learning_rate": 5.354477611940298e-09,
      "loss": 2.6159,
      "step": 15220
    },
    {
      "epoch": 75.77114427860697,
      "grad_norm": 15.367307662963867,
      "learning_rate": 5.29228855721393e-09,
      "loss": 2.6555,
      "step": 15230
    },
    {
      "epoch": 75.82089552238806,
      "grad_norm": 11.12948989868164,
      "learning_rate": 5.230099502487563e-09,
      "loss": 2.6663,
      "step": 15240
    },
    {
      "epoch": 75.87064676616916,
      "grad_norm": 9.389400482177734,
      "learning_rate": 5.1679104477611934e-09,
      "loss": 2.4859,
      "step": 15250
    },
    {
      "epoch": 75.92039800995025,
      "grad_norm": 9.823906898498535,
      "learning_rate": 5.105721393034826e-09,
      "loss": 2.6008,
      "step": 15260
    },
    {
      "epoch": 75.97014925373135,
      "grad_norm": 6.9838690757751465,
      "learning_rate": 5.043532338308457e-09,
      "loss": 2.4806,
      "step": 15270
    },
    {
      "epoch": 76.0,
      "eval_loss": 1.7371054887771606,
      "eval_runtime": 55.239,
      "eval_samples_per_second": 3.639,
      "eval_steps_per_second": 0.235,
      "eval_wer": 0.6133515424164524,
      "step": 15276
    }
  ],
  "logging_steps": 10,
  "max_steps": 16080,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 80,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 10,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.65704961503232e+19,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
