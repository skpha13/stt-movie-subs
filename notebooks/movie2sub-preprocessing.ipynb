{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d2f06c",
   "metadata": {
    "papermill": {
     "duration": 0.002436,
     "end_time": "2025-05-07T07:45:00.279870",
     "exception": false,
     "start_time": "2025-05-07T07:45:00.277434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Audio Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b314d7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T07:45:00.285720Z",
     "iopub.status.busy": "2025-05-07T07:45:00.285306Z",
     "iopub.status.idle": "2025-05-07T07:45:06.978410Z",
     "shell.execute_reply": "2025-05-07T07:45:06.977058Z"
    },
    "papermill": {
     "duration": 6.698114,
     "end_time": "2025-05-07T07:45:06.980178",
     "exception": false,
     "start_time": "2025-05-07T07:45:00.282064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "class AudioProcessor:\n",
    "    SAMPLE_RATE = 16_000\n",
    "    N_MELS = 64\n",
    "    WINDOW_SIZE = 0.02  # 20 ms\n",
    "    HOP_LENGTH = 0.01  # 10 ms\n",
    "    N_FFT = 512\n",
    "    EPSILON = 1e-6\n",
    "    MIN_FREQUENCY = 85\n",
    "    MAX_FREQUENCY = 3000\n",
    "\n",
    "    # wiener filter parameters\n",
    "    WIENER_N_FFT = 512\n",
    "    WIENER_HOP_LENGTH = 128\n",
    "    WIENER_WIN_LENGTH = 512\n",
    "    NOISE_FRAME_COUNT = 5  # first 5 frames for noise estimation\n",
    "\n",
    "    mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=N_FFT,\n",
    "        win_length=int(WINDOW_SIZE * SAMPLE_RATE),\n",
    "        hop_length=int(HOP_LENGTH * SAMPLE_RATE),\n",
    "        n_mels=N_MELS,\n",
    "        center=True,\n",
    "        power=2.0,\n",
    "        f_min=MIN_FREQUENCY,\n",
    "        f_max=MAX_FREQUENCY,\n",
    "    )\n",
    "    log_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "    @staticmethod\n",
    "    def wiener_filter(waveform: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Applies Wiener filtering for noise reduction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            waveform: Input audio tensor (1, T)\n",
    "        Returns\n",
    "        -------\n",
    "            Denoised waveform (1, T)\n",
    "        \"\"\"\n",
    "        stft = torch.stft(\n",
    "            waveform.squeeze(0),\n",
    "            n_fft=AudioProcessor.WIENER_N_FFT,\n",
    "            hop_length=AudioProcessor.WIENER_HOP_LENGTH,\n",
    "            win_length=AudioProcessor.WIENER_WIN_LENGTH,\n",
    "            window=torch.hann_window(AudioProcessor.WIENER_WIN_LENGTH).to(waveform.device),\n",
    "            return_complex=True,\n",
    "        )\n",
    "\n",
    "        # estimate noise from first few frames\n",
    "        magnitude = torch.abs(stft)\n",
    "        noise_estimate = magnitude[:, : AudioProcessor.NOISE_FRAME_COUNT].mean(dim=1, keepdim=True)\n",
    "\n",
    "        # wiener gain\n",
    "        gain = (magnitude - noise_estimate).clamp(min=0) / (magnitude + AudioProcessor.EPSILON)\n",
    "\n",
    "        # reconstruct waveform\n",
    "        enhanced_stft = stft * gain\n",
    "        enhanced_waveform = torch.istft(\n",
    "            enhanced_stft,\n",
    "            n_fft=AudioProcessor.WIENER_N_FFT,\n",
    "            hop_length=AudioProcessor.WIENER_HOP_LENGTH,\n",
    "            win_length=AudioProcessor.WIENER_WIN_LENGTH,\n",
    "            window=torch.hann_window(AudioProcessor.WIENER_WIN_LENGTH).to(waveform.device),\n",
    "        )\n",
    "\n",
    "        return enhanced_waveform.unsqueeze(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(waveform: torch.Tensor, original_sample_rate: int, apply_wiener: bool = True) -> torch.Tensor:\n",
    "        # convert to mono\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # normalize waveform to [-1, 1]\n",
    "        waveform = waveform / waveform.abs().max()\n",
    "\n",
    "        # resample to 16kHz if needed\n",
    "        if original_sample_rate != AudioProcessor.SAMPLE_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                orig_freq=original_sample_rate, new_freq=AudioProcessor.SAMPLE_RATE\n",
    "            )\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        if apply_wiener:\n",
    "            waveform = AudioProcessor.wiener_filter(waveform)\n",
    "\n",
    "        torchaudio.save(\"./test.wav\", waveform, AudioProcessor.SAMPLE_RATE)\n",
    "\n",
    "        # compute log-mel spectrogram\n",
    "        mel_spec = AudioProcessor.mel_spectrogram_transform(waveform)\n",
    "        log_mel_spec = AudioProcessor.log_transform(mel_spec + AudioProcessor.EPSILON)  # add epsilon to avoid log(0)\n",
    "\n",
    "        return log_mel_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10056f02",
   "metadata": {
    "papermill": {
     "duration": 0.001625,
     "end_time": "2025-05-07T07:45:06.984012",
     "exception": false,
     "start_time": "2025-05-07T07:45:06.982387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0359317",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-07T07:45:06.989100Z",
     "iopub.status.busy": "2025-05-07T07:45:06.988716Z",
     "iopub.status.idle": "2025-05-07T07:48:17.750548Z",
     "shell.execute_reply": "2025-05-07T07:48:17.749282Z"
    },
    "papermill": {
     "duration": 190.767424,
     "end_time": "2025-05-07T07:48:17.753240",
     "exception": false,
     "start_time": "2025-05-07T07:45:06.985816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading samples: 100%|██████████| 702/702 [02:09<00:00,  5.42it/s]\n",
      "Loading samples: 100%|██████████| 150/150 [00:28<00:00,  5.30it/s]\n",
      "Loading samples: 100%|██████████| 151/151 [00:27<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtitle text:\n",
      " i love you pumpkin\n",
      "i love you honey bunny\n",
      "everybody be cool\n",
      "this is a robbery\n",
      "customers murmuring  any one of you fucking pricks move and i'll execute every one of you motherfuckers\n",
      "you got that\n",
      "you just be quiet over there\n",
      "waitresses on the floor\n",
      "get on the fuckin' get the fuck down\n",
      "you're in a blind spot\n",
      "take your dames over to that booth on the count of ten\n",
      "mexicans out of the fucking kitchen\n",
      "1 2 3 4 5 6 7 8\n",
      "Feature shape: (2995, 64)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "\n",
    "from IPython.display import Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class MovieSubDataset(Dataset):\n",
    "    def __init__(self, samples: List[Tuple[str, str]], transform=None, target_transform=None, show_progress: bool =True):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data = []\n",
    "\n",
    "        iterator = tqdm(samples, desc=\"Loading samples\") if show_progress else samples\n",
    "\n",
    "        for wav_path, txt_path in iterator:\n",
    "            waveform, sample_rate = torchaudio.load(wav_path)\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                subtitle_text = f.read()\n",
    "\n",
    "            features = AudioProcessor.preprocess(waveform, sample_rate)\n",
    "\n",
    "            if self.transform:\n",
    "                features = self.transform(features)\n",
    "            if self.target_transform:\n",
    "                subtitle_text = self.target_transform(subtitle_text)\n",
    "            \n",
    "            self.data.append((features, subtitle_text))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pads spectrograms along time dimension and returns batched features and subtitles.\"\"\"\n",
    "    features, subtitles = zip(*batch)\n",
    "\n",
    "    # [1, mel, time] → [time, mel]\n",
    "    features = [f.squeeze(0).transpose(0, 1) for f in features]\n",
    "\n",
    "    # pad time dimension\n",
    "    padded_features = pad_sequence(features, batch_first=True)  # shape: [B, T, M]\n",
    "    \n",
    "    return padded_features, subtitles\n",
    "\n",
    "\n",
    "def load_movie_subs(root_dir: str, batch_size: int = 64) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Load movie subs dataset, split into train/val/test sets, and return corresponding DataLoaders.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_dir : str\n",
    "        Path to the dataset directory containing subdirectories for each movie.\n",
    "    batch_size : int, optional\n",
    "        Batch size for the DataLoaders. Default is 64.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[DataLoader, DataLoader, DataLoader]\n",
    "        DataLoaders for training, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    all_samples = []\n",
    "\n",
    "    for movie_name in os.listdir(root_dir):\n",
    "        movie_path = os.path.join(root_dir, movie_name)\n",
    "        if not os.path.isdir(movie_path):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(movie_path):\n",
    "            if fname.endswith(\".wav\"):\n",
    "                base = os.path.splitext(fname)[0]\n",
    "                \n",
    "                wav_path = os.path.join(movie_path, f\"{base}.wav\")\n",
    "                txt_path = os.path.join(movie_path, f\"{base}.txt\")\n",
    "                \n",
    "                if os.path.exists(wav_path) and os.path.exists(txt_path):\n",
    "                    all_samples.append((wav_path, txt_path))\n",
    "\n",
    "    # shuffle and split\n",
    "    random.seed(42)\n",
    "    random.shuffle(all_samples)\n",
    "\n",
    "    train_data, temp_data = train_test_split(all_samples, test_size=0.3, random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "    # create DataLoaders\n",
    "    train_loader = DataLoader(MovieSubDataset(train_data), batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(MovieSubDataset(val_data), batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(MovieSubDataset(test_data), batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_loader, validation_loader, test_loader = load_movie_subs(\"/kaggle/input/movie2sub-dataset/dataset\")\n",
    "\n",
    "    batch = next(iter(train_loader))\n",
    "    feature, subtitle = batch[0][0], batch[1][0]\n",
    "\n",
    "    print(\"Subtitle text:\\n\", subtitle)\n",
    "    print(f\"Feature shape: {feature.numpy().shape}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7343266,
     "sourceId": 11700058,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 205.294422,
   "end_time": "2025-05-07T07:48:20.459281",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-07T07:44:55.164859",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
